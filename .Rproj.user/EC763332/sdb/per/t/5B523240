{
    "collab_server" : "",
    "contents" : "---\ntitle: Credit Risk & Neural Networks\nauthor: Cameron Pfiffer\ndate: '2017-05-04'\nslug: ''\ncategories: []\ntags: []\ndescription: ''\n---\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)\n```\n\n```{r echo=FALSE}\ndata <- readRDS(\"C:/Users/cpfif/Documents/blogdown/web/data/loans.rds\")\n```\n\nIn my [previous post about credit risk](2017/05/01/2017-05-01-credit-risk-part-1/), I used the fairly simple method of logistic regression, and we ended up with an 89% accuracy, though we had an unfavorable ratio of true positives to false positives. I'd like to see mess around with neural networks to see if they can better predict loan default. \n\nMy background in neural networks came from Andrew Ng's [Machine Learning](https://www.coursera.org/learn/machine-learning) course on Coursera. If you have any interest in machine learning, it's an excellent course to take to get a good dose of background information. The class was taught in Matlab/Octave, and I've never actually tried training a neural network in R, so hopefully this'll be fun and interesting. \n\nI'm using the `neuralnet` package, so we should bring that in, and set a seet for reproducibility purposes.\n\n```{r}\nlibrary(neuralnet)\nlibrary(nnet)\nset.seed(1010)\n```\n\nNow we repeat a couple of data cleaning steps we did in the logistic regression article. First, we filter out any observations with missing values.\n\n```{r}\n#Filter out rows with any NAs.\ndata <- data[complete.cases(data),]\nhead(data)\n```\n\nNext, we have to convert everything into numbers. This means `home_ownership` and `grade` (which are currently factors) need to be given an integer value. The reason we do this is so that we can regularize the data - that is, scale it to a value between 0 and 1. This might seem fairly arbitrary, but I'm doing it because it can cause a couple of problems down the line when traing a neural network.\n\n```{r}\n# Convert everything into numbers. \n\n# For credit score, A = 1, B = 2, etc.\ndata$grade <- as.numeric(data$grade)\n\n# For home ownership: Mortgage = 1, Other = 2, Own = 3, Rent = 4.\ndata$home_ownership <- as.numeric(data$home_ownership)\nhead(data)\n```\n\nNow, we regularize our data. We have to find the max and min for each column, and then scale each variable to a value between 0 and 1, using the below code:\n\n```{r}\n# Create Vector of Column Max and Min Values\nmaxs <- apply(data[,2:8], 2, max)\nmins <- apply(data[,2:8], 2, min)\n\n# Use scale() and convert the resulting matrix to a data frame\nscaled <- as.data.frame(scale(data[,2:8], center = mins, scale = maxs - mins))\n\n#Add loan_status back into the dataframe\nscaled$loan_status <- data$loan_status\n\n# Check out results\nhead(scaled)\n```\n\nThen we split the data into a training set and a testing set, again 75/25:\n\n```{r}\n#Split data into a random 75/25 split of train/test datasets\nset.seed(900)\nbound <- floor((nrow(scaled)/4)*3)\nscaled <- scaled[sample(nrow(scaled)),]\ntrain <- scaled[1:bound,]\ntest <- scaled[(bound+1):nrow(scaled),]\n```\n\nThis is more of a technical thing than anything else, but the `neuralnet` package lakcs the ability to do the `'y ~ .'` notation that's so handy, where you can classify everything else in the dataset as an independent variable. The following code (and some of the other stuff in this post) is thanks to [Jose Portillo](http://www.kdnuggets.com/2016/08/begineers-guide-neural-networks-r.html/2).\n\n```{r}\n# We want the names of all the features that aren't loan_status, hence the 1:7.\nfeatures <- names(scaled[,1:7])\n\n# Concatenate strings\nf <- paste(features,collapse=' + ')\nf <- paste('loan_status ~',f)\n\n# Convert to formula\nf <- as.formula(f)\n\nf\n```\n\nNow we can get into the meat of training the network.\n\n```{r}\n#system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE, \n#                            algorithm = 'backprop', learningrate = 1, rep = 10))\nnn <- nnet(loan_status ~ ., data = train, size = 15)\n```\n\n```{r}\n# Now we check the test set, and round the estimated values up or down.\nthreshold = 0.2\npredicted <- predict(nn, test)\n#predicted$net.result <- sapply(predicted$net.result,round,digits=0)\n#predicted$net.result <- sapply(predicted$net.result, \n                               #function(x) {ifelse(x > threshold, 1, 0)})\n```\n\n```{r}\npca <- prcomp(scaled)\npci <- data.frame(pca$x, loan_status = scaled$loan_status)\n\nggplot(pci,aes(x=PC1,y=PC3,col=loan_status))+\n   geom_point()+\n   theme_classic()\n```\n\n\n\n```{r}\ntable(test$loan_status, predicted$net.result)\nplot(nn)\n```\n\n\n",
    "created" : 1493909413839.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "618769302",
    "id" : "5B523240",
    "lastKnownWriteTime" : 1493966058,
    "last_content_update" : 1493966058873,
    "path" : "C:/Users/cpfif/Documents/blogdown/web/content/post/2017-05-04-creditrisk-part-2.rmd",
    "project_path" : "content/post/2017-05-04-creditrisk-part-2.rmd",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}