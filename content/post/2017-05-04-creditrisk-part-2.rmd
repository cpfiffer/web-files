---
title: Credit Risk & Neural Networks
author: Cameron Pfiffer
date: '2017-05-04'
slug: ''
categories: []
tags: []
description: ''
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
```

```{r echo=FALSE}
data <- readRDS("C:/Users/cpfif/Documents/blogdown/web/data/loans.rds")
```

In my [previous post about credit risk](2017/05/01/2017-05-01-credit-risk-part-1/), I used the fairly simple method of logistic regression, and we ended up with an 89% accuracy, though we had an unfavorable ratio of true positives to false positives. I'd like to see mess around with neural networks to see if they can better predict loan default. 

My background in neural networks came from Andrew Ng's [Machine Learning](https://www.coursera.org/learn/machine-learning) course on Coursera. If you have any interest in machine learning, it's an excellent course to take to get a good dose of background information. The class was taught in Matlab/Octave, and I've never actually tried training a neural network in R, so hopefully this'll be fun and interesting. 

I'm using the `neuralnet` package, so we should bring that in, and set a seet for reproducibility purposes.

```{r}
library(neuralnet)
library(nnet)
set.seed(1010)
```

Now we repeat a couple of data cleaning steps we did in the logistic regression article. First, we filter out any observations with missing values.

```{r}
#Filter out rows with any NAs.
data <- data[complete.cases(data),]
head(data)
```

Next, we have to convert everything into numbers. This means `home_ownership` and `grade` (which are currently factors) need to be given an integer value. The reason we do this is so that we can regularize the data - that is, scale it to a value between 0 and 1. This might seem fairly arbitrary, but I'm doing it because it can cause a couple of problems down the line when traing a neural network.

```{r}
# Convert everything into numbers. 

# For credit score, A = 1, B = 2, etc.
data$grade <- as.numeric(data$grade)

# For home ownership: Mortgage = 1, Other = 2, Own = 3, Rent = 4.
data$home_ownership <- as.numeric(data$home_ownership)
head(data)
```

Now, we regularize our data. We have to find the max and min for each column, and then scale each variable to a value between 0 and 1, using the below code:

```{r}
# Create Vector of Column Max and Min Values
maxs <- apply(data[,2:8], 2, max)
mins <- apply(data[,2:8], 2, min)

# Use scale() and convert the resulting matrix to a data frame
scaled <- as.data.frame(scale(data[,2:8], center = mins, scale = maxs - mins))

#Add loan_status back into the dataframe
scaled$loan_status <- data$loan_status

# Check out results
head(scaled)
```

Then we split the data into a training set and a testing set, again 75/25:

```{r}
#Split data into a random 75/25 split of train/test datasets
set.seed(900)
bound <- floor((nrow(scaled)/4)*3)
scaled <- scaled[sample(nrow(scaled)),]
train <- scaled[1:bound,]
test <- scaled[(bound+1):nrow(scaled),]
```

This is more of a technical thing than anything else, but the `neuralnet` package lakcs the ability to do the `'y ~ .'` notation that's so handy, where you can classify everything else in the dataset as an independent variable. The following code (and some of the other stuff in this post) is thanks to [Jose Portillo](http://www.kdnuggets.com/2016/08/begineers-guide-neural-networks-r.html/2).

```{r}
# We want the names of all the features that aren't loan_status, hence the 1:7.
features <- names(scaled[,1:7])

# Concatenate strings
f <- paste(features,collapse=' + ')
f <- paste('loan_status ~',f)

# Convert to formula
f <- as.formula(f)

f
```

Now we can get into the meat of training the network.

```{r}
#system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE, 
#                            algorithm = 'backprop', learningrate = 1, rep = 10))
nn <- nnet(loan_status ~ ., data = train, size = 15)
```

```{r}
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- predict(nn, test)
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
#predicted$net.result <- sapply(predicted$net.result, 
                               #function(x) {ifelse(x > threshold, 1, 0)})
```

```{r}
pca <- prcomp(scaled)
pci <- data.frame(pca$x, loan_status = scaled$loan_status)

ggplot(pci,aes(x=PC1,y=PC3,col=loan_status))+
   geom_point()+
   theme_classic()
```



```{r}
table(test$loan_status, predicted$net.result)
plot(nn)
```


