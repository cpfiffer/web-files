---
title: Credit Risk & Logistic Regression
author: Cameron Pfiffer
date: '2017-05-01'
slug: ''
categories: ["Data Science"]
tags: ["R", "Kaggle", "Finance"]
description: 'An analysis of loan data using logistic regression and R.'
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
```

```{r echo=FALSE}
data <- readRDS("C:/Users/cpfif/Documents/cameron.pfiffer.org/web/data/loans.rds")
```

I thought I'd do a little bit of analysis to showcase some credit risk analysis, using __logistic regression__. I've pulled this sample loan data from a DataCamp course on [credit risk](https://www.datacamp.com/courses/introduction-to-credit-risk-modeling-in-r). It's a cool class, you should check it out if you have time. In a later post, I will try this same analysis with a neural network to see if it has better predictive capabilities.

Here's a look at the data. You can see we have all kinds of valuable information we can use in determining whether someone is likely to default - people with high rates and low credit scores are more likely to default, while people who own their homes and have had long term employment grades are less likely to default. We also have a column called `loan_status`, which is a boolean value indicating whether that particular borrower has defaulted.

```{r}
head(data)
```

# Logistic Regression
If you're unfamiliar with logistic regression, that's alright. What it does (in broad strokes) is allow you to predict a value __between 1 and 0__, and provide you with a degree of certainty. For example, if we ran a logistic regression on a bunch of variables, and then found relevant coefficients, we could use the features of a particular borrower to determine what level of risk they have. A lender could take appropriate measures with someone with a very low chance (0.02) of default by providing them with lower rates, or by simply not lending to someone with a very high chance of default (0.99).

Now we should tidy up some of the data and get rid of any rows with `NA`s. There are more efficient ways of dealing with this problem, but for our purposes we only lose about 3,000 observations, bringing us to about 25,000 observations. This is enough by most measures to build a rudimentary predictive model.

```{r}
#Filter out rows with any NAs.
data <- data[complete.cases(data),]
head(data)
```

Let's split our dataset into two pieces, 60/40. This allows us to design a model with the 60% dataset and test it on the 40% dataset. If I was performing more exploratory analysis, I'd split the 40 in half, one for messing around in and the other for testing, but I'm pretty much skipping right to the modeling for now.

```{r}
set.seed(9090)
bound <- floor((nrow(data)/4)*3)
data <- data[sample(nrow(data)),]
train <- data[1:bound,]
test <- data[(bound+1):nrow(data),]
```

Now to the preliminary model. We can use the R's built-in functions to handle this. See below a summary of the output.

```{r}
model <- glm(loan_status ~., family = binomial(link='logit'), data = train)
summary(model)
```

Now, we test our accuracy. How well does our model predict loan status? The below code spits out the accuracy when we test our model on the `test` dataframe, and we get a result of 89%. Not bad!

```{r}
fit <- predict(model, newdata = test, type = "response")
fit <- ifelse(fit > 0.5, 1, 0)
error <- mean(fit != test$loan_status)
print(paste( "Accuracy is: ", 1 - error))
```

This [lovely blogpost](https://datascienceplus.com/perform-logistic-regression-in-r/) recommends plotting the true positive vs. false positives. The code for that is below. The plot shows a nearly straight line, which means we really aren't especially predictive - the output at the bottom of 0.66 similarly shows the same. We'd like this value to be closer to one to indicate predictive ability.

```{R}
library(ROCR)
p <- predict(model, newdata = test, type = "response")
pr <- prediction(p, test$loan_status)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure="auc")
auc <- auc@y.values[[1]]
auc
```

But we've skipped a couple of important steps in modeling. The model summary shows a litany of variables that really aren't that predictive; we need to take them out. We're going to leave anything with a `.` or any number of asterisks (`*`) in, because they are significant. A 10% significant will suffice for me.

```{R}
model2 <- glm(loan_status ~ int_rate + grade + emp_length +
                (home_ownership=='OTHER') + annual_inc + age, 
              family = binomial(link='logit'), 
              data = train)
summary(model2)
```

Now to test accuracy. It basically has yielded no meaningful change in predictive ability - but then again, that's hard to do. All we've done is create a more parsimonious model in line with current thinking in statistics and econometrics.

```{r}
fit <- predict(model2, newdata = test, type = "response")
fit <- ifelse(fit > 0.5, 1, 0)
error <- mean(fit != test$loan_status)
print(paste( "Accuracy is: ", 1 - error))
```

Finally, we plot the cure we showed before. Again, no real difference, but we can feel better that we have a smaller model with less "junk" floating around.

```{R}
library(ROCR)
p <- predict(model2, newdata = test, type = "response")
pr <- prediction(p, test$loan_status)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure="auc")
auc <- auc@y.values[[1]]
auc
```

Thanks for taking the time to read this post. Check out later posts where I use neural networks to look at this same dataset. It'll be fun for the whole family!

---
