f <- paste('loan_status ~',f)
# Convert to formula
f <- as.formula(f)
f
?neuralnet
nn <- neuralnet(f, train, hidden = 10, linear.output = FALSE)
nn
predicted <- compute(nn, test[,1:7])
head(predicted)
predicted[1]
predicted[2]
predicted$net.result
head(predicted$net.result)
head(test)
head(predicted$net.result)
predicted.nn.values$net.result <- sapply(predicted.nn.values$net.result,round,digits=0)
predicted.nn.values$net.result <- sapply(predicted.nn.values$net.result,round,digits=0)
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
head(predicted$net.result)
table(test$loan_status, predicted$net.result)
plot(nn)
?table
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/', warning=FALSE, message=FALSE)
data <- readRDS("C:/Users/cpfif/Documents/blogdown/web/data/loans.rds")
library(neuralnet)
set.seed(1010)
#Filter out rows with any NAs.
data <- data[complete.cases(data),]
head(data)
# Convert everything into numbers.
# For credit score, A = 1, B = 2, etc.
data$grade <- as.numeric(data$grade)
# For home ownership: Mortgage = 1, Other = 2, Own = 3, Rent = 4.
data$home_ownership <- as.numeric(data$home_ownership)
head(data)
# Create Vector of Column Max and Min Values
maxs <- apply(data[,2:8], 2, max)
mins <- apply(data[,2:8], 2, min)
# Use scale() and convert the resulting matrix to a data frame
scaled <- as.data.frame(scale(data[,2:8], center = mins, scale = maxs - mins))
#Add loan_status back into the dataframe
scaled$loan_status <- data$loan_status
# Check out results
head(scaled)
#Split data into a random 75/25 split of train/test datasets
set.seed(9090)
bound <- floor((nrow(scaled)/4)*3)
scaled <- scaled[sample(nrow(scaled)),]
train <- scaled[1:bound,]
test <- scaled[(bound+1):nrow(scaled),]
# We want the names of all the features that aren't loan_status, hence the 1:7.
features <- names(scaled[,1:7])
# Concatenate strings
f <- paste(features,collapse=' + ')
f <- paste('loan_status ~',f)
# Convert to formula
f <- as.formula(f)
f
nn <- neuralnet(f, train, hidden = c(7,7,7), linear.output = FALSE)
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
nn
compute(nn,test)
compute(nn,test[,1:7])
nn <- neuralnet(f, train, hidden = c(7), linear.output = FALSE)
system.time(nn <- neuralnet(f, train, hidden = c(7), linear.output = FALSE))
system.time(nn <- neuralnet(f, train, hidden = c(2), linear.output = FALSE))
system.time(nn <- neuralnet(f, train, hidden = 1, linear.output = FALSE))
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
system.time(nn <- neuralnet(f, train, hidden = 10, linear.output = FALSE))
install.packages("tensorflow")
library(tensorflow)
library(tensorflow)
?blogdown
library(blogdown)
?blogdown
?new_post
library(neuralnet)
library(caret)
install.packages("caret")
library(caret)
?caret
??`caret
``
install.packages("doMC")
install.packages("doMPI")
rnorm(10)
rnorm(10) * rnorm(10)
rnorm(10) %*% rnorm(10)
rep(rnorm(10), 10)
rep(rnorm(10), 10) %*% rep(rnorm(10), 10)
library(doMPI)
install.packages("Rmpi")
library(doMPI)
install.packages("Rmpi", dependencies = TRUE)
install.packages("doMPI", dependencies = TRUE)
library(doMPI)
library("Rmpi", lib.loc="~/R/win-library/3.4")
library(doMPI)
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
data <- readRDS("C:/Users/cpfif/Documents/blogdown/web/data/loans.rds")
library(neuralnet)
set.seed(1010)
#Filter out rows with any NAs.
data <- data[complete.cases(data),]
head(data)
# Convert everything into numbers.
# For credit score, A = 1, B = 2, etc.
data$grade <- as.numeric(data$grade)
# For home ownership: Mortgage = 1, Other = 2, Own = 3, Rent = 4.
data$home_ownership <- as.numeric(data$home_ownership)
head(data)
# Create Vector of Column Max and Min Values
maxs <- apply(data[,2:8], 2, max)
mins <- apply(data[,2:8], 2, min)
# Use scale() and convert the resulting matrix to a data frame
scaled <- as.data.frame(scale(data[,2:8], center = mins, scale = maxs - mins))
#Add loan_status back into the dataframe
scaled$loan_status <- data$loan_status
# Check out results
head(scaled)
#Split data into a random 75/25 split of train/test datasets
set.seed(9090)
bound <- floor((nrow(scaled)/4)*3)
scaled <- scaled[sample(nrow(scaled)),]
train <- scaled[1:bound,]
test <- scaled[(bound+1):nrow(scaled),]
# We want the names of all the features that aren't loan_status, hence the 1:7.
features <- names(scaled[,1:7])
# Concatenate strings
f <- paste(features,collapse=' + ')
f <- paste('loan_status ~',f)
# Convert to formula
f <- as.formula(f)
f
system.time(nn <- neuralnet(f, train, hidden = 1, linear.output = FALSE))
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
plot(nn)
system.time(nn <- neuralnet(f, train, hidden = 2, linear.output = FALSE))
system.time(nn <- neuralnet(f, train, hidden = 2, linear.output = FALSE, algorithm = 'backprop'))
system.time(nn <- neuralnet(f, train, hidden = 2, linear.output = FALSE,
algorithm = 'backprop', learningrate = 1))
system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
algorithm = 'backprop', learningrate = 1))
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
plot(nn)
system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
algorithm = 'backprop', learningrate = 1, rep = 10))
table(test$loan_status, predicted$net.result)
plot(nn)
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
plot(nn)
sum(test$loan_status)
sum(predicted$net.result)
#Split data into a random 75/25 split of train/test datasets
set.seed(900)
bound <- floor((nrow(scaled)/4)*3)
scaled <- scaled[sample(nrow(scaled)),]
train <- scaled[1:bound,]
test <- scaled[(bound+1):nrow(scaled),]
# We want the names of all the features that aren't loan_status, hence the 1:7.
features <- names(scaled[,1:7])
# Concatenate strings
f <- paste(features,collapse=' + ')
f <- paste('loan_status ~',f)
# Convert to formula
f <- as.formula(f)
f
system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
algorithm = 'backprop', learningrate = 1, rep = 10))
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
plot(nn)
predicted$net.result <- sapply(predicted$net.result,round,digits=0)
# Now we check the test set, and round the estimated values up or down.
predicted <- compute(nn, test[,1:7])
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
table(test$loan_status, predicted$net.result)
#table(test$loan_status, predicted$net.result)
mean(predicted$net.result)
plot(nn)
# Now we check the test set, and round the estimated values up or down.
threshold <- 0.2
predicted <- compute(nn, test[,1:7])
predicted$net.result <- sapply(predicted$net.result,
function(x) {ifelse(x > 0.2, 1, 0))
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- compute(nn, test[,1:7])
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
predicted$net.result <- sapply(predicted$net.result,
function(x) {ifelse(x > threshold, 1, 0)})
table(test$loan_status, predicted$net.result)
plot(nn)
sum(test$loan_status)
sum(train$loan_status)
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- compute(nn, test[,1:7])
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
#predicted$net.result <- sapply(predicted$net.result,
#function(x) {ifelse(x > threshold, 1, 0)})
hist(predicted$net.result)
predicted$net.result
hist(predicted$net.result)
library(ggplot2)
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
data <- readRDS("C:/Users/cpfif/Documents/blogdown/web/data/loans.rds")
library(neuralnet)
set.seed(1010)
#Filter out rows with any NAs.
data <- data[complete.cases(data),]
head(data)
# Convert everything into numbers.
# For credit score, A = 1, B = 2, etc.
data$grade <- as.numeric(data$grade)
# For home ownership: Mortgage = 1, Other = 2, Own = 3, Rent = 4.
data$home_ownership <- as.numeric(data$home_ownership)
head(data)
# Create Vector of Column Max and Min Values
maxs <- apply(data[,2:8], 2, max)
mins <- apply(data[,2:8], 2, min)
# Use scale() and convert the resulting matrix to a data frame
scaled <- as.data.frame(scale(data[,2:8], center = mins, scale = maxs - mins))
#Add loan_status back into the dataframe
scaled$loan_status <- data$loan_status
# Check out results
head(scaled)
#Split data into a random 75/25 split of train/test datasets
set.seed(900)
bound <- floor((nrow(scaled)/4)*3)
scaled <- scaled[sample(nrow(scaled)),]
train <- scaled[1:bound,]
test <- scaled[(bound+1):nrow(scaled),]
# We want the names of all the features that aren't loan_status, hence the 1:7.
features <- names(scaled[,1:7])
# Concatenate strings
f <- paste(features,collapse=' + ')
f <- paste('loan_status ~',f)
# Convert to formula
f <- as.formula(f)
f
system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
algorithm = 'backprop', learningrate = 1, rep = 10))
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- compute(nn, test[,1:7])
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
#predicted$net.result <- sapply(predicted$net.result,
#function(x) {ifelse(x > threshold, 1, 0)})
hist(predicted$net.result)
table(test$loan_status, predicted$net.result)
col(test)
colnames(test)
plot(nn)
plot(nn)
help
nn$startweights
nn$weights
install.packages("nnet")
library(nnet)
?nnet
#system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
#                            algorithm = 'backprop', learningrate = 1, rep = 10))
nn <- nn(loan_status ~ ., data = train)
#system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
#                            algorithm = 'backprop', learningrate = 1, rep = 10))
nn <- nnet(loan_status ~ ., data = train)
#system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
#                            algorithm = 'backprop', learningrate = 1, rep = 10))
nn <- nnet(loan_status ~ ., data = train, size = 15)
hist(predicted$net.result)
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- predict(nn, test[,1:7])
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
#predicted$net.result <- sapply(predicted$net.result,
#function(x) {ifelse(x > threshold, 1, 0)})
hist(predicted)
predicted
sum(predicted)
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- predict(nn, test[,1:7], type = class)
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- predict(nn, test[,1:7], type = 'class')
predicted <- predict(nn, test[,1:7])
predicted <- predict(nn, test)
hist(predicted)
sum(predicted)
install.packages("pcacomp")
install.packages("prcomp")
?prcomp
prcomp(data)
pca <- prcomp(data)
plot pca
plot(pca)
library(ggplot2)
autoplot(pca)
library(ggfortify)
install.packages("ggfortify")
library(ggfortify)
autoplot(pca)
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE)
data <- readRDS("C:/Users/cpfif/Documents/blogdown/web/data/loans.rds")
library(neuralnet)
library(nnet)
set.seed(1010)
#Filter out rows with any NAs.
data <- data[complete.cases(data),]
head(data)
# Convert everything into numbers.
# For credit score, A = 1, B = 2, etc.
data$grade <- as.numeric(data$grade)
# For home ownership: Mortgage = 1, Other = 2, Own = 3, Rent = 4.
data$home_ownership <- as.numeric(data$home_ownership)
head(data)
# Create Vector of Column Max and Min Values
maxs <- apply(data[,2:8], 2, max)
mins <- apply(data[,2:8], 2, min)
# Use scale() and convert the resulting matrix to a data frame
scaled <- as.data.frame(scale(data[,2:8], center = mins, scale = maxs - mins))
#Add loan_status back into the dataframe
scaled$loan_status <- data$loan_status
# Check out results
head(scaled)
#Split data into a random 75/25 split of train/test datasets
set.seed(900)
bound <- floor((nrow(scaled)/4)*3)
scaled <- scaled[sample(nrow(scaled)),]
train <- scaled[1:bound,]
test <- scaled[(bound+1):nrow(scaled),]
# We want the names of all the features that aren't loan_status, hence the 1:7.
features <- names(scaled[,1:7])
# Concatenate strings
f <- paste(features,collapse=' + ')
f <- paste('loan_status ~',f)
# Convert to formula
f <- as.formula(f)
f
#system.time(nn <- neuralnet(f, train, hidden = c(10,10,10), linear.output = FALSE,
#                            algorithm = 'backprop', learningrate = 1, rep = 10))
nn <- nnet(loan_status ~ ., data = train, size = 15)
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- predict(nn, test)
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
#predicted$net.result <- sapply(predicted$net.result,
#function(x) {ifelse(x > threshold, 1, 0)})
hist(predicted)
table(test$loan_status, predicted$net.result)
pca <- prcomp(predicted)
library(ggplot2)
library(ggfortify)
autoplot(pca)
pca
# Now we check the test set, and round the estimated values up or down.
threshold = 0.2
predicted <- predict(nn, test)
#predicted$net.result <- sapply(predicted$net.result,round,digits=0)
#predicted$net.result <- sapply(predicted$net.result,
#function(x) {ifelse(x > threshold, 1, 0)})
predicted
pca <- prcomp(predicted)
autoplot(pca)
pca
pca <- prcomp(data)
pca <- prcomp(train)
autoplot(pca)
pca <- prcomp(train)
pca <- prcomp(train)
ggplot2(data = pca) +
pca <- prcomp(train)
ggplot2(data = pca) +
geom_line()
pca <- prcomp(train)
ggplot(data = pca) +
geom_line(
pca <- prcomp(train)
ggplot(data = pca) +
geom_line()
pca <- prcomp(train)
ggplot(data = pca) +
geom_dotplot()
pca <- prcomp(train)
autoplot(pca, color = "loan_status")
pca <- prcomp(train)
autoplot(pca, data = train, color = "loan_status")
autoplot(pca, data = train, color = loan_status)
autoplot(pca, data = train, color = loan_status)
sum(train$loan_status)
autoplot(pca, data = train, color = 'red')
autoplot(pca, data = train, color = 'red', loadings = TRUE)
autoplot(pca, data = train, color = 'red')
autoplot(pca, data = train, color = 'red')
autoplot(pca, data = train, color = 'BLUE')
autoplot(pca, data = data, color = 'BLUE')
pca <- prcomp(data)
autoplot(pca, data = data, color = 'red')
autoplot(pca, data = data, color = 'blue')
help(autoplot.prcomp)
autoplot(pca, data = data, color = 'blue', x = 5, y = 6)
pca
autoplot(pca, data = data, color = 'blue', x = 7, y = 8)
autoplot(pca, data = data, color = data$loan_status, x = 7, y = 8)
pca <- prcomp(scaled)
autoplot(pca, data = scaled, color = 'loan_status', x = 7, y = 8)
autoplot(pca, data = scaled, color = 'loan_status')
autoplot(pca, data = scaled, color = 'loan_status', x=2,y=3)
install.packages("cluster")
library(cluster)
autoplot(clara(scaled))
autoplot(clara(scaled,2))
?clara
mean(scaled)
head(scaled)
head(sample(scaled,10)
)
head(sample(scaled,10))
head(sample(scaled,1))
head(sample(scaled))
head(sample(scaled))
head(sample(scaled))
head(sample(scaled))
head(sample(scaled))
head(sample(scaled))
summary(pca)
autoplot(pca, data = scaled, color = loan_status, x=2,y=3)
autoplot(pca, data = scaled, color = loan_status)
autoplot(pca, data = scaled, color = loan_status, x=2,y=3)
autoplot(pca, data = scaled, color = loan_status, x=2,y=3, frame = T)
autoplot(pca, data = scaled, color = loan_status, x=4,y=5, frame = T)
autoplot(pca, data = scaled, color = loan_status, x=5,y=6, frame = T)
ggplot(pca)
ggplot(pca) + geom_point()
ggplot(pca) + geom_point(aes = c(PC1,PC2))
pca$x
pca$PC1
pca
pca$rotation
pca$rotation$PC1
pca <- prcomp(scaled)
pci <- data.frame(pca$x, loan_status = scaled$loan_status)
pca <- prcomp(scaled)
pci <- data.frame(pca$x, loan_status = scaled$loan_status)
ggplot(PCi,aes(x=PC1,y=PC2,col=loan_status))+
geom_point()+
scale_color_manual(values = c("#FF1BB3","#A7FF5B","#99554D"))+ #your colors here
theme_classic()
pca <- prcomp(scaled)
pci <- data.frame(pca$x, loan_status = scaled$loan_status)
ggplot(pci,aes(x=PC1,y=PC2,col=loan_status))+
geom_point()+
scale_color_manual(values = c("#FF1BB3","#A7FF5B","#99554D"))+ #your colors here
theme_classic()
pca <- prcomp(scaled)
pci <- data.frame(pca$x, loan_status = scaled$loan_status)
ggplot(pci,aes(x=PC1,y=PC2,col=loan_status))+
geom_point()+
theme_classic()
pca <- prcomp(scaled)
pci <- data.frame(pca$x, loan_status = scaled$loan_status)
ggplot(pci,aes(x=PC2,y=PC3,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC4,y=PC5,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC7,y=PC8,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC1,y=PC2,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC1,y=PC3,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC1,y=PC4,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC1,y=PC5,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC2,y=PC5,col=loan_status))+
geom_point()+
theme_classic()
ggplot(pci,aes(x=PC2,y=PC8,col=loan_status))+
geom_point()+
theme_classic()
