<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.20.6" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
  <title>Cameron Pfiffer</title>
  

  
  <link rel="stylesheet" href="./css/poole.css">
  <link rel="stylesheet" href="./css/syntax.css">
  <link rel="stylesheet" href="./css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="./apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="./favicon.png">

  
  <link href="./index.xml" rel="alternate" type="application/rss+xml" title="Cameron Pfiffer" />
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    });
    </script>
    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

<body class="theme-base-0f ">

<link href="https://fonts.googleapis.com/css?family=Lato:100|PT+Serif" rel="stylesheet">
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="./"><h1>Cameron Pfiffer</h1></a>
      <p class="lead">
        
 Programming, finance, writing. 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="./">Home</a> </li>
      
        <li><a href="./about/index.html"> About </a></li>
      
        <li><a href="./post"> Posts </a></li>
      
    </ul>

  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="./post/corporate-debt/">
        Corporate Debt
      </a>
    </h1>

    <span class="post-date">Tue, Sep 12, 2017</span>

    <p>This strikes me as <a href="https://www.ft.com/content/46027dd2-8f6c-11e7-9084-d0c17942ba93">a strange use of corporate cash</a>.</p>
<blockquote>
<p>Thirty US companies together have more than $800bn of fixed-income investments, according to a Financial Times analysis of their most recent filings with the US Securities and Exchange Commission.</p>
</blockquote>
<p>That’s a lot of money. You might be wondering who those thirty companies are, and it’s probably who you’d expect – tech companies with far too much cash sitting around collecting dust. Apple, Alphabet, and Microsoft are cited.</p>
<p>This strikes me as a huge problem (<a href="https://www.economist.com/news/business-and-finance/21722809-their-excuses-doing-so-dont-add-up-tech-firms-hoard-huge-cash-piles">I’m not the only one</a>): fixed income investing is really useful if you’re trying to meet fixed obligations, like an insurer or pension fund might, or for hedges on inflation and foreign exchange. It’s certainly not something research and development companies should be doing. They should be piling that cash into R&amp;D, acquisitions, buybacks, literally anything other than fixed income investments.</p>
<p>I’d also be more than willing to take it. You know, for the team.</p>
<p>We’ll close on this happy note:</p>
<blockquote>
<p>The emergence of US companies as a leading investor in corporate debt alongside traditional asset managers comes at a time many in the market express concern about a bond market bubble that could be vulnerable to bursting should inflation and economic growth accelerate.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/the-american-dream/">
        The American Dream
      </a>
    </h1>

    <span class="post-date">Sun, Sep 3, 2017</span>

    <p>Edmund Phelps writing at <a href="https://www.project-syndicate.org/commentary/recalling-the-american-dream-by-edmund-s--phelps-2017-08?utm_source=Project+Syndicate+Newsletter&amp;utm_campaign=e8bef8a0bc-sunday_newsletter_3_9_2017&amp;utm_medium=email&amp;utm_term=0_73bad5b7d8-e8bef8a0bc-104329629">Project Syndicate</a>:</p>

<blockquote>
<p>What made the American Dream distinctive was neither the hope of winning the lottery nor of being buoyed by national market forces or public policy. It was the hope of achieving things, with all that that entails: drawing on one’s personal knowledge, trusting one’s intuition, venturing into the unknown. It reflected the deep need of these Americans to have the experience of succeeding at something: a craftsman’s gratification at seeing his mastery result in better work, or a merchant’s satisfaction at seeing “his ship come in.” It was success that mattered, not relative success (would anyone want to be the sole achiever?). And the process may have mattered more than the success.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-08-26-flashcrash/">
        The Flash Crash
      </a>
    </h1>

    <span class="post-date">Sat, Aug 26, 2017</span>

    <p><a href="http://www.afajof.org/details/journalArticle/10546091/The-Flash-Crash-HighFrequency-Trading-in-an-Electronic-Market.html">A paper</a> came out in the April 2017 edition of the <em>Journal of Finance</em> that I found to be absolutely fascinating. Normally I’d just put a link up on Twitter and move on, but leaving Facebook and Twitter have limited my ability to scream about cool papers. I thought I’d do a brief post about the paper and talk about why it’s interesting to me.</p>
<p>The abstract:</p>
<blockquote>
<p>We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event—the Flash Crash—where a large automated selling program was rapidly executed in the E‐mini S&amp;P 500 stock index futures market. Using audit trail transaction‐level data for the E‐mini on May 6 and the previous three days, we find that the trading pattern of the most active nondesignated intraday intermediaries (classified as High‐Frequency Traders) did not change when prices fell during the Flash Crash.</p>
</blockquote>
<p>The flash crash<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> has always been an interesting subject, because it illustrates a lesson that anyone who is involved with markets knows – if you sell a ton of stuff, the prices go down. But perhaps more importantly, it demonstrates the highly interconnected nature of modern financial markets.</p>
<p>During the flash crash, an institutional trader initiated an algorithm to sell an incredibly large amount of E-mini shares, a futures contract on the S&amp;P 500. The algorithm used was a fairly simplistic strategy that aimed to trade 9% of the past minute’s volume.</p>
<p>I have a lot of feelings on this kind of strategy. It’s not especially tactful, it has no regard to price or timing, and fails to adapt to changing conditions. If I’m aware of someone trading a tremendous amount of shares every minute (and assuming I can devise their strategy before they complete the trade), I might try to game the system by increasing the volume traded in the previous minute – either by simply churning a position or taking large long positions – and then using the foreknowledge that a large trade is about to come to take advantage. Particularly for a trade the size that the institution was trying to make (around $4.9 billion), this is a really crappy way to do it.</p>
<p>Back to the paper. It’s well worth the read. They study audit trail level data for four days during and before the flash crash in the E-mini market, and their interesting finding is that high-frequency traders didn’t really change their trading behavior during the event. This is in contrast to a lot of the murmurings commonly bandied about in regards to the flash crash, where everyone mutters something to the effect of “HFTs got out, liquidity dried up, etc.”</p>
<p>The authors note that market makers, not high-frequency traders, altered their inventory holding behavior in response to changing prices. Interesting.</p>
<p>Give it a read.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>You can read more about the flash crash in the <a href="https://www.sec.gov/news/studies/2010/marketevents-report.pdf">SEC’s report</a> on the matter.<a href="#fnref1">↩</a></p></li>
</ol>
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/english-pronunciation/">
        English Pronunciation
      </a>
    </h1>

    <span class="post-date">Wed, Aug 16, 2017</span>

    <p>My last name tends to throw a lot of people off, because it has far too many f&rsquo;s and not enough real letters. While doing a bit of research on how to tell people how to best pronounce my name (it&rsquo;s &ldquo;fye-fur&rdquo;, not &ldquo;fiffer&rdquo;), I found a <a href="http://ncf.idallen.com/english.html">neat little poem</a> designed to showcase some of the strange ways English pronounces words. I sent it out to a couple of my friends from Norway and Greece and they politely declined to try a live reading. I can see why; I messed up about 15-20 words while reading it aloud.</p>

<p>Here&rsquo;s an excerpt:</p>

<blockquote>
<p>Compare alien with Italian,
Dandelion and battalion.
Sally with ally, yea, ye,
Eye, I, ay, aye, whey, and key.
Say aver, but ever, fever,
Neither, leisure, skein, deceiver.
Heron, granary, canary.
Crevice and device and aerie.</p>

<p>Face, but preface, not efface.
Phlegm, phlegmatic, ass, glass, bass.
Large, but target, gin, give, verging,
Ought, out, joust and scour, scourging.
Ear, but earn and wear and tear
Do not rhyme with here but ere.
Seven is right, but so is even,
Hyphen, roughen, nephew Stephen,
Monkey, donkey, Turk and jerk,
Ask, grasp, wasp, and cork and work.</p>
</blockquote>

<p>Give it a shot, and see how well you can do with the whole poem. It&rsquo;s certainly fun to do.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/perfect-numbers/">
        Perfect Numbers
      </a>
    </h1>

    <span class="post-date">Tue, Aug 15, 2017</span>

    <p>I just started reading a book called <a href="https://books.google.co.uk/books/about/How_to_Prove_it.html?id=murSjwEACAAJ&amp;redir_esc=y&amp;hl=en">How to Prove It: A Structured Approach</a>, a book all about writing and understanding proofs. My unconventional background in mathematics means that I have a fair bit of terror when it comes to proofs; they have often come up on more quantitative finance courses in a tangential manner, as the proofs are often unnessecary. The fact remains that I am terrified by them, and wanted to get over my fear of proofs.</p>
<p>In the process, I read a bit on <a href="https://en.wikipedia.org/wiki/Perfect_number">perfect numbers</a>. One of the proofs in the book was Euclid’s proof about infinite primes, and I figured I should write a little bit of code to find perfect numbers, for gits and shiggles.</p>
<p>Here’s the code to compute <a href="https://en.wikipedia.org/wiki/Aliquot_sum">aliquot sums</a> in Julia, a necessary intermediate step in perfect number evaluation.</p>
<pre><code>using Primes

function aliquot(x::Int64, verbose=false)
    divisors = []
    for i in 1:x-1
        if(x%i == 0)
            append!(divisors, i)
        end
    end
    if verbose == true print(divisors) end
    return sum(divisors)
end</code></pre>
<p>This is a function for computing the next prime number after integer <span class="math inline">\(x\)</span>.</p>
<pre><code>function next_perfect(x::Int64, limit=Inf64)
    # Given a number, find the next highest perfect number.
    found = false
    while(found==false)
        # You can set a computational limit with the &#39;limit&#39; argument.
        if (x &gt;= limit) break end
        if aliquot(x) == x
            print(x, &quot; is the next perfect number.&quot;)
            found=true
            break
        end
        x += 1
    end
    return x
end</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-08-06-a-brief-paper-on-cointegration-in-crude-oil/">
        A Brief Paper on Cointegration in Crude Oil
      </a>
    </h1>

    <span class="post-date">Sun, Aug 6, 2017</span>

    <p>I&rsquo;ve just finished putting the final touches on my final paper for my master&rsquo;s degree, and I thought I&rsquo;d write up a bit about the process. You can get the fancy LaTeX version <a href="https://drive.google.com/file/d/0B6yUWclvz_SNREhObUxEVjFESWs/view?usp=sharing">here</a>.</p>

<p>My final summer course for the master&rsquo;s program was all about Energy Finance, taught by an econometrician<sup class="footnote-ref" id="fnref:de644491"><a rel="footnote" href="#fn:de644491">1</a></sup>. Commensurate with the professor&rsquo;s experience, the final paper was along the same lines. We were allowed to choose between three different project options. Here&rsquo;s the one I picked:</p>

<blockquote>
<p>Are the prices of WTI and Brent co-integrated? Is it possible to profitably trade the price differential? First, summarize the results of the empirical literature. Then conduct your own analysis. Carefully explain your methodology.</p>
</blockquote>

<p>This was easily the most difficult amongst the three options, because you had to design a trading strategy. For many of my colleagues who like programming less, it proved difficult to design a system to evaluate a trading system. But I thought it might be a fun challenge (and it was!) to do so.</p>

<p>I&rsquo;d read the paper if you&rsquo;re particularly interested in the specifics, but I thought I&rsquo;d include a little bit more about the backend. I don&rsquo;t have the chance to get into it in the text, but I basically wrote an entire trading system to measure transaction costs, portfolio value, etc. That was what took the most time. I spent more time debugging the transaction system than writing the paper, and I don&rsquo;t actually talk about it in the paper.</p>

<p>I also got pretty deep into <a href="https://en.wikipedia.org/wiki/Cointegration">cointegration</a>, which is a fascinating concept. Basically, two series that are non-stationary<sup class="footnote-ref" id="fnref:stationary"><a rel="footnote" href="#fn:stationary">2</a></sup> can have a <em>cointegrating vector</em> which makes a linear combination of the series stationary. This has some pretty interesting knock-on effects:</p>

<ol>
<li>If the combined series is stationary, you know that the series crosses the mean frequently, and that it is bound to revert. In crude oil prices, this means that the spread between two assets (like Brent and WTI crude oils) cannot remain too far from the x-intercept for too long.</li>
<li>The variance is (theoretically<sup class="footnote-ref" id="fnref:variance"><a rel="footnote" href="#fn:variance">3</a></sup>) bound to a constant state, so you can moderately achieve returns without excess variance.</li>
</ol>

<p>The bulk on my trading strategy relies on these two factors &ndash; if the cointegrated series is far from zero (or close to zero) then it will <em>eventually</em> return to the mean. I had some pretty neat returns, and actually beat the S&amp;P in and out of sample.</p>

<p>I&rsquo;m really proud of the paper, and hope that maybe someone might get to make use of the strategy I developed. Maybe when I&rsquo;m older and actually have capital, I&rsquo;ll do it, though by then the cointegrating relationship may have changed.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:de644491">These dudes tend to make everything more fun.
 <a class="footnote-return" href="#fnref:de644491"><sup>[return]</sup></a></li>
<li id="fn:stationary">This means that the series is essentially random and has no trend, has a mean of zero, and a constant variance.
 <a class="footnote-return" href="#fnref:stationary"><sup>[return]</sup></a></li>
<li id="fn:variance">I say theoretically, because empirically, the variance is not constant. Particularly the period between 2008 and 2013 or so was especially volatile, and the period between 1994 and 2005 was extremely placid.
 <a class="footnote-return" href="#fnref:variance"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-07-05-julia/">
        Julia
      </a>
    </h1>

    <span class="post-date">Wed, Jul 5, 2017</span>

    <p>As I&rsquo;m wrapping up my master&rsquo;s degree, I have somehow managed to find a large amount of time to pursue personal interests. One of those interests is <a href="https://julialang.org/">Julia</a>, a technical computing language with C-comparable speed. I&rsquo;m not exactly sure where I stumbled on it, but it stuck with me. Of course, the best way to learn something is to do something <em>cool</em> with it, and <a href="https://fivethirtyeight.com/features/pick-a-number-any-number/">FiveThiryEight&rsquo;s Riddler</a> often tends to supply great cannon fodder for programming. This past week&rsquo;s one was a computationally difficult one:</p>

<blockquote>
<p>From Itay Bavly, a chain-link number problem:</p>

<p>You start with the integers from one to 100, inclusive, and you want to organize them into a chain. The only rules for building this chain are that you can only use each number once and that each number must be adjacent in the chain to one of its factors or multiples. For example, you might build the chain:</p>

<p>4, 12, 24, 6, 60, 30, 10, 100, 25, 5, 1, 97</p>

<p>You have no numbers left to place after 97, leaving you with a finished chain of length 12.</p>

<p>What is the longest chain you can build?</p>
</blockquote>

<p>There really doesn&rsquo;t appear to be an easy answer to the problem &ndash; my brother noted this:</p>

<blockquote>
<p>Friend of mine says that traversing a directed graph is NP-Complete, so brute-force is the way to do it. Probably<sup class="footnote-ref" id="fnref:a14987c3"><a rel="footnote" href="#fn:a14987c3">1</a></sup>.</p>
</blockquote>

<p>I thought it seemed like a perfect time to try out Julia. The past two weeks or so I&rsquo;ve been idly combing through Julia&rsquo;s <a href="https://docs.julialang.org/en/stable/">fantastic documentation</a>, and I&rsquo;ve been really impressed by the syntax<sup class="footnote-ref" id="fnref:110a8281"><a rel="footnote" href="#fn:110a8281">2</a></sup> and ease at which you can handle very fast processes.</p>

<p>What I wanted to do was basically try and brute force the problem. Here&rsquo;s my pseudocode.</p>

<ol>
<li>Pick a random number.</li>
<li>Pick a valid number to follow it.</li>
<li>Repeat until you can&rsquo;t find a number.</li>
<li>Do steps 1-3 with new chains, discarding the shortest chain.</li>
</ol>

<p>Mathematically, it&rsquo;s very simple to define what&rsquo;s a multiple and what&rsquo;s a factor, here&rsquo;s two functions that do that. <code>valid</code> is a function where you pass an <code>x</code> and a <code>y</code> and return <code>true</code> if <code>x</code> can be followed by <code>y</code>.</p>

<pre><code># Test if x can be followed by y
function valid(x, y, limit)
    # Determine if y is a multiple of x
    mul = multiples(x, limit) # Get multiples of x
    index = findin(mul, y) # Find if y is in the list of x's multiples
    if index != [] # If the index isn't zero
        return true
    end

    # Now determine if y is a factor of x
    if x % y == 0
        return true
    end
    return false
end
</code></pre>

<p>Multiples generates a list of multiples and returns it.</p>

<pre><code>function multiples(x, limit)
    vals = [0]
    for i in 1:limit
        #print(i, &quot;\n&quot;)
        if (i % x == 0) &amp; (i != x)
            append!(vals, i)
        end
    end
    if vals == [0]
        print(&quot;No multiples of &quot;, x, &quot;.&quot;, &quot;\n&quot;)
    end

    return vals
end
</code></pre>

<p>These two functions are called by <code>makechain</code>, which picks the first number<sup class="footnote-ref" id="fnref:f0d5da3f"><a rel="footnote" href="#fn:f0d5da3f">3</a></sup>, and then tests if subsequent random numbers are valid. When it runs out of valid numbers, it spits out the answer.</p>

<pre><code>function makechain(limit::Int64)
  possible = Array(1:limit)
  first = rand(possible)
  remove = getindex(possible, first)
  deleteat!(possible,remove)

  chain = [first]

    # Pick a random number.
    # Check if that number is valid.
    # If it isn't pick a new one, until they're all gone.
    testPosition = possible
    shuffle!(testPosition)
    for i in testPosition
        v = valid(chain[end], i, limit)
        if v == true
            append!(chain, i)
        end
    end

  return chain
end
</code></pre>

<p>Finally, the final function just runs <code>makechain</code> a bunch of times and finds the longest chain it can.</p>

<pre><code>function find_longest(iterations::Int64, limit=100)
    longest = []
    for i in 1:iterations
        chain = makechain(limit)
        if length(chain) &gt; length(longest)
            longest = chain
        end
    end
    return longest
end
</code></pre>

<p>My biggest output was something like 27 integers long after building 10 million chains, which was far below the 77 found by two other contestants. One guy apparently solved it with some nifty combinatorics software.</p>

<p>Even though I didn&rsquo;t get the right answer, I had a lot of fun working with Julia for the first time and I&rsquo;m looking forward to finding neat things to do with it. Also, Julia is <strong>wicked fast</strong>.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:a14987c3">Later, this was confirmed by <a href="https://fivethirtyeight.com/features/is-this-bathroom-occupied/">Oliver Roeder at the Riddler</a>.
 <a class="footnote-return" href="#fnref:a14987c3"><sup>[return]</sup></a></li>
<li id="fn:110a8281">It kind of reads like Python with a bit of Matlab.
 <a class="footnote-return" href="#fnref:110a8281"><sup>[return]</sup></a></li>
<li id="fn:f0d5da3f">The function&rsquo;s argument, <code>limit</code>, allows you to test chains between 1 and any integer.
 <a class="footnote-return" href="#fnref:f0d5da3f"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-12-kaggle-titanic/">
        Kaggle Titanic
      </a>
    </h1>

    <span class="post-date">Tue, May 23, 2017</span>

    <p>I thought I’d start getting into <a href="www.kaggle.com">Kaggle</a> to work on some non-finance data to get a feel for the messiness of real-world information. Kaggle’s introductory competition is about predicting which passengers on the Titanic are going to survive using a handful of features, so let’s launch into mucking about. This post follows a “lab book” style and is quite scattered, as I develop ideas about what to do.</p>
<pre class="r"><code># Libraries
library(tidyverse)</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.4.1</code></pre>
<pre class="r"><code>library(stringr)
# Load data
train &lt;- read_csv(&quot;../../data/Titanic/train.csv&quot;)
test &lt;- read_csv(&quot;../../data/Titanic/test.csv&quot;)</code></pre>
<p>Now let’s take a look at the data.</p>
<pre class="r"><code>str(train)</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ...
##  $ Sex        : chr  &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  NA &quot;C85&quot; NA &quot;C123&quot; ...
##  $ Embarked   : chr  &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ...
##  - attr(*, &quot;spec&quot;)=List of 2
##   ..$ cols   :List of 12
##   .. ..$ PassengerId: list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_integer&quot; &quot;collector&quot;
##   .. ..$ Survived   : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_integer&quot; &quot;collector&quot;
##   .. ..$ Pclass     : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_integer&quot; &quot;collector&quot;
##   .. ..$ Name       : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   .. ..$ Sex        : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   .. ..$ Age        : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_double&quot; &quot;collector&quot;
##   .. ..$ SibSp      : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_integer&quot; &quot;collector&quot;
##   .. ..$ Parch      : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_integer&quot; &quot;collector&quot;
##   .. ..$ Ticket     : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   .. ..$ Fare       : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_double&quot; &quot;collector&quot;
##   .. ..$ Cabin      : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   .. ..$ Embarked   : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   ..$ default: list()
##   .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_guess&quot; &quot;collector&quot;
##   ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot;</code></pre>
<p>What I want to do first is add a couple of features. <a href="https://www.datacamp.com/community/open-courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic#gs.UrAze5E">DataCamp’s excellent tutorial</a><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> on this data set uses <code>Title</code> and <code>FamilySize</code>, which I’ll add now. I also thought it might be cool to separate out family names to see if certain families were likely to survive.</p>
<pre class="r"><code># Do it with test and train, don&#39;t want to reconcile them later.
test &lt;- test %&gt;% 
  mutate(Surname = as.factor(word(test$Name, sep = fixed(&quot;,&quot;))),
         Title = word(test$Name, start = 1, sep = fixed(&quot;.&quot;)))

test$Title &lt;- test$Title %&gt;% 
  str_replace(&quot;[.]&quot;, &quot;&quot;) %&gt;%
  word(start = -1) %&gt;% 
  as.factor(.)

# Remove uncommon titles
uncommon &lt;- test %&gt;% 
  group_by(Title) %&gt;%
  count() %&gt;% 
  filter (n &gt;=5) 

levels(test$Title) &lt;- c(levels(test$Title), &quot;Other&quot;)
test$Title[!(test$Title %in% uncommon$Title)] &lt;- as.factor(&quot;Other&quot;)
test$Title &lt;- droplevels.data.frame(test)$Title

# Update embarkment location to factor
test$Embarked &lt;- as.factor(test$Embarked)

# Gender to factor
test$Sex &lt;- as.factor(test$Sex)

test$FamilySize &lt;- test$Parch + test$SibSp + 1

# Change training dataset
train &lt;- train %&gt;% 
  mutate(Surname = as.factor(word(train$Name, sep = fixed(&quot;,&quot;))),
         Title = word(train$Name, start = 1, sep = fixed(&quot;.&quot;)))

train$Title &lt;- train$Title %&gt;% 
  str_replace(&quot;[.]&quot;, &quot;&quot;) %&gt;%
  word(start = -1) %&gt;% 
  as.factor(.)

# Remove uncommon titles
uncommon &lt;- train %&gt;% 
  group_by(Title) %&gt;%
  count() %&gt;% 
  filter (n &gt;=5) 

levels(train$Title) &lt;- c(levels(train$Title), &quot;Other&quot;)
train$Title[!(train$Title %in% uncommon$Title)] &lt;- as.factor(&quot;Other&quot;)
train$Title &lt;- droplevels.data.frame(train)$Title

# Update embarkment location to factor
train$Embarked &lt;- as.factor(train$Embarked)

# Gender to factor
train$Sex &lt;- as.factor(train$Sex)

train$FamilySize &lt;- train$Parch + train$SibSp + 1

summary(train)</code></pre>
<pre><code>##   PassengerId       Survived          Pclass          Name          
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##      Sex           Age            SibSp           Parch       
##  female:314   Min.   : 0.42   Min.   :0.000   Min.   :0.0000  
##  male  :577   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  
##               Median :28.00   Median :0.000   Median :0.0000  
##               Mean   :29.70   Mean   :0.523   Mean   :0.3816  
##               3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  
##               Max.   :80.00   Max.   :8.000   Max.   :6.0000  
##               NA&#39;s   :177                                     
##     Ticket               Fare           Cabin           Embarked  
##  Length:891         Min.   :  0.00   Length:891         C   :168  
##  Class :character   1st Qu.:  7.91   Class :character   Q   : 77  
##  Mode  :character   Median : 14.45   Mode  :character   S   :644  
##                     Mean   : 32.20                      NA&#39;s:  2  
##                     3rd Qu.: 31.00                                
##                     Max.   :512.33                                
##                                                                   
##       Surname       Title       FamilySize    
##  Andersson:  9   Dr    :  7   Min.   : 1.000  
##  Sage     :  7   Master: 40   1st Qu.: 1.000  
##  Carter   :  6   Miss  :182   Median : 1.000  
##  Goodwin  :  6   Mr    :517   Mean   : 1.905  
##  Johnson  :  6   Mrs   :125   3rd Qu.: 2.000  
##  Panula   :  6   Rev   :  6   Max.   :11.000  
##  (Other)  :851   Other : 14</code></pre>
<p>That’s a lot of Anderssons! I wonder if they’re related - let’s check family size by surname.</p>
<pre class="r"><code>train %&gt;% 
  filter(Surname == &quot;Andersson&quot;) %&gt;% 
  select(Name, FamilySize, Survived, SibSp, Parch)</code></pre>
<pre><code>## # A tibble: 9 × 5
##                                                        Name FamilySize
##                                                       &lt;chr&gt;      &lt;dbl&gt;
## 1                               Andersson, Mr. Anders Johan          7
## 2                           Andersson, Miss. Erna Alexandra          7
## 3                         Andersson, Miss. Ellis Anna Maria          7
## 4              Andersson, Mr. August Edvard (&quot;Wennerstrom&quot;)          1
## 5                      Andersson, Miss. Ingeborg Constanzia          7
## 6                         Andersson, Miss. Sigrid Elisabeth          7
## 7 Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)          7
## 8                        Andersson, Miss. Ebba Iris Alfrida          7
## 9                   Andersson, Master. Sigvard Harald Elias          7
## # ... with 3 more variables: Survived &lt;int&gt;, SibSp &lt;int&gt;, Parch &lt;int&gt;</code></pre>
<p>They are all related - except for Erna and August, and <a href="https://titanicstory.wordpress.com/2012/04/04/the-entire-andersson-family-was-lost-on-the-titanic/">the whole family died</a>. This is a really sad data set.</p>
<p>Na’s are the bane of any good analysis, and I want to try to remove some of them. Let’s try to pull out as many as we can.</p>
<pre class="r"><code>clean_age &lt;- function(df) {
  # Turns missing values into the average for the column.
  NA2mean &lt;- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
  df[,&#39;Age&#39;] &lt;- lapply(df[,&#39;Age&#39;], NA2mean)
  return(df)
}

clean_embarkment &lt;- function(df) {
  # The most people embarked from &#39;S&#39;, so I&#39;m just setting
  # the two missing values to &#39;S&#39;.
  df[is.na(df[,&#39;Embarked&#39;]), &#39;Embarked&#39;] &lt;- &#39;S&#39;
  return (df)
}

test &lt;- clean_age(test)
train &lt;- clean_age(train)

test &lt;- clean_embarkment(test)
train &lt;- clean_embarkment(train)</code></pre>
<p>I also want to scale all my features to between 0 and 1, to make processing easier. This also means scrapping the names and turning all numerical values into numbers.</p>
<pre class="r"><code>scaler &lt;- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

cleanse &lt;- function(df) {
  # Remove character variables
  df &lt;- subset(df, select = -c(Name, Ticket, Cabin, Surname))
  
  # If it&#39;s a prediction set or otherwise, break it out
  if(&#39;Survived&#39; %in% colnames(df)){
    id &lt;- select(df, Survived, PassengerId)
    df &lt;- subset(df, select = -c(Survived, PassengerId))
  } else {
    id &lt;- select(df, PassengerId)
    df &lt;- subset(df, select = -c(PassengerId))
  }
  
  # Convert factors to numbers
  factname = c(&#39;Embarked&#39;, &#39;Title&#39;, &#39;Sex&#39;)
  df[,factname] &lt;- lapply(df[,factname] , as.integer)
  
  # Scale variables
  df &lt;- as.tibble(map(df, na.rm = TRUE, scaler))
  
  # Again, separate by labeled or not
  if(&#39;Survived&#39; %in% colnames(id)){
    df$PassengerId &lt;- id$PassengerId
    df$Survived &lt;- id$Survived
  } else {
    df$PassengerId &lt;- id$PassengerId
  }
  
  return(df)
}

train_scl &lt;- cleanse(train)
test_scl &lt;- cleanse(test)

summary(train_scl)</code></pre>
<pre><code>##      Pclass            Sex              Age             SibSp        
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  
##  1st Qu.:0.5000   1st Qu.:0.0000   1st Qu.:0.2712   1st Qu.:0.00000  
##  Median :1.0000   Median :1.0000   Median :0.3679   Median :0.00000  
##  Mean   :0.6543   Mean   :0.6476   Mean   :0.3679   Mean   :0.06538  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.4345   3rd Qu.:0.12500  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  
##      Parch             Fare            Embarked          Title       
##  Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0.01544   1st Qu.:0.5000   1st Qu.:0.3333  
##  Median :0.0000   Median :0.02821   Median :1.0000   Median :0.5000  
##  Mean   :0.0636   Mean   :0.06286   Mean   :0.7682   Mean   :0.4805  
##  3rd Qu.:0.0000   3rd Qu.:0.06051   3rd Qu.:1.0000   3rd Qu.:0.5000  
##  Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  
##    FamilySize       PassengerId       Survived     
##  Min.   :0.00000   Min.   :  1.0   Min.   :0.0000  
##  1st Qu.:0.00000   1st Qu.:223.5   1st Qu.:0.0000  
##  Median :0.00000   Median :446.0   Median :0.0000  
##  Mean   :0.09046   Mean   :446.0   Mean   :0.3838  
##  3rd Qu.:0.10000   3rd Qu.:668.5   3rd Qu.:1.0000  
##  Max.   :1.00000   Max.   :891.0   Max.   :1.0000</code></pre>
<p>Let’s start the analysis with a good old-fashioned logistic regression. Throw everything we’ve got into the pot.</p>
<pre class="r"><code>logit &lt;- glm(Survived ~ Pclass + Sex + Age + SibSp + 
               Parch + Fare + Embarked + Title,
             family = binomial(),
             data = train_scl,
             na.action = na.omit)
summary(logit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked + Title, family = binomial(), data = train_scl, 
##     na.action = na.omit)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6460  -0.5874  -0.4168   0.6330   2.4134  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   4.4881     0.5158   8.701  &lt; 2e-16 ***
## Pclass       -2.1785     0.2789  -7.811 5.66e-15 ***
## Sex          -2.7493     0.1995 -13.783  &lt; 2e-16 ***
## Age          -2.7969     0.6686  -4.183 2.87e-05 ***
## SibSp        -2.7339     0.8755  -3.123  0.00179 ** 
## Parch        -0.5260     0.7104  -0.740  0.45910    
## Fare          0.9325     1.2190   0.765  0.44427    
## Embarked     -0.4345     0.2300  -1.889  0.05891 .  
## Title        -0.8603     0.6672  -1.289  0.19724    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1186.66  on 890  degrees of freedom
## Residual deviance:  783.39  on 882  degrees of freedom
## AIC: 801.39
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Basically what the above tells us is that Pretty much everything decreases your chances of living. You start at a high level (the intercept has a coefficient of 4.6) and decrease from there. Men have a sex of 1, and women have a sex of 0, so being a man is a strong predictor of dying. The strongest indicator by far is age - being older decreases your chances of living. Let’s take the testing data set and predict what we think the results are likely to be.</p>
<pre class="r"><code># Now we predict using the model
threshold &lt;- 0.5
logit_pred &lt;- predict(logit, newdata = test_scl, type = &#39;response&#39;)
hist(logit_pred)</code></pre>
<p><img src="./post/2017-05-12-kaggle-titanic_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>logit_pred &lt;- ifelse(logit_pred &gt; threshold, 1, 0)
# If we&#39;re missing data, predict 0.
logit_pred[is.na(logit_pred)] &lt;- 0
summary(logit_pred)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.3589  1.0000  1.0000</code></pre>
<p>Cool. Let’s export it and see what results we get!</p>
<pre class="r"><code>all &lt;- data.frame(test$PassengerId, logit_pred)
colnames(all) &lt;- c(&quot;PassengerID&quot;, &quot;Survived&quot;)
write_csv(all, &quot;../../data/Titanic/predictions/logit_prediction.csv&quot;)</code></pre>
<div id="neural-networks" class="section level1">
<h1>Neural Networks</h1>
<p>After submitting to Kaggle, this method gives me an accuracy of 76%, worse than the random forest method, which gave 79%. Let me see if a neural network is any better.</p>
<pre class="r"><code>library(neuralnet)
set.seed(91)

# Model the neural network
nnet &lt;- neuralnet(Survived ~ Pclass + Sex + Age + SibSp + 
             Parch + Fare + Embarked + Title,
             hidden = c(2,2,2),
             threshold = 0.035,
             stepmax = 400000000,
             data = train_scl,
             lifesign = &#39;full&#39;)</code></pre>
<pre><code>## hidden: 2, 2, 2    thresh: 0.035    rep: 1/1    steps:      1000 min thresh: 0.1103394579
##                                                             2000 min thresh: 0.05215387676
##                                                             3000 min thresh: 0.04034919191
##                                                             4000 min thresh: 0.03822729544
##                                                             5000 min thresh: 0.03554090753
##                                                             5122 error: 54.00459 time: 3.79 secs</code></pre>
<pre class="r"><code># Predict the test set
nnet.c &lt;- compute(nnet, test_scl[,1:8])
nnet.c &lt;- nnet.c$net.result
hist(nnet.c)</code></pre>
<p><img src="./post/2017-05-12-kaggle-titanic_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>nnet.c &lt;- ifelse(nnet.c &gt; threshold, 1, 0)
# If we&#39;re missing data, predict 0.
nnet.c[is.na(nnet.c)] &lt;- 0

summary(nnet.c)</code></pre>
<pre><code>##        V1           
##  Min.   :0.0000000  
##  1st Qu.:0.0000000  
##  Median :0.0000000  
##  Mean   :0.3755981  
##  3rd Qu.:1.0000000  
##  Max.   :1.0000000</code></pre>
<pre class="r"><code>all &lt;- data.frame(test$PassengerId, nnet.c)
colnames(all) &lt;- c(&quot;PassengerID&quot;, &quot;Survived&quot;)
write_csv(all, &quot;../../data/Titanic/predictions/nnet_prediction.csv&quot;)</code></pre>
<p>I’ve run a lot of other computation on a variety of neural networks, with up to five layers and a variety of node amounts - I only ever matched random forest accuracy with a relatively uncomplicated neural network with three layers of two nodes, at 79%. I suspect that for this data set, predicting survival is best suited to other algorithms.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I used random forests and decision trees as my first submissions. DataCamp’s tutorial does an excellent job explaining the methodology and code, so you can check out the hyperlink above if you’re interested.<a href="#fnref1">↩</a></p></li>
</ol>
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-16-the-rise-of-diy-quants/">
        The Rise of DIY Quants
      </a>
    </h1>

    <span class="post-date">Tue, May 16, 2017</span>

    <p><strong>This post first appeared in the May 2017 edition of the Reading University Investment Society newspaper - you can find a copy of the whole paper <a href="https://www.dropbox.com/s/bn6kukgloae9sr9/May.pdf?dl=0">here</a>.</strong></p>

<p>Modern finance is a constantly evolving field. In the 1970s after Black-Scholes published their seminal paper, derivatives in their current form (for primitive derivatives are found as far back as in ancient Sumerian culture) became common and ubiquitous. Approximately ten years later, we saw the rise of the first quantitative powerhouse Renaissance Technologies, a firm that to this day makes absurd profits. Even today, the rise of passive investing is driven by mathematics as firms construct smart-beta models and strive for mean-variance portfolio optimization.</p>

<p>But the past five years have seen another interesting development - the rise of DIY quants. Idle software engineers, physicists, students, hedge fund managers, and even yours truly have engaged in the construction of systematic and automated trading tools with free tools like Quantopian, Quandl, CloudQuant, Numerai, and many others. These platforms combine free data with programming and software development tools, and many offer tutorials on quantitative investment strategies and techniques.</p>

<p>Take Quantopian for example. In about ten minutes from creating an account, you can have a algorithmized strategy for mean-reversion up and running. You can even hook your algorithm up to two brokers for live trading, either Interactive Brokers or Robinhood. Quantopian offers minute-level equities data, as well as futures prices to trade on, all callable by an easy-to-manage API. Participants on the platform can place their algorithms in competitions, with the prize being capital awarded from Quantopian and its full deployment in the market.</p>

<p>You can couple your Quantopian strategy with Quandl&rsquo;s vast array of core and alternative data, such as satellite imagery, oil tank storage levels, retailer email receipt data, and other custom datasets.  One could imagine a complex algorithm that locates all oil tankers currently shipping, weights the levels in reserve, and prices equities for oil and shipping as well as oil derivatives. In fact, it&rsquo;s likely that some hedge fund somewhere in the world is already doing such a thing.</p>

<p>The world is becoming more accepting of this type of behavior. In some ways, it reflects global culture&rsquo;s growing aversion to high finance, as we shift away from large banks and financial institutions and move to passive investing. In the same way, DIY quants are democratizing previously unavailable services and reclaiming sovereignty over their investments.</p>

<p>This type of investing wouldn&rsquo;t have been able to occur even in the 2000s - few had the skills necessary to write the code, the computers were too slow, the digital infrastructure was lacking, and the data was prohibitively expensive. Python and other high-level languages (often condemned by institutional quantitative investors as being too slow for production) have risen in ubiquity. Even in the late 2000&rsquo;s the difference in speed between Python and a faster language, like C or Fortran, was nearly insurmountable due to the slowness of the computers they ran on. Now, a budding math-geek can run a trading bot on a cloud server from a Chromebook, and utilize thousands of times more computational power than was used to send men to the moon. Lastly, the data is too expensive! In the past, someone who wanted to do what someone in their garage today could do for free would have to pay thousands of dollars to have CDs shipped to their house in order to model the data - now, all you need is a quick API call and you can have world-class data.</p>

<p>I will note that this type of investing cannot compete with true high-frequency trading outfits. Such firms pay millions to co-locate inside broker&rsquo;s facilities to reduce latency, and often have the ability of paying near-zero explicit transaction costs. DIY quantitative investing should work on minute scale or longer. I personally have a portfolio management algorithm that simple performs mean-variance optimization and rebalances once a month, so I don&rsquo;t have to think about my investments.</p>

<p>If you think you&rsquo;d like to be a cog in the efficient market, go check out any of the quantitative investment sites! Try and find a signal that nobody else has found. If you do, give me a call – maybe I’ll invest.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-13-github-pages/">
        Github Pages
      </a>
    </h1>

    <span class="post-date">Sat, May 13, 2017</span>

    <p>I recently moved my site away from <a href="zeit.co">zeit</a>, and migrated it to <a href="pages.github.com">GitHub Pages</a>. It&rsquo;s much easier to maintain the site with git than with zeit&rsquo;s <code>now</code> feature, which is a little too high-powered for my tases.</p>

<p>I&rsquo;ve also changed the theme to a modified version of <a href="https://github.com/damiencaselli/paperback">paperback</a>. The colors were originally all sepia tone, and I liked the current color scheme and typography.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-01-credit-risk-part-1/">
        Credit Risk &amp; Logistic Regression
      </a>
    </h1>

    <span class="post-date">Mon, May 1, 2017</span>

    <p>I thought I’d do a little bit of analysis to showcase some credit risk analysis, using <strong>logistic regression</strong>. I’ve pulled this sample loan data from a DataCamp course on <a href="https://www.datacamp.com/courses/introduction-to-credit-risk-modeling-in-r">credit risk</a>. It’s a cool class, you should check it out if you have time. In a later post, I will try this same analysis with a neural network to see if it has better predictive capabilities.</p>
<p>Here’s a look at the data. You can see we have all kinds of valuable information we can use in determining whether someone is likely to default - people with high rates and low credit scores are more likely to default, while people who own their homes and have had long term employment grades are less likely to default. We also have a column called <code>loan_status</code>, which is a boolean value indicating whether that particular borrower has defaulted.</p>
<pre class="r"><code>head(data)</code></pre>
<pre><code>##   loan_status loan_amnt int_rate grade emp_length home_ownership
## 1           0      5000    10.65     B         10           RENT
## 2           0      2400       NA     C         25           RENT
## 3           0     10000    13.49     C         13           RENT
## 4           0      5000       NA     A          3           RENT
## 5           0      3000       NA     E          9           RENT
## 6           0     12000    12.69     B         11            OWN
##   annual_inc age
## 1      24000  33
## 2      12252  31
## 3      49200  24
## 4      36000  39
## 5      48000  24
## 6      75000  28</code></pre>
<div id="logistic-regression" class="section level1">
<h1>Logistic Regression</h1>
<p>If you’re unfamiliar with logistic regression, that’s alright. What it does (in broad strokes) is allow you to predict a value <strong>between 1 and 0</strong>, and provide you with a degree of certainty. For example, if we ran a logistic regression on a bunch of variables, and then found relevant coefficients, we could use the features of a particular borrower to determine what level of risk they have. A lender could take appropriate measures with someone with a very low chance (0.02) of default by providing them with lower rates, or by simply not lending to someone with a very high chance of default (0.99).</p>
<p>Now we should tidy up some of the data and get rid of any rows with <code>NA</code>s. There are more efficient ways of dealing with this problem, but for our purposes we only lose about 3,000 observations, bringing us to about 25,000 observations. This is enough by most measures to build a rudimentary predictive model.</p>
<pre class="r"><code>#Filter out rows with any NAs.
data &lt;- data[complete.cases(data),]
head(data)</code></pre>
<pre><code>##   loan_status loan_amnt int_rate grade emp_length home_ownership
## 1           0      5000    10.65     B         10           RENT
## 3           0     10000    13.49     C         13           RENT
## 6           0     12000    12.69     B         11            OWN
## 7           1      9000    13.49     C          0           RENT
## 8           0      3000     9.91     B          3           RENT
## 9           1     10000    10.65     B          3           RENT
##   annual_inc age
## 1      24000  33
## 3      49200  24
## 6      75000  28
## 7      30000  22
## 8      15000  22
## 9     100000  28</code></pre>
<p>Let’s split our dataset into two pieces, 60/40. This allows us to design a model with the 60% dataset and test it on the 40% dataset. If I was performing more exploratory analysis, I’d split the 40 in half, one for messing around in and the other for testing, but I’m pretty much skipping right to the modeling for now.</p>
<pre class="r"><code>set.seed(9090)
bound &lt;- floor((nrow(data)/4)*3)
data &lt;- data[sample(nrow(data)),]
train &lt;- data[1:bound,]
test &lt;- data[(bound+1):nrow(data),]</code></pre>
<p>Now to the preliminary model. We can use the R’s built-in functions to handle this. See below a summary of the output.</p>
<pre class="r"><code>model &lt;- glm(loan_status ~., family = binomial(link=&#39;logit&#39;), data = train)
summary(model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_status ~ ., family = binomial(link = &quot;logit&quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1467  -0.5358  -0.4416  -0.3374   3.3591  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         -3.006e+00  2.151e-01 -13.976  &lt; 2e-16 ***
## loan_amnt           -2.241e-06  4.141e-06  -0.541  0.58839    
## int_rate             9.058e-02  2.301e-02   3.936 8.29e-05 ***
## gradeB               3.338e-01  1.084e-01   3.080  0.00207 ** 
## gradeC               4.932e-01  1.569e-01   3.143  0.00167 ** 
## gradeD               5.809e-01  1.995e-01   2.911  0.00360 ** 
## gradeE               5.946e-01  2.505e-01   2.374  0.01760 *  
## gradeF               8.550e-01  3.343e-01   2.558  0.01053 *  
## gradeG               1.242e+00  4.367e-01   2.844  0.00446 ** 
## emp_length           5.405e-03  3.655e-03   1.479  0.13920    
## home_ownershipOTHER  7.172e-01  3.331e-01   2.153  0.03130 *  
## home_ownershipOWN   -1.000e-01  9.608e-02  -1.041  0.29795    
## home_ownershipRENT  -1.647e-02  5.329e-02  -0.309  0.75723    
## annual_inc          -5.325e-06  7.722e-07  -6.896 5.36e-12 ***
## age                 -5.048e-03  3.911e-03  -1.291  0.19685    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13297  on 19177  degrees of freedom
## Residual deviance: 12761  on 19163  degrees of freedom
## AIC: 12791
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Now, we test our accuracy. How well does our model predict loan status? The below code spits out the accuracy when we test our model on the <code>test</code> dataframe, and we get a result of 89%. Not bad!</p>
<pre class="r"><code>fit &lt;- predict(model, newdata = test, type = &quot;response&quot;)
fit &lt;- ifelse(fit &gt; 0.5, 1, 0)
error &lt;- mean(fit != test$loan_status)
print(paste( &quot;Accuracy is: &quot;, 1 - error))</code></pre>
<pre><code>## [1] &quot;Accuracy is:  0.893477240732051&quot;</code></pre>
<p>This <a href="https://datascienceplus.com/perform-logistic-regression-in-r/">lovely blogpost</a> recommends plotting the true positive vs. false positives. The code for that is below. The plot shows a nearly straight line, which means we really aren’t especially predictive - the output at the bottom of 0.66 similarly shows the same. We’d like this value to be closer to one to indicate predictive ability.</p>
<pre class="r"><code>library(ROCR)
p &lt;- predict(model, newdata = test, type = &quot;response&quot;)
pr &lt;- prediction(p, test$loan_status)
prf &lt;- performance(pr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)
plot(prf)</code></pre>
<p><img src="./post/2017-05-01-credit-risk-part-1_files/figure-html/unnamed-chunk-7-1.png" width="1152" /></p>
<pre class="r"><code>auc &lt;- performance(pr, measure=&quot;auc&quot;)
auc &lt;- auc@y.values[[1]]
auc</code></pre>
<pre><code>## [1] 0.6606138</code></pre>
<p>But we’ve skipped a couple of important steps in modeling. The model summary shows a litany of variables that really aren’t that predictive; we need to take them out. We’re going to leave anything with a <code>.</code> or any number of asterisks (<code>*</code>) in, because they are significant. A 10% significant will suffice for me.</p>
<pre class="r"><code>model2 &lt;- glm(loan_status ~ int_rate + grade + emp_length +
                (home_ownership==&#39;OTHER&#39;) + annual_inc + age, 
              family = binomial(link=&#39;logit&#39;), 
              data = train)
summary(model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_status ~ int_rate + grade + emp_length + (home_ownership == 
##     &quot;OTHER&quot;) + annual_inc + age, family = binomial(link = &quot;logit&quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1355  -0.5361  -0.4424  -0.3373   3.3724  
## 
## Coefficients:
##                                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                   -3.030e+00  2.117e-01 -14.309  &lt; 2e-16 ***
## int_rate                       8.976e-02  2.300e-02   3.903 9.51e-05 ***
## gradeB                         3.324e-01  1.082e-01   3.071  0.00213 ** 
## gradeC                         4.950e-01  1.569e-01   3.155  0.00161 ** 
## gradeD                         5.807e-01  1.994e-01   2.912  0.00359 ** 
## gradeE                         5.941e-01  2.501e-01   2.375  0.01755 *  
## gradeF                         8.522e-01  3.338e-01   2.553  0.01067 *  
## gradeG                         1.230e+00  4.358e-01   2.822  0.00478 ** 
## emp_length                     5.473e-03  3.593e-03   1.523  0.12767    
## home_ownership == &quot;OTHER&quot;TRUE  7.319e-01  3.316e-01   2.208  0.02728 *  
## annual_inc                    -5.386e-06  6.847e-07  -7.867 3.65e-15 ***
## age                           -5.070e-03  3.911e-03  -1.296  0.19490    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13297  on 19177  degrees of freedom
## Residual deviance: 12762  on 19166  degrees of freedom
## AIC: 12786
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Now to test accuracy. It basically has yielded no meaningful change in predictive ability - but then again, that’s hard to do. All we’ve done is create a more parsimonious model in line with current thinking in statistics and econometrics.</p>
<pre class="r"><code>fit &lt;- predict(model2, newdata = test, type = &quot;response&quot;)
fit &lt;- ifelse(fit &gt; 0.5, 1, 0)
error &lt;- mean(fit != test$loan_status)
print(paste( &quot;Accuracy is: &quot;, 1 - error))</code></pre>
<pre><code>## [1] &quot;Accuracy is:  0.893477240732051&quot;</code></pre>
<p>Finally, we plot the cure we showed before. Again, no real difference, but we can feel better that we have a smaller model with less “junk” floating around.</p>
<pre class="r"><code>library(ROCR)
p &lt;- predict(model2, newdata = test, type = &quot;response&quot;)
pr &lt;- prediction(p, test$loan_status)
prf &lt;- performance(pr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)
plot(prf)</code></pre>
<p><img src="./post/2017-05-01-credit-risk-part-1_files/figure-html/unnamed-chunk-10-1.png" width="1152" /></p>
<pre class="r"><code>auc &lt;- performance(pr, measure=&quot;auc&quot;)
auc &lt;- auc@y.values[[1]]
auc</code></pre>
<pre><code>## [1] 0.661003</code></pre>
<p>Thanks for taking the time to read this post. Check out later posts where I use neural networks to look at this same dataset. It’ll be fun for the whole family!</p>
<hr />
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-01-i-made-a-newspaper/">
        I made a newspaper
      </a>
    </h1>

    <span class="post-date">Mon, May 1, 2017</span>

    <p>A couple of my friends here at the University of Reading came up with an excellent idea to start a financial newspaper. The concept was that the society<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup> would collect articles on investing and other things finance and publish them in a monthly paper. Being one of the few native English speakers with an interest in editing and journalism, I was selected to run the paper as Chief Editor.</p>

<p>I&rsquo;m very proud of the results, you can find the finished version of the April edition <a href="https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0">here</a>. I wrote a couple articles, about the <a href="./2017/04/16/2017-05-01-federal-funds-hike/">federal funds hike</a> and <a href="./2017/04/16/2017-05-01-iex-s-crumbling-quote/">IEX</a>. I also wrote a lovely editors note, available in the full edition.</p>

<p>We intend to release at least a few more editions before we all graduate, and we&rsquo;ll see how they turn out.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">That&rsquo;s what they call &ldquo;clubs&rdquo; in the UK.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-04-30-hugo-and-blogdown/">
        Introduction
      </a>
    </h1>

    <span class="post-date">Sun, Apr 30, 2017</span>

    <p>Hello! I&rsquo;ve just finished building this site with Hugo and <a href="https://github.com/rstudio/blogdown"><code>blogdown</code></a>, an excellent R package built up by the folks at RStudio. The first iteration of this site is being hosted by <a href="https://zeit.co/now">zeit</a>, a super interesting cloud company. Deployment takes about a minute, unlike the absolute nightmare that is Google Cloud. They take Dockerfiles as well, something that I haven&rsquo;t had any experience with until lately.</p>

<p>As this is the first blog post here, I just thought I&rsquo;d lay out a bit of why I built this. Over the years I have accumulated a strange variety of skills, like:</p>

<ul>
<li>Market Microstructure</li>
<li><a href="https://www.youtube.com/channel/UCTcsdFvTCGyJ8vSp3iHuTyg">Improvisational Piano</a></li>
<li>Corporate Finance</li>
<li>Econometrics</li>
<li>Programming</li>
<li>Lighting Design</li>
<li>Carpentry</li>
<li>Bookbinding</li>
<li>Machine Learning</li>
</ul>

<p>Some of those are finance, which is easily my favorite thing to study. At some point I&rsquo;d lke to go back to graduate school and study for a PhD, but at the moment, I&rsquo;m tired of living on student loans. Others are related to my previous career as a professional stagehand, lighting designer, entertainment electrician, and wrench monkey. The piano thing just sort of happened one day.</p>

<p>My undergraduate degree is in theater arts, and my master&rsquo;s degree is in corporate finance, so it has always been very difficult to demonstrate to employers that I am (i) good at what they&rsquo;re hiring for and (ii) interested in the subject.</p>

<p>To that end, I hope to produce a series of posts exploring a handful of the things I enjoy. More coming soon.</p>

<hr />

<p>$$ PV_n=FV_n\left(1+r\right)^n $$</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-01-federal-funds-hike/">
        Federal Funds Hike
      </a>
    </h1>

    <span class="post-date">Sun, Apr 16, 2017</span>

    <p><strong>This article first appeared in the Reading University Investment Society&rsquo;s  <a href="https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0">April edition</a> on April 16th, 2017.</strong></p>

<p>Last month, the Federal Reserve raised the short-term interest rate target range by 0.25%, to a range of 0.75% to 1%. This is in line with the central bank’s “slow and steady” approach to rate increases after a decade of historically low interest rates, and is the third such rate increase since June of 2006. The Federal Reserve has further signaled its intentions to complete at least two more quarter-point increases during 2017, with the median projection among Fed officials sitting at 1.4% by year-end. This reflects a positive outlook for the US economy, as wage and job growth continue to display robustness in the face of a volatile global market place. But how likely are rates to continue rising?</p>

<p>The impending and steady rate increases has caused a dramatic rise in bond issuance, as investors pile in to low rate debt securities. Companies sold a record-high amount of bonds in March to the tune of $414 billion, the Wall Street Journal reported earlier this week. This signals that the market believes the Fed’s intentions to raise rates, lending further evidence to the certainty of rate increases.</p>

<p>Perhaps a bigger indicator that rates will continue to raise steadily – and may even have an accelerated pace – is the split between US monetary and fiscal policy. As the Federal Reserve undertakes monetary tightening, the Trump administration and the Republican majority have indicated expansionary fiscal policy in preliminary budgets. This includes infrastructure spending, which Trump has stated could be as much as $1 trillion, as well as increased military and defense spending. Each of these are certain to increase money supply in the economy and drive inflation expectations, which may increase Federal Reserve rate pacing.</p>

<p>Thus, the likelihood of continued rate hikes seems high and potentially increasing. This could signal bad news for both personal income investors who will see the market value of their holdings drop, and institutional investors with actively managed bond funds who will struggle to find profitable investment opportunities. The global impacts will be significant, as the federal funds rate is tightly linked with other interest rates. In fact, one day after the Federal Reserve raised its rates a quarter percent, the People’s Bank of China kept its rates in lock step with its American counterpart.</p>

<p>The markets are headed towards the normalcy previous generations experienced, with 30-year interest rates above 3% and federal fund rate changes that nobody noticed. Now, when a rate hike is expected, it is talked about endlessly weeks before and after. This is good for savers and pension funds, but bond investors are going to suffer during the transition from almost-free money to rates I haven’t seen since I was 12.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./post/2017-05-01-iex-s-crumbling-quote/">
        IEX&#39;s Crumbling Quote
      </a>
    </h1>

    <span class="post-date">Sun, Apr 16, 2017</span>

    <p><strong>This article first appeared in the Reading University Investment Society&rsquo;s  <a href="https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0">April edition</a> on April 16th, 2017.</strong></p>

<p>The Investor’s Exchange, also referred to as IEX, came to prominence in 2014 after Michael Lewis’ book Flash Boys was released. IEX was designed as an exchange to protect lumbering buy side firms from high-frequency traders and other nimble market actors.</p>

<p>Their first innovation was to treat what is referred to as stale quote arbitrage. In this type of arbitrage, a fast trader could detect when the best bid or offer had changed on one exchange, and then immediately go to another exchange and pick off a midpoint quote before the midpoint updated. To solve this problem, IEX famously built a box in a warehouse with a very long coil of fiber optic cable designed to introduce a 350-microsecond delay into quote updates. Any trader who could detect a midpoint quote before it updated on another exchange could not immediately post an order on IEX, because it would suffer a delay while the exchange updated the midpoint quotes.</p>

<p>IEX’s dedication to ensuring low transaction costs to its buy side clients is ceaseless, and it has published a new working paper about dealing with another type of arbitrage, which it calls “crumbling quote arbitrage”. The paper, written by Allison Bishop is entitled “<a href="https://iextrading.com/docs/The%20Evolution%20of%20the%20Crumbling%20Quote%20Signal.pdf?utm_medium=email&amp;utm_source=newsletter&amp;utm_term=170411&amp;utm_campaign=moneystuff">The Evolution of the Crumbling Quote Signal</a>” and details the exchange’s cunning way of dealing with an interesting problem.</p>

<p>Crumbling quotes refer to when the number of exchanges on the national best bid or offer is eroding over time as the market eats up posted volumes. This can and does happen in legitimate market trading, but some predatory firms may intentionally claim all posted volumes and have in place an order to take advantage of IEX’s delay.</p>

<p>The working paper is fascinating, as Bishop demonstrates IEX’s approach to predicting when a crumbling quote is likely to occur, and allowing orders pegged to the midpoint to exercise discretion when IEX’s “signal” is on. Their new rule was approved by the SEC for use in production, though it has yet to be fully installed exchange-wide.</p>

<p>IEX has been at the forefront of protecting its client’s interests, and this is simply the next step at the frontier of combating high frequency traders and other technologically advanced market participants. IEX has a powerful advantage over market predators, and that is that it – along with other exchanges – has access to both a greater quantity and quality of data that traders would usually have to pay thousands of dollars a month for. It will certainly be interesting to watch the development and competition between IEX and those it tries to protect its clients from.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="./about/">
        About
      </a>
    </h1>

    <span class="post-date">Thu, May 5, 2016</span>

    

<p><strong>I&rsquo;m looking for work! If you need a financial analyst, data scientist, or just a quick learner, you can contact me at <a href="mailto:cpfiffer@gmail.com">cpfiffer@gmail.com</a>.</strong></p>

<p><img src="https://lh3.googleusercontent.com/EjfH_u4TJChj5ysOWJfYIvILZlrO5a920MapM3rzCKRm6VPZ6uQMwrsuFaTNGQQ1kZmkcEH6qDktF6IXNVScMaT4iRXYQUH-_SkO6iP1fXjY5ko9_mSKgPDohd9AhAfZHC-RrPlgj_-e_bkSlBcjUeZGAYNbUKUwOKL0_Bgq1WhosIP7JeaEhVpVvVqNrlFhdT35AiK3nUeg2YyXUd9bcYeQovbOUa6AaKtxYOkQybhAd0soUQwVnu0yRgd_drwXejWSLqkaRmLbNYAURb4NAQZOC9EblRAIMwWO5RtvpBz6MptI3PtHvOd0bhc3z0SsuyQauBhi4dF7pjxOrdZjnZMn0XhvH3pBGcEix2oR4tprFFzzCTsTxWW-2X9Xbgneem9lGr_qIYDc2ArlRGxzH0_hyW-k20a70ZDTDkj5fYQXvb0z3ScRlzdVrvaW2bsFLMXMQCn3Au6Inc0OnlOUYTX8Nx2KHbUUxSWhdRDoTmeq-p6nDoaZoEjXU28J8lvmBzjdR4d2oPk3aT-zIg6VC9i7pIRGZ55zASs4lQ0sd4nHdgFMPIfWUvSWs65x37vqpEZ7YHRpLCVZ8kXIohKTJsA6pzqPyl5tv9YZCCVfaq4zhlsO3boza8Y9iBi_0Teo-uXG6_9joEWUGDP0tjTcrmx4vmBJqLyFgLP3FvkJK7c=w944-h1257-no" alt="Me" /></p>

<p>I&rsquo;m a graduate student in finance. I also play with code, and occasionally piano. This is a little website I built, partly as a portfolio of some of the stuff I can do, but also because it&rsquo;s fun.</p>

<p>I have a master&rsquo;s in finance from the <a href="http://www.icmacentre.ac.uk/">ICMA Centre</a> at the <a href="http://www.henley.ac.uk/">Henley Business School</a> (which itself is part of the <a href="https://www.reading.ac.uk/">University of Reading</a><sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>) Nominally, my degree is in corporate finance, though I&rsquo;ve studied most business disciplines. I tend to use a lot of programming to handle the challenges in finance, and over time I&rsquo;ll populate this blog with some posts showcasing some of my best work.</p>

<hr />

<h2 id="skills">Skills</h2>

<h3 id="finance">Finance</h3>

<ul>
<li>Liquidity Risk</li>
<li>Algorithmic, Systematic, and High Frequency Trading</li>
<li>Market Making</li>
<li>Corporate Finance</li>
<li>Equity Valuation</li>
<li>Market Research</li>
<li>Mergers &amp; Acquisitions</li>
<li>Bloomberg Terminal</li>
<li>Reuters Eikon</li>
</ul>

<h3 id="programming">Programming</h3>

<ul>
<li>R</li>
<li>Julia</li>
<li>VBA</li>
<li>Python</li>
<li>Matlab<sup class="footnote-ref" id="fnref:2"><a rel="footnote" href="#fn:2">2</a></sup></li>
<li>SQL</li>
</ul>

<h3 id="data-science">Data Science</h3>

<ul>
<li>Neural Networks</li>
<li>Forecasting</li>
<li>Nonlinear Modeling</li>
<li>Econometric Theory</li>
<li>Principal Component Analysis</li>
<li>Exploratory Analysis</li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">Yes, this is confusing, and it makes typing up resumes very difficult.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
<li id="fn:2">I say Matlab, but most of my experience has been with the GNU version called <a href="https://www.gnu.org/software/octave/">Octave</a>. The syntax and functions are pretty much identical &ndash; though Octave is <a href="https://julialang.org/benchmarks/">far slower</a> than Matlab and everything else.
 <a class="footnote-return" href="#fnref:2"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
  
</div>
</div>

  </body>
</html>
