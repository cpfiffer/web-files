<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cameron&#39;s Blog</title>
    <link>/</link>
    <description>Recent content on Cameron&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Fri, 25 Dec 2020 22:06:48 -0800</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>2020 Wrap-up</title>
      <link>/2020/12/25/2020-wrap-up/</link>
      <pubDate>Fri, 25 Dec 2020 22:06:48 -0800</pubDate>
      
      <guid>/2020/12/25/2020-wrap-up/</guid>
      <description>&lt;p&gt;I like to try to write a year-end summary. I missed 2019 last year for various reasons,
but this year feels like an especially important year.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s no secret that this year was more or less utter garbage. COVID-19 is the largest
contributor by far, but there have been other issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;My home state of Oregon suffered the most devastating fire season
&lt;a href=&#34;https://en.wikipedia.org/wiki/2020_Oregon_wildfires&#34;&gt;in recorded history&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The (continued) police killings of people. Lots of people.&lt;/li&gt;
&lt;li&gt;Systematic mismanagement of institutions that really need to be managed well.
I&amp;rsquo;m looking at you, US government.&lt;/li&gt;
&lt;li&gt;The loud and stupid in my country are getting louder and stupider.&lt;/li&gt;
&lt;li&gt;Nobody gives a shit about truth or facts or objectivity, which sucks.&lt;/li&gt;
&lt;li&gt;Donald.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Among many others.&lt;/p&gt;

&lt;p&gt;But for me, this year was perhaps the worst I have experienced in my brief 27 years of life.
Certainly there were high points, drowned as they were in stress, but they were too few
and too short to offset the generalized shittiness of the year.&lt;/p&gt;

&lt;p&gt;First and worst, I am getting a divorce after 7 years of marriage. I don&amp;rsquo;t want to talk too
much about the details because they are nobody&amp;rsquo;s business but my wife and I, but
it is an important life change that I need people to know about. I married
young (20), so everyone who has met me knows me as a married man. It is a large part
of my identity and I have been having difficulty in the past few months with trying to
replace that part of me with something else.&lt;/p&gt;

&lt;p&gt;I just want some folks to know so that I don&amp;rsquo;t have explain why I don&amp;rsquo;t seem to have
any dogs anymore, or why I&amp;rsquo;m in a new apartment, or where anything is. It&amp;rsquo;s a big change.&lt;/p&gt;

&lt;p&gt;Second, I have been struggling with depression, which is more or less new to me in any kind of
significant way. I was fortunate enough to be free from depression and other
mental disorders &amp;ndash; so fortunate I thought it due to my strength of will (like a jackass).
Of course that is not true, however. Depression goes where it may and you have to deal
with it when it shows up. I am indeed dealing with it, but it is a slow process made slower
by COVID-19.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;dep&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;dep&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;
Lots of people who have not lived with some kind of mental disorder like anxiety or
depression are often mistaken that it&amp;rsquo;s that person&amp;rsquo;s fault. It&amp;rsquo;s not. Get over yourself.
Humans are not weird-ass science robots who control their feelings. There&amp;rsquo;s lots of complex
stuff happening that you don&amp;rsquo;t know jack-shit about. Don&amp;rsquo;t be a dick if someone says they are
depressed or anxious. Just help.
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Both of these two things have been the defining themes of my life for the year. Not only
have I lost a cornerstone of my identity, but I also lack the motivation to find a new
one. Nearly everything I do has suffered, from my academic research to my work on Turing,
to various other projects that I tend to pick up here and there.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m okay though, before anyone is too worried. It&amp;rsquo;s been an awful year. Sometimes
those happen. You just work through them and hope to come out on the other side a little dinged
but still beautiful and proud.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s hoping to a path upwards and onwards. Here&amp;rsquo;s to hoping for a better year, a better me,
a better you, a better government, a better world. Here&amp;rsquo;s to a better tomorrow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian finance papers</title>
      <link>/2020/04/19/bayesian-finance-papers/</link>
      <pubDate>Sun, 19 Apr 2020 20:58:31 -0700</pubDate>
      
      <guid>/2020/04/19/bayesian-finance-papers/</guid>
      <description>

&lt;p&gt;A list of Bayesian finance papers I&amp;rsquo;ve noticed. These are mostly sourced from the financial economics literature, and primarily so from the top three journals (&lt;em&gt;Journal of Finance&lt;/em&gt;, &lt;em&gt;Review of Financial Studies&lt;/em&gt;, and the &lt;em&gt;Journal of Financial Economics&lt;/em&gt;). I exclude theoretical papers because the Bayesian component is usually not the most interesting part. I favor empirical papers that use some kind of Bayesian method. Suggestions welcome!&lt;/p&gt;

&lt;p&gt;Note: a &lt;code&gt;.bib&lt;/code&gt; file can be found &lt;a href=&#34;/notes/bayes.bib&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;

&lt;p&gt;Anderson, Evan W., and Ai-Ru (Meg) Cheng. “Robust Bayesian Portfolio Choices.” The Review of Financial Studies 29, no. 5 (May 1, 2016): 1330–75. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhw001&#34;&gt;https://doi.org/10.1093/rfs/hhw001&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Avramov, Doron. “Stock Return Predictability and Asset Pricing Models.” The Review of Financial Studies 17, no. 3 (July 1, 2004): 699–738. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhg059&#34;&gt;https://doi.org/10.1093/rfs/hhg059&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. “Stock Return Predictability and Model Uncertainty.” Journal of Financial Economics 64, no. 3 (June 1, 2002): 423–58. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(02)00131-9&#34;&gt;https://doi.org/10.1016/S0304-405X(02)00131-9&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Baks, Klaas P., Andrew Metrick, and Jessica Wachter. “Should Investors Avoid All Actively Managed Mutual Funds? A Study in Bayesian Performance Evaluation.” The Journal of Finance 56, no. 1 (2001): 45–85. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00319&#34;&gt;https://doi.org/10.1111/0022-1082.00319&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Barillas, Francisco, and Jay Shanken. “Comparing Asset Pricing Models.” The Journal of Finance 73, no. 2 (2018): 715–54. &lt;a href=&#34;https://doi.org/10.1111/jofi.12607&#34;&gt;https://doi.org/10.1111/jofi.12607&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bates, David S. “Maximum Likelihood Estimation of Latent Affine Processes.” The Review of Financial Studies 19, no. 3 (October 1, 2006): 909–65. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhj022&#34;&gt;https://doi.org/10.1093/rfs/hhj022&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bollerslev, Tim, Benjamin Hood, John Huss, and Lasse Heje Pedersen. “Risk Everywhere: Modeling and Managing Volatility.” The Review of Financial Studies 31, no. 7 (July 1, 2018): 2729–73. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy041&#34;&gt;https://doi.org/10.1093/rfs/hhy041&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Brav, Alon. “Inference in Long-Horizon Event Studies: A Bayesian Approach with Application to Initial Public Offerings.” The Journal of Finance 55, no. 5 (2000): 1979–2016. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00279&#34;&gt;https://doi.org/10.1111/0022-1082.00279&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Buehlmaier, Matthias M. M., and Toni M. Whited. “Are Financial Constraints Priced? Evidence from Textual Analysis.” The Review of Financial Studies 31, no. 7 (July 1, 2018): 2693–2728. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy007&#34;&gt;https://doi.org/10.1093/rfs/hhy007&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bulkley, George, and Paolo Giordani. “Structural Breaks, Parameter Uncertainty, and Term Structure Puzzles.” Journal of Financial Economics 102, no. 1 (October 1, 2011): 222–32. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2011.05.009&#34;&gt;https://doi.org/10.1016/j.jfineco.2011.05.009&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Busse, Jeffrey A., and Paul J. Irvine. “Bayesian Alphas and Mutual Fund Persistence.” The Journal of Finance 61, no. 5 (2006): 2251–88. &lt;a href=&#34;https://doi.org/10.1111/j.1540-6261.2006.01057.x&#34;&gt;https://doi.org/10.1111/j.1540-6261.2006.01057.x&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cavagnaro, Daniel R., Berk A. Sensoy, Yingdi Wang, and Michael S. Weisbach. “Measuring Institutional Investors’ Skill at Making Private Equity Investments.” The Journal of Finance 74, no. 6 (2019): 3089–3134. &lt;a href=&#34;https://doi.org/10.1111/jofi.12783&#34;&gt;https://doi.org/10.1111/jofi.12783&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cremers, K. J. Martijn. “Stock Return Predictability: A Bayesian Model Selection Perspective.” The Review of Financial Studies 15, no. 4 (July 1, 2002): 1223–49. &lt;a href=&#34;https://doi.org/10.1093/rfs/15.4.1223&#34;&gt;https://doi.org/10.1093/rfs/15.4.1223&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dai, Qiang, Kenneth J. Singleton, and Wei Yang. “Regime Shifts in a Dynamic Term Structure Model of U.S. Treasury Bond Yields.” The Review of Financial Studies 20, no. 5 (September 1, 2007): 1669–1706. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhm021&#34;&gt;https://doi.org/10.1093/rfs/hhm021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dangl, Thomas, and Michael Halling. “Predictive Regressions with Time-Varying Coefficients.” Journal of Financial Economics 106, no. 1 (October 1, 2012): 157–81. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2012.04.003&#34;&gt;https://doi.org/10.1016/j.jfineco.2012.04.003&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Durham, Garland B. “SV Mixture Models with Application to S&amp;amp;P 500 Index Returns.” Journal of Financial Economics 85, no. 3 (September 1, 2007): 822–56. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2006.06.005&#34;&gt;https://doi.org/10.1016/j.jfineco.2006.06.005&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Easley, David, Robert F. Engle, Maureen O’Hara, and Liuren Wu. “Time-Varying Arrival Rates of Informed and Uninformed Trades.” Journal of Financial Econometrics 6, no. 2 (March 1, 2008): 171–207. &lt;a href=&#34;https://doi.org/10.1093/jjfinec/nbn003&#34;&gt;https://doi.org/10.1093/jjfinec/nbn003&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Easley, David, Marcos Lopez de Prado, and Maureen O’Hara. “Discerning Information from Trade Data.” Journal of Financial Economics 120, no. 2 (May 1, 2016): 269–85. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2016.01.018&#34;&gt;https://doi.org/10.1016/j.jfineco.2016.01.018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Frank, Murray Z., and Ali Sanati. “How Does the Stock Market Absorb Shocks?” Journal of Financial Economics 129, no. 1 (July 1, 2018): 136–53. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2018.04.002&#34;&gt;https://doi.org/10.1016/j.jfineco.2018.04.002&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Fulop, Andras, Junye Li, and Jun Yu. “Self-Exciting Jumps, Learning, and Asset Pricing Implications.” The Review of Financial Studies 28, no. 3 (March 1, 2015): 876–912. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhu078&#34;&gt;https://doi.org/10.1093/rfs/hhu078&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gallant, A. Ronald, Mohammad R Jahan-Parvar, and Hening Liu. “Does Smooth Ambiguity Matter for Asset Pricing?” The Review of Financial Studies 32, no. 9 (September 1, 2019): 3617–66. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy118&#34;&gt;https://doi.org/10.1093/rfs/hhy118&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Garlappi, Lorenzo, Raman Uppal, and Tan Wang. “Portfolio Selection with Parameter and Model Uncertainty: A Multi-Prior Approach.” The Review of Financial Studies 20, no. 1 (January 1, 2007): 41–81. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhl003&#34;&gt;https://doi.org/10.1093/rfs/hhl003&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Geweke, John, and Guofu Zhou. “Measuring the Pricing Error of the Arbitrage Pricing Theory.” The Review of Financial Studies 9, no. 2 (April 1, 1996): 557–87. &lt;a href=&#34;https://doi.org/10.1093/rfs/9.2.557&#34;&gt;https://doi.org/10.1093/rfs/9.2.557&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gray, Stephen F. “Modeling the Conditional Distribution of Interest Rates as a Regime-Switching Process.” Journal of Financial Economics 42, no. 1 (September 1, 1996): 27–62. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(96)00875-6&#34;&gt;https://doi.org/10.1016/0304-405X(96)00875-6&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Han, Yufeng. “Asset Allocation with a High Dimensional Latent Factor Stochastic Volatility Model.” The Review of Financial Studies 19, no. 1 (March 1, 2006): 237–71. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhj002&#34;&gt;https://doi.org/10.1093/rfs/hhj002&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Harvey, Campbell R., and Yan Liu. “Cross-Sectional Alpha Dispersion and Performance Evaluation.” Journal of Financial Economics 134, no. 2 (November 1, 2019): 273–96. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2019.04.005&#34;&gt;https://doi.org/10.1016/j.jfineco.2019.04.005&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. “Detecting Repeatable Performance.” The Review of Financial Studies 31, no. 7 (July 1, 2018): 2499–2552. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy014&#34;&gt;https://doi.org/10.1093/rfs/hhy014&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Harvey, Campbell R., Yan Liu, and Heqing Zhu. “… and the Cross-Section of Expected Returns.” The Review of Financial Studies 29, no. 1 (January 1, 2016): 5–68. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhv059&#34;&gt;https://doi.org/10.1093/rfs/hhv059&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Harvey, Campbell R, and Guofu Zhou. “Bayesian Inference in Asset Pricing Tests.” Journal of Financial Economics 26, no. 2 (August 1, 1990): 221–54. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(90)90004-J&#34;&gt;https://doi.org/10.1016/0304-405X(90)90004-J&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Henkel, Sam James, J. Spencer Martin, and Federico Nardari. “Time-Varying Short-Horizon Predictability.” Journal of Financial Economics 99, no. 3 (March 1, 2011): 560–80. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2010.09.008&#34;&gt;https://doi.org/10.1016/j.jfineco.2010.09.008&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Johannes, Michael, Lars A. Lochstoer, and Yiqun Mou. “Learning about Consumption Dynamics.” The Journal of Finance 71, no. 2 (2016): 551–600. &lt;a href=&#34;https://doi.org/10.1111/jofi.12246&#34;&gt;https://doi.org/10.1111/jofi.12246&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Johannes, Michael S., Nicholas G. Polson, and Jonathan R. Stroud. “Optimal Filtering of Jump Diffusions: Extracting Latent States from Asset Prices.” The Review of Financial Studies 22, no. 7 (July 1, 2009): 2759–99. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhn110&#34;&gt;https://doi.org/10.1093/rfs/hhn110&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jones, Christopher S. “Nonlinear Mean Reversion in the Short-Term Interest Rate.” The Review of Financial Studies 16, no. 3 (July 1, 2003): 793–843. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhg014&#34;&gt;https://doi.org/10.1093/rfs/hhg014&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jones, Christopher S., and Jay Shanken. “Mutual Fund Performance with Learning across Funds.” Journal of Financial Economics 78, no. 3 (December 1, 2005): 507–52. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2004.08.009&#34;&gt;https://doi.org/10.1016/j.jfineco.2004.08.009&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Julliard, Christian, and Anisha Ghosh. “Can Rare Events Explain the Equity Premium Puzzle?” The Review of Financial Studies 25, no. 10 (October 1, 2012): 3037–76. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhs078&#34;&gt;https://doi.org/10.1093/rfs/hhs078&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Kandel, Shmuel, and Robert F. Stambaugh. “On the Predictability of Stock Returns: An Asset-Allocation Perspective.” The Journal of Finance 51, no. 2 (1996): 385–424. &lt;a href=&#34;https://doi.org/10.1111/j.1540-6261.1996.tb02689.x&#34;&gt;https://doi.org/10.1111/j.1540-6261.1996.tb02689.x&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Klein, Roger W., and Vijay S. Bawa. “The Effect of Estimation Risk on Optimal Portfolio Choice.” Journal of Financial Economics 3, no. 3 (June 1, 1976): 215–31. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(76)90004-0&#34;&gt;https://doi.org/10.1016/0304-405X(76)90004-0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. “The Effect of Limited Information and Estimation Risk on Optimal Portfolio Diversification.” Journal of Financial Economics 5, no. 1 (August 1, 1977): 89–111. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(77)90031-9&#34;&gt;https://doi.org/10.1016/0304-405X(77)90031-9&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lamoureux, Christopher G., and H. Douglas Witte. “Empirical Analysis of the Yield Curve: The Information in the Data Viewed through the Window of Cox, Ingersoll, and Ross.” The Journal of Finance 57, no. 3 (2002): 1479–1520. &lt;a href=&#34;https://doi.org/10.1111/1540-6261.00467&#34;&gt;https://doi.org/10.1111/1540-6261.00467&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lamoureux, Christopher G., and Guofu Zhou. “Temporary Components of Stock Returns: What Do the Data Tell Us?” The Review of Financial Studies 9, no. 4 (October 1, 1996): 1033–59. &lt;a href=&#34;https://doi.org/10.1093/rfs/9.4.1033&#34;&gt;https://doi.org/10.1093/rfs/9.4.1033&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Li, Minqiang, Neil D. Pearson, and Allen M. Poteshman. “Conditional Estimation of Diffusion Processes.” Journal of Financial Economics 74, no. 1 (October 1, 2004): 31–66. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2004.03.001&#34;&gt;https://doi.org/10.1016/j.jfineco.2004.03.001&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;McCulloch, Robert, and Peter E. Rossi. “Posterior, Predictive, and Utility-Based Approaches to Testing the Arbitrage Pricing Theory.” Journal of Financial Economics 28, no. 1 (November 1, 1990): 7–38. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(90)90046-3&#34;&gt;https://doi.org/10.1016/0304-405X(90)90046-3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Pástor, Ľuboš. “Portfolio Selection and Asset Pricing Models.” The Journal of Finance 55, no. 1 (2000): 179–223. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00204&#34;&gt;https://doi.org/10.1111/0022-1082.00204&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Pástor, Ľuboš, and Robert F. Stambaugh. “Comparing Asset Pricing Models: An Investment Perspective.” Journal of Financial Economics 56, no. 3 (June 1, 2000): 335–81. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(00)00044-1&#34;&gt;https://doi.org/10.1016/S0304-405X(00)00044-1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. “Costs of Equity Capital and Model Mispricing.” The Journal of Finance 54, no. 1 (1999): 67–121. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00099&#34;&gt;https://doi.org/10.1111/0022-1082.00099&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. “Investing in Equity Mutual Funds.” Journal of Financial Economics 63, no. 3 (March 1, 2002): 351–80. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(02)00065-X&#34;&gt;https://doi.org/10.1016/S0304-405X(02)00065-X&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. “Mutual Fund Performance and Seemingly Unrelated Assets.” Journal of Financial Economics 63, no. 3 (March 1, 2002): 315–49. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(02)00064-8&#34;&gt;https://doi.org/10.1016/S0304-405X(02)00064-8&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Pettenuzzo, Davide, Allan Timmermann, and Rossen Valkanov. “Forecasting Stock Returns under Economic Constraints.” Journal of Financial Economics 114, no. 3 (December 1, 2014): 517–53. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2014.07.015&#34;&gt;https://doi.org/10.1016/j.jfineco.2014.07.015&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Rouwenhorst, K. Geert. “Local Return Factors and Turnover in Emerging Stock Markets.” The Journal of Finance 54, no. 4 (1999): 1439–64. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00151&#34;&gt;https://doi.org/10.1111/0022-1082.00151&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Shanken, Jay. “A Bayesian Approach to Testing Portfolio Efficiency.” Journal of Financial Economics 19, no. 2 (December 1, 1987): 195–215. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(87)90002-X&#34;&gt;https://doi.org/10.1016/0304-405X(87)90002-X&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Shanken, Jay, and Ane Tamayo. “Payout Yield, Risk, and Mispricing: A Bayesian Analysis.” Journal of Financial Economics 105, no. 1 (July 1, 2012): 131–52. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2011.12.002&#34;&gt;https://doi.org/10.1016/j.jfineco.2011.12.002&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stambaugh, Robert F. “Analyzing Investments Whose Histories Differ in Length.” Journal of Financial Economics 45, no. 3 (September 1, 1997): 285–331. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(97)00020-2&#34;&gt;https://doi.org/10.1016/S0304-405X(97)00020-2&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Econometrics</title>
      <link>/2020/03/24/bayesian-econometrics/</link>
      <pubDate>Tue, 24 Mar 2020 09:21:25 -0700</pubDate>
      
      <guid>/2020/03/24/bayesian-econometrics/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Since I&amp;rsquo;m in social distancing mode, I figured it would be a good time to write a blog post on Bayesian methods and financial economics. I have written a post in quite a while, as the past year and a half or so have been a busy time for me. The finance PhD takes up most of my time, as well as my work on &lt;a href=&#34;https://turing.ml&#34;&gt;Turing.jl&lt;/a&gt;, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probabilistic_programming&#34;&gt;probabilistic programming language&lt;/a&gt; (PPL) for Julia.&lt;/p&gt;

&lt;p&gt;Before I continue, I want to make sure people understand my perspective. I do not work in fields of economics that people tend to think of when they think of economics &amp;ndash; maybe you think of labor, health, or macroeconomics, all of which are valuable fields that I know very little about. I study finance, which is the study of how money and securities are used and what they do to the economy. Keep that in mind as we go along. I&amp;rsquo;m in a smaller subfield of economics that shares many of the same methodologies and language, but is applied to the theory of the firm and to asset prices.&lt;/p&gt;

&lt;p&gt;Back to Turing.jl. Everyone who works on Turing is of an extremely high quality level. They are all typically very skilled in their respective computational or statistical domains, and it is easy to feel a little out of place. I am not an optimization person, or a machine learning person, or even really anyone with any measure of formal engineering training&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:training&#34;&gt;&lt;a href=&#34;#fn:training&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;I am, however, a financial economist (with training wheels). Working on Turing and spending a lot of time with CS and statistics people who are not economists has been particularly eye-opening, because economics is a unique field that I think tends to stand out in the sciences. Here&amp;rsquo;s why:&lt;/p&gt;

&lt;h2 id=&#34;ground-truth&#34;&gt;Ground truth&lt;/h2&gt;

&lt;p&gt;As with all social sciences, &lt;strong&gt;ground truth is hard to come by&lt;/strong&gt;. In any of the hard sciences like physics, you can hypothesize something, and then sometimes you can spend hundred of millions of dollars to see if it is true. In economics, we don&amp;rsquo;t really have this. You can&amp;rsquo;t run experiments where you make half of all pregnant mothers smoke to see what happens to their kids, or randomly assign particular directors to company boards.&lt;/p&gt;

&lt;h2 id=&#34;causal-inference&#34;&gt;Causal inference&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Causal inference is the name of the game&lt;/strong&gt;. Economics is about how thing A causes thing B to change. The field has built up an enormous set of statistical tools just to identify whether and how a thing is causal, and many of these tools are commonly only used in social sciences&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:iv&#34;&gt;&lt;a href=&#34;#fn:iv&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&#34;economists-love-math&#34;&gt;Economists love math&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Economics is mathematical&lt;/strong&gt;. Because economists don&amp;rsquo;t have ground truth, they build models of behavior and attempt to match empirical facts to what theory suggests should exist. Economists tend to bash other social scientists (especially sociology, sorry folks) because their methods are less sophisticated. Economists even bash financial economists like me, because we tend to be 5-10 years behind economics writ-large in terms of empirical and theoretical methodologies.&lt;/p&gt;

&lt;h1 id=&#34;bayesian-methods-and-economics&#34;&gt;Bayesian methods and economics&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m going to talk about how I think Bayesian methods are being used currently in financial economics, why I think Bayesian methods should be used more in empirical economics. I also want to pitch Turing.jl as a way for researchers to do more of this, if only because it is very easy to do so.&lt;/p&gt;

&lt;p&gt;I mentioned before that economics does not have ground truth. There will never be a point when a researcher can be confident that their model is 100% correct, or that their parameter estiamtes are accurate. It&amp;rsquo;s just not possible &amp;ndash; economics is the science of choices by people. People are made up of angry goop and they can behave irrationally at times, so a deterministic model is pretty hard to specify.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s why economists use standard errors, and think so hard about whether their model is free from material omitted bias, heteroskedasticity, etc. Standard errors in OLS (or whatever your method is) give you a good proxy for the variance of your estimator, assuming that estimator is normal.&lt;/p&gt;

&lt;p&gt;Economists have many ways to think about standard errors and causal inference &amp;ndash; do your errors have some kind of autocorrelation? What if clusters of observation share some common error? Does the instrument you are using satisfy the necessary requirements? These kinds of questions are where economics shines the brightest. Because there is no ground truth, you want to be as confident as you can when you say something.&lt;/p&gt;

&lt;h2 id=&#34;bayesian-methods&#34;&gt;Bayesian methods&lt;/h2&gt;

&lt;p&gt;What does any of this have to do with Bayesian methods? Well, my biggest issue with contemporary econometrics is the use of priors. Every single time someone runs a regression with &lt;code&gt;lm(y ~ x, data)&lt;/code&gt; or &lt;code&gt;reg y x&lt;/code&gt;, they are doing a very specific thing. OLS is simply the &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&#34;&gt;maximum a posteriori&lt;/a&gt; estimate of the model&amp;rsquo;s parameters with a flat prior everywhere, also called &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;maximum likelihood&lt;/a&gt;. By doing this, you let the data speak for you, which I am generally in favor of.&lt;/p&gt;

&lt;p&gt;But sometimes priors matter! When you have small datasets or multiple posterior  modes, sometimes priors can get your estimates to where you think is reasonable (conditional on a good prior).&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not like economists have a shortage of priors, either. Good papers are either backed by good theory or show intuitive relationships that don&amp;rsquo;t need a formal theoretical link, and in all cases you can typically say something like&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the relationships in Person (2030) hold, then $\alpha &amp;gt; 1$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sounds like a prior to me. You can use theoretical predictions to motivate priors when you&amp;rsquo;re writing models.&lt;/p&gt;

&lt;h2 id=&#34;the-state-of-bayesian-methods-in-finance&#34;&gt;The state of Bayesian methods in finance&lt;/h2&gt;

&lt;p&gt;My perception is that Bayesian methods are still somewhat fringe, but that they have a slight but regular appearance in finance. I went to our top journal, the &lt;a href=&#34;https://afajof.org/&#34;&gt;Journal of Finance&lt;/a&gt;, and searched for the word &amp;ldquo;bayesian&amp;rdquo;. I grabbed any of the papers that are not pure theory. Here&amp;rsquo;s a list of papers that turned up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cavagnaro et al. (2019). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12783&#34;&gt;Measuring Institutional Investors’ Skill at Making Private Equity Investments&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pástor (2000). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00204&#34;&gt;Portfolio Selection and Asset Pricing Models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pástor and Stambaugh (1999). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00099&#34;&gt;Costs of Equity Capital and Model Mispricing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Johannes, Lochstoer, and Mou (2016). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12246&#34;&gt;Learning About Consumption Dynamics&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Lamoureux and Witte (2002). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/1540-6261.00467&#34;&gt;Empirical Analysis of the Yield Curve: The Information in the Data Viewed through the Window of Cox, Ingersoll, and Ross&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Kandel and Stambaugh (1996). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.1996.tb02689.x&#34;&gt;On the Predictability of Stock Returns: An Asset‐Allocation Perspective&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Barillas and Shanken (2018). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12607&#34;&gt;Comparing Asset Pricing Models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Rouwenhorst (1999). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00151&#34;&gt;Local Return Factors and Turnover in Emerging Stock Markets&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Brav (2000). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00279&#34;&gt;Inference in Long‐Horizon Event Studies: A Bayesian Approach with Application to Initial Public Offerings&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Baks, Metrick, and Wachter (2001). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00319&#34;&gt;Should Investors Avoid All Actively Managed Mutual Funds? A Study in Bayesian Performance Evaluation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Busse and Irvine (2006). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.2006.01057.x&#34;&gt;Bayesian Alphas and Mutual Fund Persistence&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many of these papers use explicitly derived analytic forms, explicit Gibbs conditionals, or very basic MCMC models. Very few of these models are non-linear models, and in most cases they tend to be regular frequentist econometrics with the addition of a density function.&lt;/p&gt;

&lt;h1 id=&#34;what-s-cool&#34;&gt;What&amp;rsquo;s cool&lt;/h1&gt;

&lt;p&gt;My favorite papers apply Bayesian methods in a more interesting way. One example is &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12607&#34;&gt;Barillas and Shanken (2018)&lt;/a&gt;, who use a closed form solution to analyze the efficacy of various factor models. I like this paper quite a lot, but I think that researchers tend to work really hard to derive closed form solutions when they are not really ncessary. For example, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12854&#34;&gt;Chib, Zeng, and Zhao (2020)&lt;/a&gt; attempted to replicate Barillas and Shanken, and noticed that the use of a Jeffrey&amp;rsquo;s prior on nuisance parameters makes the closed form solution unsound.&lt;/p&gt;

&lt;p&gt;You can avoid this by just numerically solving your model. I believe that we should start thinking more and more computationally as our models become more complex, and Markov Chain Monte Carlo lets you do this. Importantly, this is easier now that ever. It&amp;rsquo;s not 2002 anymore and you don&amp;rsquo;t have to roll your own Gibbs sampler every time you need to solve some model. You can just use a probabilistic programming language like Turing!&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;book&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;book&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;
Some other good PPLs are &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;, &lt;a href=&#34;https://github.com/cscherrer/Soss.jl&#34;&gt;Soss.jl&lt;/a&gt;, &lt;a href=&#34;https://docs.pymc.io/&#34;&gt;PyMC&lt;/a&gt;, and &lt;a href=&#34;http://pyro.ai/&#34;&gt;Pyro&lt;/a&gt;, among many others.
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t get me wrong &amp;ndash; I love theory as much as the next person. Theory is good for telling stories, whereas empirics are good for proving those stories. Or theory is getting more and more complex, and our empirics should rise to meet the challenge. Additionally, I think where Bayesian methods are concerned, people try to mix theory and empirics too closely, and they end up looking for closed form solutions where there are none.&lt;/p&gt;

&lt;p&gt;I want to present a rough sketch of how I think about this, and how I&amp;rsquo;d do it computationally. Assume that there are $N$ factor models, each of which returns an expected return from a function &lt;code&gt;r(m, t, x)&lt;/code&gt; for factor model index &lt;code&gt;m&lt;/code&gt;, time &lt;code&gt;t&lt;/code&gt;, and observable data &lt;code&gt;x&lt;/code&gt;. Assume &lt;code&gt;x&lt;/code&gt; is a matrix of returns and factors for one security. One nice way to do this in Turing is&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Import Turing&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Turing

&lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Declare our probabilistic model.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# - models is a vector of functions, r(n, t, x), that return an expected return.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# - data is a matrix with returns in the first column, and factors in the remaining columns.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# &lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;@model&lt;/span&gt; factors(models, data) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# Choose which model is &amp;#34;true&amp;#34;, all models have equal priors.&lt;/span&gt;
    m &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; Categorical(length(models))

    &lt;span style=&#34;color:#75715e&#34;&gt;# Draw a variance parameter.&lt;/span&gt;
    σ &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; InverseGamma(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Estimate each return.&lt;/span&gt;
    r_hat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(m, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;size(data, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), data)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Check the model&amp;#39;s predictions:&lt;/span&gt;
    data[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; MvNormal(r_hat, σ)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And you&amp;rsquo;re done! You can run this on whatever sampling method you want, and it&amp;rsquo;ll give you posterior probabilities for models. As valuable as raw math is, sometimes it&amp;rsquo;s nice to just type the model up and see what the data says, assuming you&amp;rsquo;ve thought about all the econometric issues as you normally would.&lt;/p&gt;

&lt;h1 id=&#34;hopes&#34;&gt;Hopes&lt;/h1&gt;

&lt;p&gt;In this section, I want to talk about some things I want to see more of going forward.&lt;/p&gt;

&lt;h2 id=&#34;structural-estimation&#34;&gt;Structural estimation&lt;/h2&gt;

&lt;p&gt;Structural estimation is a really beautiful tool. When you structurally estimate something, you marry theory and empirics to determine the effect of some parameter. For the most part, it is done in a frequentist way by matching moments between simulated data and empirical data. You can do structural estimation in a Bayesian way by specifying a very general probabilistic model and then running it through your PPL of choice. Not only does this give you parameter estimates, but it also tells you how uncertain you are of those estimates. You might even learn that your parameterizations are multimodal, and that there are numerous nontrivial outcomes in your model that a simulated method of moments estimation might miss.&lt;/p&gt;

&lt;h2 id=&#34;prior-sensitivity&#34;&gt;Prior sensitivity&lt;/h2&gt;

&lt;p&gt;Bayesian methods let you test how realistic something is. I read a cool working paper a little while ago called &lt;a href=&#34;https://sites.google.com/site/lalochstoer/VolUnderreactionMain.pdf?attredirects=0&amp;amp;d=1&#34;&gt;Volatility Expectations and Returns&lt;/a&gt; by Lars Lochstoer and Tyler Muir. They propose a novel behavioral explanation of some weird patterns in the VIX, realized volatility, returns, and the variance risk premium. Essentially, if investors use too much of past variance to form their expectations about current variance, you can explain many strange effects in markets.&lt;/p&gt;

&lt;p&gt;The problem with behavioral papers is that they don&amp;rsquo;t quite site right with finance folks, because it&amp;rsquo;s very easy to say that some arbitrageur should have removed this anomaly. Rob Ready (here at the University of Oregon) asked how strong your priors would have to be on using old observations of variance for this effect to matter, and we can test this! Build a model of stoachastic volatility and conditional expectations, and you should be able to fiddle with your model priors until something cool comes out.&lt;/p&gt;

&lt;h2 id=&#34;latent-variables&#34;&gt;Latent variables&lt;/h2&gt;

&lt;p&gt;Bayesian methods are interesting when you apply them to inferring latent variables. In finance, these might be things like managerial skill, expected return, volatility, etc. We&amp;rsquo;ve got all kinds of things we don&amp;rsquo;t directly observe but have models to explain how they interact when other stuff. When you know that X goes up when Y does, you can start to run inference on the relationships between X and Y even when you can&amp;rsquo;t observe Y, though as always, it helps to have lots of data.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This was a bit of a rambling post, but I&amp;rsquo;m trying to get some thoughts on paper. What a time to be alive.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:training&#34;&gt;I do have industry experience, but it&amp;rsquo;s not a perfect substitute. It helps a lot to have thought about all the little fiddly bits that go into Turing.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:training&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:iv&#34;&gt;Do physics people use &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34;&gt;instrumental variables&lt;/a&gt;?
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:iv&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Trade Volume and Information</title>
      <link>/2019/07/28/trade-volume-and-information/</link>
      <pubDate>Sun, 28 Jul 2019 09:21:25 -0700</pubDate>
      
      <guid>/2019/07/28/trade-volume-and-information/</guid>
      <description>&lt;p&gt;Rob Ready here at the University of Oregon recommended that I read John Cochrane&amp;rsquo;s blog post on &lt;a href=&#34;https://johnhcochrane.blogspot.com/2016/10/volume-and-information.html&#34;&gt;why anyone actually trades&lt;/a&gt;. I had not read it before, and finally had some time to do so after my full-bodied sprint to Baltimore and back.&lt;/p&gt;

&lt;p&gt;The post helped me gel an idea of information-based market activity that I started to think about in Switzerland but was unable to formalize. Cochrane points to a lot of mechanisms that people have used to induce trading, such as preference shocks, noise traders, or overconfidence. Obviously some of these are more reasonable than others.  I have met professional traders from big institutions and they certainly seem to be a little overconfident.&lt;/p&gt;

&lt;p&gt;But Cochrane dances near a concept that would explain much of the trading behavior we observe:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We know what this huge volume of trading is about. It’s about
information, not preference shocks. Information seems to need trades to
percolate into prices. We just don’t understand why.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Information is, in my view, the only real reason anyone should trade, but I think my formulation of &amp;ldquo;information&amp;rdquo; is significantly broader than many others. Noise traders can be explained by the information they posess &amp;ndash; a client has just asked for cash, so the noise trader must sell some of the client&amp;rsquo;s assets. This is fundamentally an information  story!&lt;/p&gt;

&lt;p&gt;It is also something I would consider private information. The firm knows something that most other firms do not, which is that they need to liquidate the asset even though their absolute perspective on the value of the asset is unchanged. Only their relative valuation has changed. The firm&amp;rsquo;s value from holding the asset has dramatically increased, because they face consequences for &lt;em&gt;not&lt;/em&gt; selling the asset and returning the funds.&lt;/p&gt;

&lt;p&gt;The perception of value is also an important concept, which in my view is exclusively driven by information. Two firms, a hedge fund and a pension fund, both have sets of information about all securities available. A piece of information might be that Tesla is actually a company, that it is trading at $228.04 right now, or that the pension fund manager owns a Tesla and quite likes the product. It might be that the hedge fund manager saw Elon Musk pick his nose at an In-N-Out twenty years ago, and believes that Elon Musk has limited self control. These are all pieces of information with different levels of usefulness, but collectively, they allow market agents to construct an estimate of an efficient price. If both firms have exactly the same information sets, we should never expect trades. Their perception of absolute value is given only by all the little tidbits they have, and having the same tidbits mean that perceptions should be identical.&lt;/p&gt;

&lt;p&gt;We can throw some math on here to make this more specific. Consider two firms, $x$ and $y$, each with access to a subset of some global information set $I$, given by $I_x$ and $I_y$ respectively.&lt;/p&gt;

&lt;p&gt;Assume that each firm determines the fundamental value of a security $X$ using a function $f(\cdot)$, which is a deterministic function shared by both firms. Let $f(I) = X^*$ be the true fundamental value of the asset. If $I_x = I_y$, then $f(I_x) = f(I_y)$. We cannot reallly say much in general about cases where $I_x \subset I_y$, since it&amp;rsquo;s possible that the additional information held by $y$ increases or decreases their understanding of fundamental value.&lt;/p&gt;

&lt;p&gt;The problem with this set construction is that sets are often very difficult to work with. What&amp;rsquo;s the actual value of having an additional element of information? Well, ultimately, it reduces your level of uncertainty about how much of $I$ you actually observe!&lt;/p&gt;

&lt;p&gt;Being good Bayesians, both firms $x$ and $y$ know that they only have some subset of the true information set. Suppose their valuations are given by distributions $G_x(I_x)$ and $G_y(I_y)$, with $G(I) = X^*$. Neither of these distributions actually have to be centered around $X^*$. They can be lumpy, multimodal, whatever &amp;ndash; all we care about is the fact that the variance of your beliefs is much higher if you have less information.&lt;/p&gt;

&lt;p&gt;Returning to our statements on $f(\cdot)$, what can we say about the case where $I_x \subset I_y$? Well, only that $\text{var}(G_x) &amp;lt;\text{var}(G_y)$. That is, acquiring new information reduces your uncertainty about what you know and what you know you don&amp;rsquo;t know.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m going to leave this here for a moment because I have some work to do, but here are some next steps when I get around to it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Link this set construction to market prices.&lt;/li&gt;
&lt;li&gt;By what mechanism exactly do trades allows others to make inferences about the information sets of others?&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s a dynamic programming extension look like? I.e., if some agents are generally better at acquiring information than others, how might other agents form expectations about their expectations over time?&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>JuliaCon</title>
      <link>/2019/07/26/juliacon/</link>
      <pubDate>Fri, 26 Jul 2019 04:11:59 -0700</pubDate>
      
      <guid>/2019/07/26/juliacon/</guid>
      <description>&lt;p&gt;Yesterday I gave a talk on Turing.jl, and I think I only made 5-9 mistakes. You can check it out on YouTube:&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Jr6HcyHK_Q4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;There were also some spectacular talks by the other probabilitistic programming languages in Julia.
Chad Scherrer (who is pretty great)  covered &lt;a href=&#34;https://www.youtube.com/watch?v=H-Oof2wS-0A&#34;&gt;Soss.jl&lt;/a&gt;,
which has a pretty slick syntax and what I would consider an admirable user interface. Gen.jl had a
&lt;a href=&#34;https://www.youtube.com/watch?v=B7mc1wXPZR8&#34;&gt;remarkable presentation&lt;/a&gt; from Alex Lew
(who is also pretty great) that has a really fascinating perspective on inference. Will Tebbut&amp;rsquo;s
talk on Gaussian processes was really nice, considering I hand&amp;rsquo;t looked to closely at his
&lt;a href=&#34;https://github.com/willtebbutt/Stheno.jl&#34;&gt;Stheno.jl&lt;/a&gt; project before. I also learned about
GPs! Cool stuff. Lastly we had a talk on &lt;a href=&#34;https://www.youtube.com/watch?v=IE39JoVIaEw&#34;&gt;switching Kalman filters&lt;/a&gt;
from Cédric St-Jean-Leblanc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 4</title>
      <link>/2019/06/27/switzerland-day-4/</link>
      <pubDate>Thu, 27 Jun 2019 19:47:24 +0200</pubDate>
      
      <guid>/2019/06/27/switzerland-day-4/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;book&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;book&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; See &lt;a href=&#34;https://www.amazon.com/Empirical-Market-Microstructure-Institutions-Econometrics/dp/0195301641&#34;&gt;here&lt;/a&gt; for a link to purchase Joel&amp;rsquo;s book. It&amp;rsquo;s a good one. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Today was the first day of the empirical session with &lt;a href=&#34;https://albertjmenkveld.com/&#34;&gt;Albert Menkveld&lt;/a&gt; . We covered the first couple chapters of Joel Hasbrouck&amp;rsquo;s excellent book.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;roll&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;roll&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Roll, Richard, 1984, A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market, &lt;em&gt;The Journal of Finance&lt;/em&gt; 39, 1127–1139. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We started with the general Roll (1984) model, which is a really straightforward way to think about how order processing costs make it into the bid-ask spread. Basically, it is assumed that trade prices $p^t$ have a random walk with drift evolution, such that&lt;/p&gt;

&lt;p&gt;$$
p_t = p_{t-1} + \mu + u_t
$$&lt;/p&gt;

&lt;p&gt;Hasbrouck notes that the drift term $\mu$ is largely unnessecary, especially since at micro-scale it&amp;rsquo;s hard to have any notion of expected return.&lt;/p&gt;

&lt;p&gt;The model above is expanded upon by including an efficient price (fundamental value) $m_t$, which is a martingale:&lt;/p&gt;

&lt;p&gt;$$
m_t = m_{t-1} + u_t
$$&lt;/p&gt;

&lt;p&gt;Prices are then a noisy proxy of the true value, as a function of a cost that market makers need to recoup for processing orders&lt;/p&gt;

&lt;p&gt;$$
p_t = m_t + q_tc
$$&lt;/p&gt;

&lt;p&gt;where $c$ is a fixed per-trade cost incurred by the dealer and $q_t$ is an indicator for a buy or sell ($+1$ for a buy and $-1$ for a sell).&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;hasbrouck-2009 &#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;hasbrouck-2009 &#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Hasbrouck, Joel, 2009, Trading Costs and Returns for U.S. Equities: Estimating Effective Costs from Daily Data, &lt;em&gt;The Journal of Finance&lt;/em&gt; 64, 1445–1477. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a good model. I&amp;rsquo;m particularly interested in how Hasbrouck (2009) approaches trying to approximate the $c$ variable, as he uses a Gibbs sampler to run a Bayesian linear regression. Given my association with &lt;a href=&#34;https://turing.ml&#34;&gt;Turing.jl&lt;/a&gt;, I can&amp;rsquo;t help but feel that there is a hierarchical model that would provide a better structural estimate of things like adverse selection cost and order processing cost. That&amp;rsquo;d need a more sophisticated model than the Roll model, however, and I&amp;rsquo;m not quite sure I&amp;rsquo;m up to the task (yet).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 3</title>
      <link>/2019/06/26/switzerland-day-3/</link>
      <pubDate>Wed, 26 Jun 2019 17:46:48 +0200</pubDate>
      
      <guid>/2019/06/26/switzerland-day-3/</guid>
      <description>&lt;p&gt;Not much news today. We had the morning off, presumably so that some students could work on their presentations. The afternoon included five presentations, generally covering&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A theoretical examination of the impacts of financial transaction taxes&lt;/li&gt;
&lt;li&gt;The effect of speed bumps on market liquidity&lt;/li&gt;
&lt;li&gt;The asymmetric returns volatility effect (theory and empirics)&lt;/li&gt;
&lt;li&gt;Determinants of market fragmentation&lt;/li&gt;
&lt;li&gt;The effects of primary issuance on secondary market liquidity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The presentation quality was pretty excellent, overall.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 2</title>
      <link>/2019/06/25/switzerland-day-2/</link>
      <pubDate>Tue, 25 Jun 2019 18:38:51 +0200</pubDate>
      
      <guid>/2019/06/25/switzerland-day-2/</guid>
      <description>

&lt;p&gt;&lt;label for=&#34;glosten&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;glosten&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Glosten, Lawrence R., 1994, Is the Electronic Open Limit Order Book Inevitable?, &lt;em&gt;The Journal of Finance&lt;/em&gt; 49, 1127–1161. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;miller&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;miller&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Vayanos, Dimitri, 1999, Strategic Trading and Welfare in a Dynamic Market, &lt;em&gt;Review of Economic Studies&lt;/em&gt; 66, 219–254. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;miller&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;miller&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Grossman, Sanford J., and Merton H. Miller, 1988, Liquidity and Market Structure, &lt;em&gt;The Journal of Finance&lt;/em&gt; 43, 617–633. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Today was the final lecture series on microstructural theory. We covered models of the limit order book, primarily Glosten (1994), and a model of inventory holding costs and imperfect competition among dealers. The dealer competition model was based on Vayanos (1999) and (I think) Grossman and Miller (1988).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m generally quite pleased with my level of understanding of both microeconomics and microstructure. I didn&amp;rsquo;t feel lost or unable to grasp the model&amp;rsquo;s steps at any point &amp;mdash; several times I was able to infer potential next steps, and I suspect one time I may even have been right. Many models fall out of very general equilibrium conditions and simple first order conditions, so it&amp;rsquo;s fairly straightforward to adapt the general microeconomics tools.&lt;/p&gt;

&lt;h2 id=&#34;glosten-1994&#34;&gt;Glosten (1994)&lt;/h2&gt;

&lt;p&gt;I think that the most complex model we reviewed is that of the Glosten (1994) paper. It&amp;rsquo;s a three-period model, which is generally fairly simple. In this case, the complexity comes from the mental jumps you must make to find the intuition in how expectations are form. The model has some very interesting predictions on how people decide to place order in limit order books based on execution belief and adverse selection risk. Orders accumulate at various levels on the price grid, such that the price at any particular point ($A^k$ for the $k^{th}$ ask or $B^k$ for the $k^{th}$ bid) satisfies&lt;/p&gt;

&lt;p&gt;$$
A_k = E[V \space \vert \space q \ge Y_k^*]
$$&lt;/p&gt;

&lt;p&gt;where $V$ is the valuation of some asset, and $q$ is the size of a market order that arrives in the second period. The term $Y^*_k$ represents the cumulative quantity available up to price $A_k$ (or $B_k$).&lt;/p&gt;

&lt;p&gt;This equilibrium condition comes out of each trader&amp;rsquo;s decisions. A trader looks at each price level, and decides whether adding to the quantity at that price level is likely to be profitable.  If it is not, because the adverse selection risk is too high or there are too many orders on the book already, the trader goes to the next price level and repeats the process.&lt;/p&gt;

&lt;p&gt;A couple of interesting points arise from this particular model. First, the Glosten (1994) model explains why there might be unfilled price levels in the order book. At any given price level, the expectation is formed based on &lt;em&gt;cumulative&lt;/em&gt; quantity on offer. If the cumulative quantity at price levels below satisfies the condition $q \ge Y_k^*$ for several levels of $k$, then those levels will be unfilled. Second, this model predicts that informed traders never submit small orders, but I think this is largely a byproduct of the fact that the model does not consider strategic optimization. Thierry noted that this is common to many models, and that it makes since, considering the complexity involved in optimizing the behavior of multiple agent types.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tomorrow, five students will be presenting their papers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 1</title>
      <link>/2019/06/24/switzerland-day-1/</link>
      <pubDate>Mon, 24 Jun 2019 18:09:08 +0200</pubDate>
      
      <guid>/2019/06/24/switzerland-day-1/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;lugano&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;lugano&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;A picture of Lugano from some balcony somewhere.&lt;img src=&#34;/images/lugano.png&#34; alt=&#34;&#34; /&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Today was the first day of the market &lt;a href=&#34;https://microstructure-course.com/&#34;&gt;microstructure summer school&lt;/a&gt;, where &lt;a href=&#34;https://albertjmenkveld.com/&#34;&gt;Albert Menkveld&lt;/a&gt; and &lt;a href=&#34;https://thierryfoucault.com/&#34;&gt;Thierry Foucault&lt;/a&gt; teach the theory and empirics of market microstructure. It&amp;rsquo;s held in Lugano, Switzerland, a large city in the Italian-speaking canton of Ticino.&lt;/p&gt;

&lt;p&gt;Thierry began the week with theory&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:theory&#34;&gt;&lt;a href=&#34;#fn:theory&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, primarily on determinants of the bid-ask spread, as well as how asymmetric information is applied in many of the classic models. Much of the conversation was based on a simplified model of Biais, Foucault, and Moinas (2015).&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;asymmetrypaper&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;asymmetrypaper&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Biais, Bruno, Thierry Foucault, and Sophie Moinas, 2015, Equilibrium fast trading, &lt;em&gt;Journal of Financial Economics&lt;/em&gt; 116, 292–313. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The model is largely a way of examining how exactly it is that informed trades impact the spread by examining the paremeterizations that define a market. The comparative statics suggest that spreads are increasing in the&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proportion of fast traders (informed)&lt;/li&gt;
&lt;li&gt;Degree of market fragmentation&lt;/li&gt;
&lt;li&gt;Private valuation volatility&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So far it&amp;rsquo;s been pleasant. Looking forward to tomorrow&amp;rsquo;s lecture.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:theory&#34;&gt;I am often made fun of for saying the phrase &amp;ldquo;theory models&amp;rdquo;. I recognize this is probably not right, but there has been so much mockery that I must now stick to my terminology.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:theory&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>AFA Webcasts</title>
      <link>/2019/01/08/afa-webcasts/</link>
      <pubDate>Tue, 08 Jan 2019 08:08:25 -0800</pubDate>
      
      <guid>/2019/01/08/afa-webcasts/</guid>
      <description>&lt;p&gt;From Susan Athey&amp;rsquo;s &lt;a href=&#34;https://www.aeaweb.org/webcasts/2019/aea-afa-joint-luncheon-impact-of-machine-learning&#34;&gt;luncheon address&lt;/a&gt; on machine learning to the AFA:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; and so we have to then develop on top of that best practices for analyzing a black box algorithm. And there&amp;rsquo;s not really like a science and convention on that. If somebody writes a difference-in-difference paper, there&amp;rsquo;s ten things you should do &amp;ndash; There are best practices, and we know how to evaluate whether you should believe the results or not. But we don&amp;rsquo;t have that analogous set of best practices and conventions around these black boxes yet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Susan Athey&amp;rsquo;s whole address was pretty spectacular, so I&amp;rsquo;d set aside some time to watch it if you have a spare error.&lt;/p&gt;

&lt;p&gt;A large portion of the talk was dedicated to this idea that economists and financial academics writ large need to be better at re-purposing advances in machine learning towards our own ends, and that we need to start thinking about providing frameworks on how to think about the interpretation of models that do not come from traditional econometrics.&lt;/p&gt;

&lt;p&gt;I was really delighted to see this kind of thinking. There&amp;rsquo;s a growing number of very prominent economists (Mullainathan had a &lt;a href=&#34;https://www.youtube.com/watch?v=xl3yQBhI6vY&#34;&gt;similar address&lt;/a&gt; in 2017) who are starting to think critically about how and when these tools should be used.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018 Retrospective</title>
      <link>/2019/01/01/2018-retrospective/</link>
      <pubDate>Tue, 01 Jan 2019 12:31:22 -0800</pubDate>
      
      <guid>/2019/01/01/2018-retrospective/</guid>
      <description>

&lt;p&gt;It&amp;rsquo;s a new year, and I&amp;rsquo;ve gathered some notes from the year gone by.&lt;/p&gt;

&lt;h1 id=&#34;achievements&#34;&gt;Achievements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Became a C# software developer.&lt;/li&gt;
&lt;li&gt;Went to Miami.&lt;/li&gt;
&lt;li&gt;Passed CFA Level One.&lt;/li&gt;
&lt;li&gt;Learned &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;, got better at &lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt;, introduced myself to &lt;a href=&#34;https://ocaml.org/&#34;&gt;OCaml&lt;/a&gt;, &lt;a href=&#34;https://www.haskell.org/&#34;&gt;Haskell&lt;/a&gt;, and &lt;a href=&#34;https://chapel-lang.org/&#34;&gt;Chapel&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Started a &lt;a href=&#34;https://business.uoregon.edu/faculty/cameron-pfiffer&#34;&gt;PhD&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Visited my grandmother in upstate New York with my brother.&lt;/li&gt;
&lt;li&gt;Built and designed a tool to detect outlier performance in investment activities.&lt;/li&gt;
&lt;li&gt;Endured a lot of Southern Oregon&amp;rsquo;s wildfire smoke.&lt;/li&gt;
&lt;li&gt;Moved from Medford, OR to Eugene, OR.&lt;/li&gt;
&lt;li&gt;Finally got a piano in the house again. It is super out of tune so I&amp;rsquo;ve mostly been playing honky tonk blues and stuff.&lt;/li&gt;
&lt;li&gt;Started working on &lt;a href=&#34;http://turing.ml/&#34;&gt;Turing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Drilled through a bunch of 220v electrical lines.&lt;/li&gt;
&lt;li&gt;Attended the Pacific Northwest Finance Conference in Seattle.&lt;/li&gt;
&lt;li&gt;Had my first ever office hours.&lt;/li&gt;
&lt;li&gt;Rowed 5km in 23:03.&lt;/li&gt;
&lt;li&gt;Deadlifted 220lbs.&lt;/li&gt;
&lt;li&gt;Made it through the first academic term with some pretty good grades. Courses taken so far are:

&lt;ul&gt;
&lt;li&gt;Math Camp&lt;/li&gt;
&lt;li&gt;Core Microeconomics 1 (Consumer theory, mostly)&lt;/li&gt;
&lt;li&gt;Econometrics 1 (Mathematical statistics)&lt;/li&gt;
&lt;li&gt;Accounting Theory and Disclosure (disclosure theory, what accounting does, etc.)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Not insane yet.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;goals&#34;&gt;Goals&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Learn more about market microstructure. It&amp;rsquo;s a fascinating field and nobody is going to tell me everything I want to know, so I have to make an effort to do so myself.&lt;/li&gt;
&lt;li&gt;Keep up my programming skills. Don&amp;rsquo;t get stale.&lt;/li&gt;
&lt;li&gt;Really get better at understanding the nuance of MCMC methods. There&amp;rsquo;s a lot of beauty there that needs to be unpacked.&lt;/li&gt;
&lt;li&gt;Row a 5km in 22:30.&lt;/li&gt;
&lt;li&gt;Save up enough to purchase a desktop computer.&lt;/li&gt;
&lt;li&gt;Be better about remembering stuff.&lt;/li&gt;
&lt;li&gt;Write at least one post a month.&lt;/li&gt;
&lt;li&gt;Read academic papers more deliberately. Keep better summaries and focus on what matters.&lt;/li&gt;
&lt;li&gt;Broaden my repertoire of left-hand licks (grooves? base lines? I have no idea) to include things requiring greater hand independence.&lt;/li&gt;
&lt;li&gt;Do great in all my coursework.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;bullets&#34;&gt;Bullets&lt;/h1&gt;

&lt;p&gt;I do a type of pseudo-bullet journaling, where I write down a couple of points from various days. Here are some of my favorites. I tend to swear a lot on in my journal, so I have censored them in accordance with the fact that I&amp;rsquo;m sort of a professional adult.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;January 11th - &amp;ldquo;C# isn&amp;rsquo;t a terrible language.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 14th - &amp;ldquo;Had brownies for lunch and breakfast.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 15th - &amp;ldquo;steroid injections hurt&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 21st - &amp;ldquo;Flying is stupid and I hate it.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 23rd - &amp;ldquo;Passed CFA level one, which means I have to keep going. ****.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 26th - &amp;ldquo;Being paid money is nice.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 28th - &amp;ldquo;I want to make a cryptocurrency exchange where the rules change every day. That&amp;rsquo;d be fun.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;February 3rd - &amp;ldquo;Liz made some bomb-ass chili for dinner.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;February 17th - &amp;ldquo;Tried to refinance the car, failed dramatically.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;February 20th - &amp;ldquo;Bricked my Linux partition after trying to remove Python 2. Time for bed.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;March 8th - &amp;ldquo;Seems nobody has tried to break into the house yet.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;March 20th - &amp;ldquo;Another day, another step towards an inevitable death.&amp;rdquo; Hilariously, this is immediately followed with &amp;ldquo;Nothing interesting happened today.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;March 27th - &amp;ldquo;Worked on code all day. It was nice.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;May 24th - &amp;ldquo;Made Assist reading time go from 13 minutes to 30 seconds.&amp;rdquo; The assist is a giant CSV parser we used at work, for background on this.&lt;/li&gt;
&lt;li&gt;June 10th - &amp;ldquo;Filled the pond with vinegar.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;July 12th - &amp;ldquo;Biked in $104^∘$ weather. It was OK.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;August 10th - &amp;ldquo;Made fancy mac + cheese.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;August 24th - &amp;ldquo;I am officially unemployed.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;September 11th - &amp;ldquo;I hate bus people. I should take my bike more.&amp;rdquo; This didn&amp;rsquo;t happen much. I live 12 miles one-way from school, and it&amp;rsquo;s about two hours of biking a day to commute that way. I have done this kind of thing in the past and it is not great. Good way to lose 80 pounds, though.&lt;/li&gt;
&lt;li&gt;September 14th - &amp;ldquo;I made lava cake. Lava cake is &lt;em&gt;dope&lt;/em&gt;.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;September 27th - &amp;ldquo;While putting up a pot rack, we hit an electrical line.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;October 11th - &amp;ldquo;Drank beer and worked on data structures.&amp;rdquo; I am unclear about what this means.&lt;/li&gt;
&lt;li&gt;November 18th - &amp;ldquo;Drilled through &lt;strong&gt;another&lt;/strong&gt; electrical cable in the wall, this time for the dryer. **** everything.&amp;rdquo; Hilariously, this was a second attempt to install the pot rack introduced on September 27th.&lt;/li&gt;
&lt;li&gt;December 9th - &amp;ldquo;The mice in the ceiling continue to be spectacularly annoying.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Stock Prediction With News</title>
      <link>/2018/12/31/stock-prediction-with-news/</link>
      <pubDate>Mon, 31 Dec 2018 06:15:40 -0800</pubDate>
      
      <guid>/2018/12/31/stock-prediction-with-news/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;source&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Sardelich, Marcelo, and Suresh Manandhar, 2018, Multimodal deep learning for short-term stock volatility prediction, &lt;em&gt;arXiv:1812.10479 [cs, q-fin, stat]&lt;/em&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m always interested in seeing more people apply natural language processing to financial statements and headlines. A lot of the accounting literature where those kinds of tools are most likely to be useful uses very primitive techniques for textual analysis.&lt;/p&gt;

&lt;p&gt;So I was delighted to read this paper by Marcelo and Manandhar &lt;a href=&#34;https://arxiv.org/abs/1812.10479v1&#34;&gt;on arXiv&lt;/a&gt; where the authors gather headlines and attempt volatility forecasting. By their measure, they do pretty well, better than $\text{GARCH}(1,1)$. I don&amp;rsquo;t know enough about GARCH to make an educated assessment on that, but their MSE and $R^2$ is pretty remarkable for financial time series.&lt;/p&gt;

&lt;p&gt;The authors segmented their predictions by sector, and I noticed that the $R^2$ for the energy sector is substantially higher than the other sectors (~0.4 vs. ~0.2) for both GARCH and their model. I have to wonder why this is the case. Perhaps the energy sector is simply more responsive to news? I suppose that makes sense when you think about how dramatically oil prices change in response to almost every event.&lt;/p&gt;

&lt;p&gt;Regardless, this was an interesting paper with a surprising level of financial knowledge and a very interesting ML model (word embeddings, sentence encoders, LTSM networks, oh my) and it&amp;rsquo;s worth a read.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing Market Marking</title>
      <link>/2018/12/27/optimizing-market-marking/</link>
      <pubDate>Thu, 27 Dec 2018 16:48:26 -0800</pubDate>
      
      <guid>/2018/12/27/optimizing-market-marking/</guid>
      <description>

&lt;p&gt;&lt;label for=&#34;source&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Patel, Yagna, 2018, Optimizing Market Making using Multi-Agent Reinforcement Learning, &lt;em&gt;arXiv:1812.10252 [cs, q-fin, stat]&lt;/em&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Apparently I&amp;rsquo;ve been on a kick reading some of these reinforcement learning/market making papers. Yagna Patel published an interesting paper on arXiv&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:arxiv&#34;&gt;&lt;a href=&#34;#fn:arxiv&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; discussing the application of a reinforcement learning agent to market making, one of my favorite topics. Market making, that is, not necessarily reinforcement learning.&lt;/p&gt;

&lt;h1 id=&#34;why&#34;&gt;Why&lt;/h1&gt;

&lt;p&gt;As Patel points out, there&amp;rsquo;s three big concerns that users of machine learning in finance have to watch out for.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Time. Significantly complex models take a while to make predictions.&lt;/li&gt;
&lt;li&gt;Accuracy. Predictive accuracy in financial markets can be low.&lt;/li&gt;
&lt;li&gt;Policy. Even if you have a model that shows a 55% chance of a uptick in price, how do you define policy to act on that information? More importantly, how do you define a policy that adapts to a changing fitness landscape?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The overall goal here is to determine whether reinforcement learning is useful in market making.&lt;/p&gt;

&lt;h1 id=&#34;what&#34;&gt;What&lt;/h1&gt;

&lt;p&gt;From the abstract:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In this paper, reinforcement learning is applied to the problem of optimizing market making. A multi-agent reinforcement learning framework is used to optimally place limit orders that lead to successful trades. The framework consists of two agents. The macro-agent optimizes on making
the decision to buy, sell, or hold an asset. The micro-agent optimizes on placing limit orders within the limit order book. For the context of this paper, the proposed framework is applied and studied on the Bitcoin cryptocurrency market. The goal of this paper is to show that reinforcement learning is a viable strategy that can be applied to complex problems (with complex environments) such as market making.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Patel essentially presents a two-stage RL model for tackling trading, though he fails to actually approach market making methods.&lt;/p&gt;

&lt;h1 id=&#34;how&#34;&gt;How&lt;/h1&gt;

&lt;p&gt;Patel somewhat misses the point on what market makers do in how his model is set up. There are two components. The first is the macro agent, which decides whether to &lt;em&gt;buy&lt;/em&gt;, &lt;em&gt;sell&lt;/em&gt;, or &lt;em&gt;hold&lt;/em&gt;. The second component is a micro agent, which determines where to place the macro agent&amp;rsquo;s buy or sell order. The missed point is that market makers shouldn&amp;rsquo;t really be deciding whether to buy or sell; their goal is to place both buy and sell orders optimally within the order book, such that they minimize adverse selection and inventory risk. This macro agent design fails to model that behaviour, and already the paper has failed to answer the question as to whether RL can be applied to market making.&lt;/p&gt;

&lt;p&gt;Patel also chooses to use discrete-time modeling, for this reason:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As noted in the problem statement, discrete time-steps are chosen (rather than continuous time-steps) for the  reason that continuous time-steps would not be possible in the real world since the WebSocket data itself arrives at discrete time-steps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I don&amp;rsquo;t know that I buy that. Lots of microstructure folk (the crazy ones, anyway) use continuous time for this kind of thing, even though the WebSocket feeds are discrete. I suspect that it would be hard to model, so I can accept this.&lt;/p&gt;

&lt;p&gt;As this is a RL paper, the choice of reward function is somewhat important. The micro agent is evaluated against VWAP, which strikes me as odd. I&amp;rsquo;m not sure this framework of a macro and micro agent works that well, as the micro agent should be making choices based on probabilities of informed trading and such.&lt;/p&gt;

&lt;h1 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h1&gt;

&lt;p&gt;All in all, this is a really interesting paper, but it kind of misses the mark on market making. I&amp;rsquo;d like to see something closer to the finance literature on this.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:arxiv&#34;&gt;Where none of the best finance papers go. This is something of a sorry state of affairs &amp;mdash; I really love the machine learning/microstructure papers that end up on arXiv, but many of the other papers are of an extremely low quality.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:arxiv&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection mechanism design</title>
      <link>/2018/12/17/selection-mechanism-design/</link>
      <pubDate>Mon, 17 Dec 2018 07:28:24 -0800</pubDate>
      
      <guid>/2018/12/17/selection-mechanism-design/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;source&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Dewhurst, David Rushing, Michael Vincent Arnold, and Colin Michael Van Oort, 2018, Selection mechanism design affects volatility in a market of evolving zero-intelligence agents, arXiv:1812.05657 [cs, q-fin].&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.05657&#34;&gt;Here&amp;rsquo;s an interesting paper&lt;/a&gt; from Dewherst, Arnold, and Van Oort published to arXiv. Dewherst et al. use an evolving multi-agent landscape to model evolutionary selection mechanisms applicable to markets.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;amg&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;amg&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Lo, Andrew W, 2004, The Adaptive Markets Hypothesis, Journal of Portfolio Management, 15.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Choosing a representative selection mechanism is somewhat important if you follow Andrew Lo&amp;rsquo;s Adaptive Markets Hypothesis, where financial agents are weeded out in response to changing market conditions. Deherst et al. examine three selection mechanisms.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;fba&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;fba&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; The authors use frequent batch auctions instread of continuous dual auctions. I&amp;rsquo;m a big fan of frequent batch auctions, and you can read more about them in the &lt;a href=&#34;https://faculty.chicagobooth.edu/eric.budish/research/HFT-FrequentBatchAuctions.pdf&#34;&gt;Budish, Cramton and Shim (2015) paper&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The first is a global quantile-based measure, where the bottom 10% of all agents are eliminated when a selection event occurs. The second is a localized variant, where agents in a subsample are kept in the environment according to the probability&lt;/p&gt;

&lt;p&gt;$$p_i(t)=\frac{π_i(t)}{∑{π_j}}$$&lt;/p&gt;

&lt;p&gt;where $π_i$ is the fitness of agent $i$. This can be understood as awarding higher retention probabilities to agents with a high share of the sampled agent&amp;rsquo;s fitness. The final metric uses the first metric with probability ½ and the second metric with probability ½.&lt;/p&gt;

&lt;p&gt;The agents have risk aversion, and are allowed to &amp;ldquo;innovate&amp;rdquo; by drawing their parameters from distributions unaffected by existing agents. If you don&amp;rsquo;t innovate, you draw your parameters from a distribution represented by agents who were not removed.&lt;/p&gt;

&lt;p&gt;The results show that the quantile metric creates agents with higher average profitability, but that the localized variant has a higher correlation between micro and macro volatility.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not quite sold at the end of the paper as to their selection mechanism, but I think it&amp;rsquo;s an interesting vein of research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MD&amp;A Paper Summary</title>
      <link>/2018/10/13/mda-paper-summary/</link>
      <pubDate>Sat, 13 Oct 2018 17:34:17 -0700</pubDate>
      
      <guid>/2018/10/13/mda-paper-summary/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This was written as a summary for an interesting paper I read for a class on whether or not the MD&amp;amp;A section of financial statements can be predictive of bankruptcy.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;paper-source&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;paper-source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Mayew, William J., Mani Sethuraman, and Mohan Venkatachalam. &lt;em&gt;MD&amp;amp;A Disclosure and the Firm’s Ability to Continue as a Going Concern&lt;/em&gt;. The Accounting Review 90, no. 4 (July 2015): 1621–51. &lt;a href=&#34;https://doi.org/10.2308/accr-50983&#34;&gt;https://doi.org/10.2308/accr-50983&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Mayew, Sethuraman and Venkatachalam (2015) present a study on the role that the textual properties of a firm&amp;rsquo;s MD&amp;amp;A section plays in predicting bankruptcy. The research topic came about primarily to inform the debate on whether or not the FASB should require management to assess if the firm is a going concern, though it is more interesting in general for the methodology by which they analyze the MD&amp;amp;A text.&lt;/p&gt;

&lt;p&gt;Prior to this paper, literature focused on small samples to determine whether MD&amp;amp;A was predictive of bankruptcy. Mayew et al. use a fairly substantial sample size to broadly describe the predictive capacity of MD&amp;amp;A disclosures. They include numerous other predictive factors, such as the auditor&amp;rsquo;s opinion and the financials. They were able to accomplish this by using almost entirely automated techniques&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:automated&#34;&gt;&lt;a href=&#34;#fn:automated&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, a contribution to the literature in itself. This paper also joins a litany of others in the bankruptcy prediction space.&lt;/p&gt;

&lt;p&gt;Given a collection of textual data (MD&amp;amp;A) can one predict the likelihood of a firm entering bankruptcy? Mayew et al. answer this in a straightforward manner. The probability of a firm going bankrupt next year is determined by estimating a hazard model, which can be used to estimate the probability of a firm failing in a given time frame. I found it interesting (based on my limited knowledge of hazard models) that their hazard model can be reduced so succintly to a simple regression.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;lough&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;lough&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Loughran, Tim and McDonald, Bill, &lt;em&gt;When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks&lt;/em&gt; (March 4, 2010). Journal of Finance, Forthcoming. Available at SSRN: &lt;a href=&#34;https://ssrn.com/abstract=1331573&#34;&gt;https://ssrn.com/abstract=1331573&lt;/a&gt;  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Mayew et al. use three primary variables sourced from the MD&amp;amp;A text. &lt;em&gt;GC_MGMT&lt;/em&gt; is coded as a 1 when the management has expressed a going concern. This coding was performed manually, and the appendix provides some examples of what was considered to be an expression of going concern. &lt;em&gt;POSMDA(%)&lt;/em&gt; represents the percentage of positive words in the MD&amp;amp;A, and &lt;em&gt;NEGMDA(%)&lt;/em&gt; represents the percentage of negative words in the MD&amp;amp;A. Words are determined to be positive or negative as provided in Loughran and McDonald (2011) who created an alternative list of positive and negative words that are applicable in financial contexts.&lt;/p&gt;

&lt;p&gt;The above MD&amp;amp;A independent variables are used to predict &lt;em&gt;BRUPT&lt;/em&gt;, which is coded as a one when a firm goes bankrupt the following year and zero otherwise. Mayew et al. use numerous other variables, such as the auditor&amp;rsquo;s going concern opinion or sales to total assets. In my opinion, these are not exceptionally important to the main finding of the paper, except insofar as they demonstrate the incremental predictive ability of the MD&amp;amp;A text.&lt;/p&gt;

&lt;p&gt;The sample includes 460 bankrupt firms between 1995 and 2012, as well as the accompanying MD&amp;amp;A, auditor&amp;rsquo;s reports, and financial variables for each firm-year. There is an additional 42,265 non-bankrupt firm-years as a baseline. This sample would appear to be soundly assembled. 460 bankrupt firms is a fairly substantial sample size to estimate on.&lt;/p&gt;

&lt;p&gt;Mayew et al. present striking results for models constructed only on &lt;em&gt;GC_MGMT&lt;/em&gt;, &lt;em&gt;POSMDA(%)&lt;/em&gt;, and &lt;em&gt;NEGMDA(%)&lt;/em&gt;. The model has a pseudo-R$^2$ of 14.58%, with very high significance for each variable. The highest parameter estimate was on &lt;em&gt;GC_MGMT&lt;/em&gt; with -2.880&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:gc&#34;&gt;&lt;a href=&#34;#fn:gc&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, which indicates that an expression of going concern is highly explanatory of a firm&amp;rsquo;s future bankruptcy. Perhaps more importantly, the parameters estimate declines to only 2.096 when all the financial variables are included.&lt;/p&gt;

&lt;p&gt;The author&amp;rsquo;s paper generally well supports the idea that MD&amp;amp;A disclosures contain information relevant to predicting a bankruptcy, even using fairly simlistic methods. Taking the percentage of positive and negative words and determining whether the management has expressed a going concern have proved to be powerful predictors. The model inclusive of the auditor&amp;rsquo;s opinion, the MD&amp;amp;A variables, and financial variables proved to be well-fit with a Pseudo-R$^2$ of 21.13%.&lt;/p&gt;

&lt;p&gt;While this is an interesting finding, there is much more that could be done in this space. The natural-language processing space in machine learning has experienced tremendous growth in the past several years, and I suspect that applications from that space would apply well to MD&amp;amp;A analysis.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;nlp&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;nlp&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Kraus, Mathias, and Stefan Feuerriegel. &lt;em&gt;Decision Support from Financial Disclosures with Deep Neural Networks and Transfer Learning&lt;/em&gt;. Decision Support Systems 104 (December 2017): 38–48. &lt;a href=&#34;https://doi.org/10.1016/j.dss.2017.10.001&#34;&gt;https://doi.org/10.1016/j.dss.2017.10.001&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For example, Kraus et al. apply a variety of deep learning techniques traditionally used to model arbitrary text sequences, such as recurrent (RNN) and long-term short-memory (LTSM) neural networks. These architectures may address issues that Mayew et al. generate with their modeling technique.&lt;/p&gt;

&lt;p&gt;A key issue with Mayew et al. is that using &lt;em&gt;POSMDA(%)&lt;/em&gt; and &lt;em&gt;NEGMDA(%)&lt;/em&gt; discard any possibility of understanding the &lt;em&gt;context&lt;/em&gt; of a word and the context of the words around it. The phrase &amp;ldquo;Our performance was spectacular&amp;rdquo; and &amp;ldquo;Our performance was not spectacular&amp;rdquo; are nearly identical, but have radically different meanings. LTSM and RNN networks can model the relationship between words for exceptionally long sentences, paragraphs, and documents, and better improve on Mayew et al.&amp;rsquo;s predictive variables. Kraus et al. demonstrated a test sample accuracy of 57%&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:kraus-table&#34;&gt;&lt;a href=&#34;#fn:kraus-table&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; on classifying the direction of a stock return after the release of a financial disclosure. With some modifications, the entirety of the model could be used to predict Mayew et al.&amp;rsquo;s bankruptcy variable.&lt;/p&gt;

&lt;p&gt;Additionally, it is possible to include the text of the auditor&amp;rsquo;s opinion in the same model type to increase predictive power. It would be interesting to see if there was additional information that could be gathered from the opinion that was more comprehensive than whether or not the auditor had expressed a material going concern.&lt;/p&gt;

&lt;p&gt;Financial variables as well could be included, though in a less straightforward manner. An optimal solution would be to design a deep learning architecture such that the relationship between the lexical structure of the MD&amp;amp;A and the entire time-series of current and previous financial variables could be discerned. This is a markedly more complex topic as it would seem to require numerous compositional models.&lt;/p&gt;

&lt;p&gt;Mayew et al. demonstrate using pleasantly straightforward methods the fact that brankrupcty can be predicted by the content of the MD&amp;amp;A. This confirms an intuitive understanding of the paper&amp;rsquo;s topic, which is that management often knows more about the firm than do average investors, and that financial variables do not necessarily contain all the information necessary to get a holistic understanding of the firm.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:automated&#34;&gt;Much of the manual work seems to have been related to manually coding whether or not the management had expressed a going concern.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:automated&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:gc&#34;&gt;Sourced from Table 3, column 4.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:gc&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:kraus-table&#34;&gt;This is located in Table 2, column 2.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:kraus-table&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dat Site</title>
      <link>/2018/09/20/dat-site/</link>
      <pubDate>Thu, 20 Sep 2018 20:58:31 -0700</pubDate>
      
      <guid>/2018/09/20/dat-site/</guid>
      <description>&lt;p&gt;This site is now accessible via the peer-to-peer browser &lt;a href=&#34;https://beakerbrowser.com/&#34;&gt;Beaker&lt;/a&gt; via &lt;a href=&#34;dat://cameron.pfiffer.org&#34;&gt;dat://cameron.pfiffer.org&lt;/a&gt; or directly at &lt;a href=&#34;dat://d9bb03a966c47ea49469c48111085635a6388a532dfe671cab057688560ff3e4/&#34;&gt;dat://d9bb03a966c47ea49469c48111085635a6388a532dfe671cab057688560ff3e4/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Math Camp - Week 2</title>
      <link>/2018/09/09/math-camp-week-2/</link>
      <pubDate>Sun, 09 Sep 2018 18:33:01 -0700</pubDate>
      
      <guid>/2018/09/09/math-camp-week-2/</guid>
      <description>&lt;p&gt;The second week of math camp has been finished up. This week, we got into the nuts and bolts of matrices, eigenvalue decompositions, and a couple other linear algebra-type things. I will say, I did not think that I would be spending so much time doing homework &amp;ndash; everyone I knew said something along the lines of &amp;ldquo;only two hours a day? Sounds easy.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Turns out, I have spent at least 4+ hours a day on miscellaneous math homework. I&amp;rsquo;ve also been working on tutorials and documentation for the &lt;a href=&#34;https://github.com/TuringLang/Turing.jl&#34;&gt;Turing.jl&lt;/a&gt; project, so I&amp;rsquo;ve had little free time as of late. Still, being busy like this has been extremely rewarding.&lt;/p&gt;

&lt;p&gt;I have also been trying to cram in about 30-60 minutes a night of reading &lt;a href=&#34;https://www.springer.com/us/book/9780387310732&#34;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;, a book recommended by some of the smart folks at Turing. Spectacularly reading, especially for those of us who come from the frequentist persuasion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Math Camp - Week 1</title>
      <link>/2018/09/02/math-camp-week-1/</link>
      <pubDate>Sun, 02 Sep 2018 18:33:01 -0700</pubDate>
      
      <guid>/2018/09/02/math-camp-week-1/</guid>
      <description>&lt;p&gt;At the University of Oregon, all the economist and economist-light &lt;label for=&#34;econ&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;econ&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Here, &amp;ldquo;economist-light&amp;rdquo; just means all the business school disciplines; finance, marketing, accounting, etc. &lt;/span&gt;
 folks take a three week math camp designed to get them up to speed on everything they could ever possibly hope to know about the mathematics used in economics.&lt;/p&gt;

&lt;p&gt;Math camp started on Wednesday, August 29th, and so far I have had three math camp sessions which have shown me how little I know about absolutely anything.&lt;/p&gt;

&lt;p&gt;One of the biggest problems with being (i) mostly self-taught and (ii) highly computational is that I am not at all prepared for the abstract nature of so much of what mathematics is at its heart. Proofs, for example, remain mostly alien to me. Until Wednesday I had never written a proof of any kind, &lt;label for=&#34;proof&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;proof&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; If you saw the proofs I &lt;em&gt;have&lt;/em&gt; written down since Wednesday, you might say that I still have yet to right one. &lt;/span&gt;
 and I have never been in a math class so far removed from numbers or expressions that are grounded in \(ℝ^{1-3}\) are largely beyond my ken.&lt;/p&gt;

&lt;p&gt;Fortunately I am learning a tremendous amount of new things. So far we&amp;rsquo;ve covered:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Vectorspaces&lt;/li&gt;
&lt;li&gt;Subspaces&lt;/li&gt;
&lt;li&gt;Spans&lt;/li&gt;
&lt;li&gt;Bases&lt;/li&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Linear maps&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There&amp;rsquo;s been a total of eight questions on the homework so far, which require about 2-4 hours of my considerably-useless time. Honestly, I am having a lot of fun being in the dark on all this stuff, as it&amp;rsquo;s kind of what I signed up for with the whole doctorate thing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interesting Papers</title>
      <link>/2018/08/13/interesting-papers/</link>
      <pubDate>Mon, 13 Aug 2018 21:14:11 -0700</pubDate>
      
      <guid>/2018/08/13/interesting-papers/</guid>
      <description>&lt;p&gt;Via arXiv:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We develop a large-scale deep learning model to predict price movements from limit order book (LOB) data of cash equities. The architecture utilises convolutional filters to capture the spatial structure of the limit order books as well as LSTM modules to capture longer time dependencies. The model is trained using electronic market quotes from the London Stock Exchange. Our model delivers a remarkably stable out-of-sample prediction accuracy for a variety of instruments and outperforms existing methods such as Support Vector Machines, standard Multilayer Perceptrons, as well as other previously proposed convolutional neural network (CNN) architectures. The results obtained lead to good profits in a simple trading simulation, especially when compared with the baseline models. Importantly, our model translates well to instruments which were not part of the training set, indicating the model&amp;rsquo;s ability to extract universal features. In order to better understand these features and to go beyond a &amp;ldquo;black box&amp;rdquo; model, we perform a sensitivity analysis to understand the rationale behind the model predictions and reveal the components of LOBs that are most relevant. The ability to extract robust features which translate well to other instruments is an important property of our model which has many other applications.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Find the paper &lt;a href=&#34;https://arxiv.org/abs/1808.03668&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Finance</title>
      <link>/2018/07/09/why-finance/</link>
      <pubDate>Mon, 09 Jul 2018 19:36:58 -0700</pubDate>
      
      <guid>/2018/07/09/why-finance/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This coming fall, I will be a doctoral student at the University of Oregon. I wanted to get whatever thoughts I could ahead of my time there &amp;ndash; to try and make a &amp;ldquo;before and after&amp;rdquo;. What will I think is interesting now, and what will I think is interesting when (or if) I graduate? Will I have achieved whatever goals I thought I wanted to?&lt;/p&gt;

&lt;p&gt;To that end, I&amp;rsquo;ve collected some of my thoughts on finance, why I personally like it, and what I&amp;rsquo;m going to do after I graduate.&lt;/p&gt;

&lt;h2 id=&#34;what-is-finance&#34;&gt;What is finance?&lt;/h2&gt;

&lt;p&gt;For posterity, here is my definition of finance:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Finance is a social science concerned with how and when people move, use, and consume financial assets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finance is a &lt;em&gt;social science&lt;/em&gt; because finance would not exist without people. There would be nothing to pay for twinkies with, &lt;label for=&#34;twinkie&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;twinkie&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; And no twinkies for that matter.  &lt;/span&gt;
 no stocks to buy or sell, no bonds from sketchy south American countries.&lt;/p&gt;

&lt;p&gt;It is concerned wth &lt;em&gt;how&lt;/em&gt; and &lt;em&gt;why&lt;/em&gt; because those are both critical components of a dynamic system. How a system works is a nontrivial factor in why anyone interacts with a financial markets. Financial markets that are complex and unpleasant to use don&amp;rsquo;t find many participants &lt;label for=&#34;participants&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;participants&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Complex financial markets also generate middlemen and arbitrage opportunities. &lt;/span&gt;
. That &lt;em&gt;why&lt;/em&gt; is important because finance is ultimately concerned with the timing, magnitude, and nature of financial market activity from humans, and humans do not make choices in a vacuum. They make choices when confronted with circumstance and context.&lt;/p&gt;

&lt;p&gt;The financial activities that economic actors can take impact the entire system. For example, an elderly couple entering retirement has hopefully assembled a pool of assets they can rely upon for the rest of their lives. Over several decades, they will convert stocks and bonds to cash to meet liquidity needs using the financial markets. Selling those securities has a ripple effect throughout the market, though for most couples the selling of any single couple is unlikely to cause much change in the aggregate. But on this small scale there are numerous other entities who are impacted by those transactions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Brokers who negotiate the trades.&lt;/li&gt;
&lt;li&gt;Banks who might make markets in the sold securities.&lt;/li&gt;
&lt;li&gt;Clearinghouses who clear and settle the trades.&lt;/li&gt;
&lt;li&gt;Mutual funds, hedge funds, index funds, arbitrageurs, and speculators who might make decisions based on observed activity.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finance is the study of the network of entities who exchange assets with one another. Neat stuff.&lt;/p&gt;

&lt;h2 id=&#34;why-finance&#34;&gt;Why finance?&lt;/h2&gt;

&lt;p&gt;So why is finance something I&amp;rsquo;m into? My background would not indicate to anyone that finance is a lifelong passion of mine. I have a Bachelors of Science in Theater Arts. Not exactly finance.&lt;/p&gt;

&lt;p&gt;I had been exposed to some interesting stuff during my MSc in Finance. The first thing to pique my interest was algorithmic trading, which is the study of how to trade in an algorithmic fashion so as to meet some requirement, such as limited market impact or limited cost uncertainty. This is a heavily automated process. People can perform algorithmic trading without computers, but it is a slow and unplesant process.&lt;/p&gt;

&lt;p&gt;More broadly I came to notice a lot more of the curious ways in which technological systems impact financial markets. &lt;label for=&#34;trades&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;trades&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; This isn&amp;rsquo;t even counting the number of orders placed and revoked, which is orders of magnitudes greater. &lt;/span&gt;
Exchanges are heavily optimized for a tremendous amount of transactions. NASDAQ alone handled around &lt;a href=&#34;http://www.nasdaqtrader.com/Trader.aspx?id=DailyMarketSummary&#34;&gt;11.5 million trades&lt;/a&gt; on July 12th, 2018.&lt;/p&gt;

&lt;p&gt;High frequency traders use incredibly sophisticated software and hardware &lt;label for=&#34;fpga&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;fpga&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; My favorite thing among the HFT tricks is &lt;a href=&#34;https://quant.stackexchange.com/questions/10519/how-are-hft-systems-implemented-on-fpga-nowadays&#34;&gt;the use of FPGAs&lt;/a&gt;. &lt;/span&gt;
 to gain microseconds on their competitors. There&amp;rsquo;s some evidence to suggest that much of the &amp;ldquo;real&amp;rdquo; liqudity provided today comes from high frequency traders.&lt;/p&gt;

&lt;p&gt;What about cryptocurrencies? Many financial economists I have spoken with are somewhere between dismissive and uninterested. But to me they appear to be the perfect distillation of my interests &amp;ndash; sophisticated technological underpinnings, financial assets, and informational egalitarianism.&lt;/p&gt;

&lt;p&gt;I am interested in finance because it is interesting. We all have money, sometimes however little, and we all use it. But more importantly, we are still in the midst of what may be the largest change in financial markets since fiat money. We are still trying to figure out how computational power can effect financial markets.&lt;/p&gt;

&lt;h2 id=&#34;what-are-you-going-to-do-after-you-graduate&#34;&gt;What are you going to do after you graduate?&lt;/h2&gt;

&lt;p&gt;Academia absolutely has it&amp;rsquo;s allure. In academia there is substantially less pressure to produce fiscally viable research &amp;ndash; you can grind away on something merely because it is personally interesting or valuable in a nonmonetary sense either to your peers or to society&amp;rsquo;s understanding of financial markets. The resources you have access to are not small, but they are also not &lt;strong&gt;enormous&lt;/strong&gt; like they might be in the private sector.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;right-now&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;right-now&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; I say &amp;ldquo;right now&amp;rdquo; because everyone has informed me that my interests will change dramatically over the course of my studies. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As of right now, here are the broad interests I might like to work in:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Computational finance&lt;/li&gt;
&lt;li&gt;Asset pricing&lt;/li&gt;
&lt;li&gt;High-frequency, algorithmic, and systematic trading&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Computational financial engineering
&lt;label for=&#34;simon&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;simon&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; See &lt;em&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/contracts-icfp.pdf&#34;&gt;Composing contracts: An adventure in financial engineering&lt;/a&gt;&lt;/em&gt; by Jones, Eber, and Seward. This is not strictly finance, but it is something I am heavily interested in and would likely enjoy branching out into. I think my favorite thing about this paper is it was the first time I saw how &lt;a href=&#34;https://www.haskell.org/&#34;&gt;Haskell&lt;/a&gt; could be unique and useful. Granted, I am still not a particularly talented Haskell programmer, but it is always nice to dream. &lt;/span&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cryptocurrencies&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Blockchain technology&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fintech&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What about the private sector? The allure there is strong too. In the private sector I could have the responsibility for a thing I worked on. If it was a portion of a pricing engine for a market making desk, I would have responsibility for making sure that it is &lt;em&gt;accurate&lt;/em&gt;, &lt;em&gt;timely&lt;/em&gt;, and &lt;em&gt;faultless&lt;/em&gt;. That responsibility is something I don&amp;rsquo;t think people get in quite so direct a manner in academia. I think the problems would also tend to be a bit more focused and less nebulous which I&amp;rsquo;ve noticed tends to substantially focus my ability to work.&lt;/p&gt;

&lt;p&gt;I suspect it might be fun to work for &lt;a href=&#34;https://www.janestreet.com/&#34;&gt;Jane Street&lt;/a&gt;, though I doubt I am smart enough to work there. Most everyone I&amp;rsquo;ve seen give talks from Jane Street is absolutely whip smart and quick. I like that they use &lt;a href=&#34;https://ocaml.org/&#34;&gt;OCaml&lt;/a&gt; for everything. I like that they are market makers, I like that they are problem solvers, and I like that they are technologists. There are a lot of people in finance as of 2018 who are very much rooted in the way finance worked in previous decades. It doesn&amp;rsquo;t work that way anymore. Markets are ever more automated and complex.&lt;/p&gt;

&lt;p&gt;I guess I&amp;rsquo;ll just have to find out in a couple years, and see how I feel about how hard the PhD was for me. I suspect I&amp;rsquo;ll end up going for whatever is the most difficult thing to do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Theory</title>
      <link>/2018/06/25/information-theory/</link>
      <pubDate>Mon, 25 Jun 2018 21:30:55 -0700</pubDate>
      
      <guid>/2018/06/25/information-theory/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;alink&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;alink&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; &lt;a href=&#34;https://www.thriftbooks.com/w/an-introduction-to-information-theory_john-robinson-pierce/294512/?mkwid=sbM6YJYtB%7cdc&amp;amp;pcrid=70112900832&amp;amp;pkw=&amp;amp;pmt=&amp;amp;plc=&amp;amp;gclid=Cj0KCQjwpcLZBRCnARIsAMPBgF28X1wNo0AKYjuBEjeeBKTk73gnywqwZUJWQFnZ9DQwigTTOEG7_R8aArg3EALw_wcB#isbn=0486240614&amp;amp;idiq=3821805&#34;&gt;&lt;em&gt;An Introduction to Information Theory&lt;/em&gt;&lt;/a&gt; by John R. Pierce. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve recently been perusing an introductory text of &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_theory&#34;&gt;information theory&lt;/a&gt;. Information theory is one of those things that I have always wanted to look into but never gotten around to &amp;ndash; but now I am an adult with real things to try not to do, so I figured what the hell.&lt;/p&gt;

&lt;p&gt;Information theory is the scientific study of how information can be quantified, stored, and communicated efficiently. It&amp;rsquo;s fascinating to read the version of the book I have as the author (and Claude Shannon, the father of information theory) all viewed everything as telephonic or telegraphic. Pierce mentions computers a handful of times, but often only to describe what they are incapable of doing.&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; He calls them &amp;ldquo;automata&amp;rdquo;, which is such a lovely old-timey phrase. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Much of the content regards speech and how we can predict, transmit, and describe it. It appears to be primarily based on probability theory, and for good reason &amp;ndash; if you know that &lt;strong&gt;E&lt;/strong&gt; appears 13% of the time and &lt;strong&gt;Z&lt;/strong&gt; appears almost never, you can make asusmptions about how you code letters by assigning easily transmitted values for common symbols and more complex values for infrequent symbols.&lt;/p&gt;

&lt;p&gt;The primary reason I&amp;rsquo;m reading it is for the applications to financial markets. A lot of the reason why information theory came about was because there was a need for sophisticated &lt;em&gt;signal processing&lt;/em&gt; techniques during World War II. &lt;a href=&#34;https://en.wikipedia.org/wiki/Signal_processing&#34;&gt;Signal processing&lt;/a&gt; is something commonly applied to finance. Trades indicate resources, desire, risk tolerance, what have you &amp;ndash; but there&amp;rsquo;s also a lot of noise. How do you tell people who are &lt;em&gt;informed&lt;/em&gt; from people who are &lt;em&gt;uninformed&lt;/em&gt;? How do you know whether a trade is actually meaningful to the long-term price of the stock?&lt;/p&gt;

&lt;p&gt;Dunno. Thought I&amp;rsquo;d read about it though.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Links</title>
      <link>/links/</link>
      <pubDate>Wed, 20 Jun 2018 06:56:21 -0700</pubDate>
      
      <guid>/links/</guid>
      <description>

&lt;p&gt;Miscellaneous links I think are good.&lt;/p&gt;

&lt;h2 id=&#34;books&#34;&gt;Books&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;data-science-econometrics&#34;&gt;Data Science &amp;amp; Econometrics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://otexts.org/fpp2/&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econometrics-with-r.org/&#34;&gt;Econometrics with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;machine-learning-ai&#34;&gt;Machine Learning &amp;amp; AI&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.stewartcalculus.com/media/16_home.php&#34;&gt;Calculus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://learnyouahaskell.com/&#34;&gt;Learn You a Haskell&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;services&#34;&gt;Services&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;media&#34;&gt;Media&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.plex.tv/&#34;&gt;Plex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;news&#34;&gt;News&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.wsj.com/&#34;&gt;Wall Street Journal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.washingtonpost.com/&#34;&gt;Washington Post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fivethirtyeight.com/&#34;&gt;FiveThirtyEight&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;education&#34;&gt;Education&lt;/h3&gt;

&lt;p&gt;&lt;label for=&#34;khan&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;khan&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; I&amp;rsquo;ve learned pretty much everything I know about math from Khan Academy. Best place on the internet. &lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/&#34;&gt;Khan Academy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;code-1&#34;&gt;Code&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;languages&#34;&gt;Languages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/dotnet/csharp/&#34;&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.haskell.org/&#34;&gt;Haskell&lt;/a&gt; &lt;label for=&#34;haskell&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;haskell&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Do you like purity? Do you like a typing system so fierce it&amp;rsquo;ll make your head spin? Do you like unlearning everything you ever thought you knew about programming? Try Haskell! &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;probabalistic-programming&#34;&gt;Probabalistic Programming&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;An &lt;a href=&#34;https://www.cs.cornell.edu/courses/cs4110/2016fa/lectures/lecture33.html&#34;&gt;introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/TuringLang/Turing.jl&#34;&gt;Turing.jl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;general&#34;&gt;General&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;cool-sites&#34;&gt;Cool Sites&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2019/visual-exploration-gaussian-processes/&#34;&gt;Gaussian Processes at Distill&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s all, folks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Good Book</title>
      <link>/2018/06/20/good-book/</link>
      <pubDate>Wed, 20 Jun 2018 06:51:27 -0700</pubDate>
      
      <guid>/2018/06/20/good-book/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a spectacular book on using R for forecasting online &amp;ndash; so far, it seems like a breezy, informative read. &lt;a href=&#34;https://otexts.org/fpp2/&#34;&gt;Take a look here&lt;/a&gt;. No more to note on that, just thought I&amp;rsquo;d stick a link up.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Is Insufficient</title>
      <link>/2018/06/08/deep-learning-is-insufficient/</link>
      <pubDate>Fri, 08 Jun 2018 06:20:22 -0700</pubDate>
      
      <guid>/2018/06/08/deep-learning-is-insufficient/</guid>
      <description>


&lt;p&gt;Via &lt;a href=&#34;https://arxiv.org/abs/1801.00631&#34;&gt;Gary Marcus&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thus, for example, in a system like Lerer et al’s (2016) efforts to learn about the physics of falling towers, there is no prior knowledge of physics (beyond what is implied in convolution). Newton’s laws, for example, are not explicitly encoded; the system instead (to some limited degree) approximates them by learning contingencies from raw, pixel level data. As I note in a forthcoming paper in innate (Marcus, in prep) researchers in deep learning appear to have a very strong bias against including prior knowledge even when (as in the case of physics) that prior knowledge is well known.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Marcus’ article contains a wide variety of detailed critiques on why he believes that deep learning is flawed in a lot of ways – namely, that it is not the end-all-be-all of artificial intelligence, only that it is a component part of future generalized intelligence. It’s a wonderful read.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Economist</title>
      <link>/2018/06/06/privacy-economist/</link>
      <pubDate>Wed, 06 Jun 2018 05:33:04 -0700</pubDate>
      
      <guid>/2018/06/06/privacy-economist/</guid>
      <description>


&lt;p&gt;Via &lt;a href=&#34;https://www.economist.com/technology-quarterly/2018-05-02/justice?utm_campaign=Data_Elixir&amp;amp;utm_medium=email&amp;amp;utm_source=Data_Elixir_185&#34;&gt;The Economist&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some people argue that those who have done nothing wrong need not worry. But that justifies limitless state surveillance, and risks a chilling effect on citizens’ fundamental civil liberties. After all, if you are not planning crimes while talking on the phone, why not just let police officers listen to every call? Police need oversight not because they are bad people but because maintaining the appropriate balance between liberty and security requires constant vigilance by engaged citizens. This is doubly true for new technologies that make police better at their jobs when policy, due process and public opinion have not caught up.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Rust &amp; More</title>
      <link>/2018/06/03/rust-more/</link>
      <pubDate>Sun, 03 Jun 2018 21:01:18 -0700</pubDate>
      
      <guid>/2018/06/03/rust-more/</guid>
      <description>


&lt;p&gt;For the past two or so weeks I’ve been spending a lot of time writing &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;. Rust is a really spectacularly designed language with perhaps the greatest package manager&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;It’s called &lt;a href=&#34;https://crates.io/&#34;&gt;Cargo&lt;/a&gt;.&lt;/span&gt; I have ever experienced.&lt;/p&gt;
&lt;p&gt;Rust is fascinating to me because it’s one of my first true compiled languages. In contrast to the other languages I’m somewhat good at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R (mostly interpereted)&lt;/li&gt;
&lt;li&gt;Python (interpereted)&lt;/li&gt;
&lt;li&gt;C# (JIT compiled)&lt;/li&gt;
&lt;li&gt;Julia (JIT compiled)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am only passingly familiar with C and have mostly repressed C++, so Rust was both new and refreshing to me. The workflows are strikingly different – with Julia or R or what have you, I go through a very rapid iterative process. Write something, run it, see what worked, fix it, move on to the next issue. It’s very slapdash.&lt;/p&gt;
&lt;p&gt;With Rust (and, I presume other compiled languages), I have to stop and &lt;em&gt;think&lt;/em&gt; a lot more about what I’m doing and how I’m doing it. This is a bit more important in Rust because of the concept of &lt;strong&gt;ownership&lt;/strong&gt;, an entirely bizarre concept of who owns what thing in any given program.&lt;/p&gt;
&lt;p&gt;My (admittedly frail) understanding of ownership is that everything that exists in a program is owned by a varible. Once that variable goes out of scope, the memory allocated goes with it. It’s how Rust manages memory safety. It has a pretty steep learning curve initially, but once you’ve learned it it’s almost effortless to use.&lt;/p&gt;
&lt;p&gt;All in all, it’s a spectacular language. Fast, too. And ultra strongly typed with none of the rigid puritanical stuff that comes with Haskell.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Down With Business</title>
      <link>/2018/05/22/down-with-business/</link>
      <pubDate>Tue, 22 May 2018 05:39:43 -0700</pubDate>
      
      <guid>/2018/05/22/down-with-business/</guid>
      <description>


&lt;p&gt;From &lt;a href=&#34;https://www.theguardian.com/news/2018/apr/27/bulldoze-the-business-school&#34;&gt;Martin Parker&lt;/a&gt; at the Guardian:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see how this works if we look a bit more closely at the business-school curriculum and how it is taught. Take finance, for instance. This is a field concerned with understanding how people with money invest it. It assumes that there are people with money or capital that can be used as security for money, and hence it also assumes substantial inequalities of income and wealth. The greater the inequalities within any given society, the greater the interest in finance, as well as the market in luxury yachts. Finance academics almost always assume that earning rent on capital (however it was acquired) is a legitimate and perhaps even praiseworthy activity, with skilful investors being lionised for their technical skills and success. The purpose of this form of knowledge is to maximise the rent from wealth, often by developing mathematical or legal mechanisms that can multiply it. Successful financial strategies are those that produce the maximum return in the shortest period, and hence that further exacerbate the social inequalities that made them possible in the first place.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The article in its entirety is Martin’s rejection of the business school, and how it fosters some ideological problems amongst its students. His characterization of finance &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;He had many other points, but I am not really qualified to talk about operations management.&lt;/span&gt; seems a bit reductive.&lt;/p&gt;
&lt;p&gt;First, finance does not necessitate inequality. Finance is merely the grease of capital, because without it money wouldn’t move as much as it does – the presence of a tool does not force its use for ill-ends. Certainly, finance allows those with capital to gain more from that capital base than the poor, but at the same time, well-capitalized people and institutions are theoretically the ones who are most likely to make investments in capital goods &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;1. Capital goods benefit us because they are expensive, and the production of (most) capital goods touches lots of hands.&lt;/span&gt; or research, things which benefit us all.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;2. Research is good because it allows society to do things it couldn’t before, like cure polio, send messages across the world, or write blog posts.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Second, I don’t know that characterizing finance as the science of extracting rent from wealth is the right way to go about it. Again, yes, those with capital benefit more from finance, but so does society. Finance is the science of making sure those who &lt;em&gt;have&lt;/em&gt; assets can give money to those who &lt;em&gt;do not&lt;/em&gt;, and can do better things with that money. Without finance we would have no modern agriculture, iPhones, shopping malls, computers, or nearly anything else – if investors cannot engage in maturity and liquidity transformation, nothing will be done.&lt;/p&gt;
&lt;p&gt;The article is very well written and worth a read, though I would hesitate to destroy business schools just yet; I’ve only just been accepted to one. Wait about five years, please.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Updates</title>
      <link>/2018/05/12/updates/</link>
      <pubDate>Sat, 12 May 2018 15:08:05 -0700</pubDate>
      
      <guid>/2018/05/12/updates/</guid>
      <description>


&lt;p&gt;I’m trying to get better about writing more things. To that end, I figured I should post some general updates on the things I’ve done lately.&lt;/p&gt;
&lt;div id=&#34;new-york&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New York&lt;/h2&gt;
&lt;p&gt;My &lt;a href=&#34;qpfiffer.com&#34;&gt;brother&lt;/a&gt; and I just got back from a week-long trip to upstate New York to see our grandmother. She’s a cool lady, and we figured since we’re both sort of adults with jobs we should drop in. I raced BMX a little bit, wiped out a bit, and generally had a good time with cool folks. I also had the advantage of driving down to Palmyra/Hershey Pennsylvania to seet my cousins, three of whom are traditionally brilliant and one of which is at the fantastic &lt;a href=&#34;https://www.mhskids.org/&#34;&gt;Milton Hershey School&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-12-updates_files/figure-html/fig-margin-1.png&#34; width=&#34;672&#34; /&gt; &lt;em&gt;New York State&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;university-of-oregon-visit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;University of Oregon Visit&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-12-updates_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt; &lt;em&gt;Eugene, OR – Home of the University of Oregon&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I had previously mentioned that I was fortunate enough to be accepted to a PhD program at the University of Oregon, a superb school with some brilliant people. I drove down to Eugene this past Friday to introduce myself to everyone. I was honestly a bit afraid that my market microstructure interests would be swept under the rug to work on whatever the faculty was most skilled in&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, but that was very quickly laid to rest when I spoke with everyone. I’m perhaps the happiest I have been in a long time, and I drove the two hours home with a bit of a grin on my face.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chapel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chapel&lt;/h2&gt;
&lt;p&gt;Commensurate with my inability to focus on any particular piece of software for more than a week, I spent a good portion of the past couple days messing around with &lt;a href=&#34;https://github.com/chapel-lang/chapel&#34;&gt;Chapel&lt;/a&gt;, Cray’s simple parallel solution. It was probably the most substantive interation I’ve ever had with a sort-of-C language. Man, it is incredibly simple to do some otherwise complicated parallelization feats, and it’s all first class. My issue with it is that it seems like it may die long before it ever catches on – it’s been in development for years now and the package ecosystem is miniscule.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Primarily corporate finance, asset management, and asset pricing. Mostly on the empirical side.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PhD</title>
      <link>/2018/04/19/phd/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/19/phd/</guid>
      <description>


&lt;p&gt;I’m going to be a PhD student at the &lt;a href=&#34;https://business.uoregon.edu/phd/concentrations/finance&#34;&gt;University of Oregon&lt;/a&gt;! I am looking forward to working with some spectacularly bright people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scientific Papers</title>
      <link>/2018/04/05/scientific-papers/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/05/scientific-papers/</guid>
      <description>&lt;p&gt;An &lt;a href=&#34;https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/&#34;&gt;excellent article&lt;/a&gt; from the Atlantic explores the way that computational essays haven&amp;rsquo;t seemed to penetrate academia. There&amp;rsquo;s some information about the contest between open source and for-profit tools, like this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But it’s hard to sell the scientific community on a piece of commercial software. Even though Wolfram Research has given away a free Mathematica notebook viewer for years, and even though most major universities already have a site license that lets their students and faculty use Mathematica freely, it might be too much to ask publishers to abandon PDFs, an open format, for a proprietary product.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Fair point. Why use Mathematica when you have Jupyter, which gives you access to R (in which this post is written), Julia, Python, and nearly any other worthwhile language?&lt;/p&gt;

&lt;p&gt;The broader point is about the failings in take up in the scientific community. Honestly, it&amp;rsquo;s one of those blatantly obvious points I don&amp;rsquo;t ever think about. There&amp;rsquo;s very little reason to publish the same droll LaTeX paper when you can spit out a highly interactive web document even just a PDF that exposes some of your code &amp;ndash; it&amp;rsquo;s just about providing information and making sure people &lt;em&gt;understand&lt;/em&gt; it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Business Cards</title>
      <link>/2018/02/27/business-cards/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/27/business-cards/</guid>
      <description>


&lt;p&gt;A couple months ago while I was bumming around London trying to get people to give me work, I made some (I think) really cool business card designs. I made a couple of different card designs that shared the same front and had varying backs. I have gotten some pretty good feedback whenever I’ve given these out.&lt;/p&gt;
&lt;p&gt;All the designs are made by me in Adobe Illustrator. You can get similar cards made at &lt;a href=&#34;moo.com&#34; class=&#34;uri&#34;&gt;moo.com&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;front&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Front&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Boilerplate front matter.&lt;/span&gt; &lt;img src=&#34;/cards/front.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Accounting&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A graphical representation of the accounting identity. This might be my least favorite design.&lt;/span&gt; &lt;img src=&#34;/cards/accounting.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;capital-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Capital Structure&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A pie chart showing the way that companies can be financed, along with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Weighted_average_cost_of_capital&#34;&gt;weighted average cost of capital&lt;/a&gt; function.&lt;/span&gt; &lt;img src=&#34;/cards/cap.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;event-study&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Event Study&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A visual depiction of how event studies are conducted in finance and economics.&lt;/span&gt; &lt;img src=&#34;/cards/event.jpg&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-descent-type-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gradient Descent (Type 1)&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;The first style of gradient descent I designed. I did not use this one in my actual cards in favor of the next version.&lt;/span&gt; &lt;img src=&#34;/cards/graph1.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-descent-type-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gradient Descent (Type 2)&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;This one is by far the most popular one I have made, and my personal favorite. It’s a representation of 3d &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;gradient descent&lt;/a&gt;, a optimization algorithm commonly used in machine learning.&lt;/span&gt; &lt;img src=&#34;/cards/graph2.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;markowitz-portfolio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Markowitz Portfolio&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;The Markowitz &lt;a href=&#34;https://en.wikipedia.org/wiki/Modern_portfolio_theory&#34;&gt;mean-variance portfolio&lt;/a&gt;.&lt;/span&gt; &lt;img src=&#34;/cards/markowitz.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Neural Network&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A representation of a small neural net. In retrospect, this design is a bit cluttered and unpleasant, and I’m often reluctant to give it out.&lt;/span&gt; &lt;img src=&#34;/cards/nn.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Options&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;This is a short &lt;a href=&#34;https://en.wikipedia.org/wiki/Butterfly_(options)&#34;&gt;butterfly option&lt;/a&gt;, shown with all the constituent options.&lt;/span&gt; &lt;img src=&#34;/cards/options.jpg&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Linear regression. Nice, clean, and simple&lt;/span&gt; &lt;img src=&#34;/cards/regression.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-time-value-of-money&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Time Value of Money&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A fundamental part of finance and economics. I really like the way this one looks.&lt;/span&gt; &lt;img src=&#34;/cards/tvm.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you’d like a business card, send me an email at &lt;a href=&#34;mailto:cameron@pfiffer.org&#34;&gt;cameron@pfiffer.org&lt;/a&gt; and maybe I’ll mail you one. They’re pretty cool.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tufte Stufte</title>
      <link>/2018/02/26/tufte-stufte/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/26/tufte-stufte/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/proj4js/proj4.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/highcharts/css/motion.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/highstock.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/highcharts-3d.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/highcharts-more.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/annotations.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/broken-axis.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/data.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/drilldown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/exporting.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/funnel.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/heatmap.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/map.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/no-data-to-display.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/offline-exporting.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/solid-gauge.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/treemap.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/annotations.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/draggable-legend.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/draggable-points.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/export-csv.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/grouped-categories.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/motion.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/pattern-fill-v2.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/tooltip-delay.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/custom/reset.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/custom/symbols-extra.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/custom/text-symbols.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/fontawesome/font-awesome.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/rmarkdown-libs/htmlwdgtgrid/htmlwdgtgrid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/highchart-binding/highchart.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; a margin note.&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;This &lt;em&gt;is&lt;/em&gt; a margin note. As you can see, it is to the right of the left, and it indeed appears to be smaller.&lt;/span&gt; Were it a margin note as one might presuppose, it would be to the right of the left and perhaps a bit smaller.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;newthought&#34;&gt;In the having of a new thought,&lt;/span&gt; I find myself overcome with thoughtfulness. You might say, perhaps you would be! &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;You would be overcome with thoughtfulness, as it is a thought.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thank you for saying so.&lt;/p&gt;
&lt;p&gt;Quick, look at some math! &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[ x = 2+2 \]&lt;/span&gt;&lt;/span&gt; Tell me, what is &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Even better, here is a chart with made-up numbers. Look how fancy and &lt;em&gt;&lt;strong&gt;interactive&lt;/strong&gt;&lt;/em&gt; it is.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;highchart html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;hc_opts&#34;:{&#34;title&#34;:{&#34;text&#34;:&#34;Some Random Numbers&#34;},&#34;yAxis&#34;:{&#34;title&#34;:{&#34;text&#34;:null}},&#34;credits&#34;:{&#34;enabled&#34;:false},&#34;exporting&#34;:{&#34;enabled&#34;:false},&#34;plotOptions&#34;:{&#34;series&#34;:{&#34;turboThreshold&#34;:0},&#34;treemap&#34;:{&#34;layoutAlgorithm&#34;:&#34;squarified&#34;},&#34;bubble&#34;:{&#34;minSize&#34;:5,&#34;maxSize&#34;:25}},&#34;annotationsOptions&#34;:{&#34;enabledButtons&#34;:false},&#34;tooltip&#34;:{&#34;delayForDisplay&#34;:10},&#34;chart&#34;:{&#34;type&#34;:&#34;scatter&#34;},&#34;series&#34;:[{&#34;data&#34;:[{&#34;x&#34;:-2.00105432964465,&#34;y&#34;:-2.88115513128663},{&#34;x&#34;:1.69395853857261,&#34;y&#34;:1.97180250490159},{&#34;x&#34;:0.258289147957936,&#34;y&#34;:-0.239482012536848},{&#34;x&#34;:0.553222285812558,&#34;y&#34;:2.34415852933954},{&#34;x&#34;:1.89746186591957,&#34;y&#34;:5.43937040769522},{&#34;x&#34;:-0.795813697664382,&#34;y&#34;:-1.21905610173255},{&#34;x&#34;:-1.52625691634341,&#34;y&#34;:-1.81730698575304},{&#34;x&#34;:-0.39507050087908,&#34;y&#34;:-1.0932591894235},{&#34;x&#34;:0.93066776670334,&#34;y&#34;:2.81221446244224},{&#34;x&#34;:-0.370167736234375,&#34;y&#34;:-0.334107214472779},{&#34;x&#34;:-0.438063484790336,&#34;y&#34;:0.668336733308717},{&#34;x&#34;:1.56813462946513,&#34;y&#34;:3.78811167109424},{&#34;x&#34;:-0.174834974067972,&#34;y&#34;:-0.584157629035353},{&#34;x&#34;:0.159541444208419,&#34;y&#34;:0.934886997094617},{&#34;x&#34;:-0.783732745650928,&#34;y&#34;:-0.241464277801196}]}]},&#34;theme&#34;:{&#34;colors&#34;:[&#34;#737373&#34;,&#34;#D8D7D6&#34;,&#34;#B2B0AD&#34;,&#34;#8C8984&#34;],&#34;chart&#34;:{&#34;style&#34;:{&#34;fontFamily&#34;:&#34;Cardo&#34;}},&#34;xAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;},&#34;yAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;,&#34;tickWidth&#34;:1,&#34;gridLineColor&#34;:&#34;transparent&#34;},&#34;legend&#34;:{&#34;enabled&#34;:false}},&#34;conf_opts&#34;:{&#34;global&#34;:{&#34;Date&#34;:null,&#34;VMLRadialGradientURL&#34;:&#34;http =//code.highcharts.com/list(version)/gfx/vml-radial-gradient.png&#34;,&#34;canvasToolsURL&#34;:&#34;http =//code.highcharts.com/list(version)/modules/canvas-tools.js&#34;,&#34;getTimezoneOffset&#34;:null,&#34;timezoneOffset&#34;:0,&#34;useUTC&#34;:true},&#34;lang&#34;:{&#34;contextButtonTitle&#34;:&#34;Chart context menu&#34;,&#34;decimalPoint&#34;:&#34;.&#34;,&#34;downloadJPEG&#34;:&#34;Download JPEG image&#34;,&#34;downloadPDF&#34;:&#34;Download PDF document&#34;,&#34;downloadPNG&#34;:&#34;Download PNG image&#34;,&#34;downloadSVG&#34;:&#34;Download SVG vector image&#34;,&#34;drillUpText&#34;:&#34;Back to {series.name}&#34;,&#34;invalidDate&#34;:null,&#34;loading&#34;:&#34;Loading...&#34;,&#34;months&#34;:[&#34;January&#34;,&#34;February&#34;,&#34;March&#34;,&#34;April&#34;,&#34;May&#34;,&#34;June&#34;,&#34;July&#34;,&#34;August&#34;,&#34;September&#34;,&#34;October&#34;,&#34;November&#34;,&#34;December&#34;],&#34;noData&#34;:&#34;No data to display&#34;,&#34;numericSymbols&#34;:[&#34;k&#34;,&#34;M&#34;,&#34;G&#34;,&#34;T&#34;,&#34;P&#34;,&#34;E&#34;],&#34;printChart&#34;:&#34;Print chart&#34;,&#34;resetZoom&#34;:&#34;Reset zoom&#34;,&#34;resetZoomTitle&#34;:&#34;Reset zoom level 1:1&#34;,&#34;shortMonths&#34;:[&#34;Jan&#34;,&#34;Feb&#34;,&#34;Mar&#34;,&#34;Apr&#34;,&#34;May&#34;,&#34;Jun&#34;,&#34;Jul&#34;,&#34;Aug&#34;,&#34;Sep&#34;,&#34;Oct&#34;,&#34;Nov&#34;,&#34;Dec&#34;],&#34;thousandsSep&#34;:&#34; &#34;,&#34;weekdays&#34;:[&#34;Sunday&#34;,&#34;Monday&#34;,&#34;Tuesday&#34;,&#34;Wednesday&#34;,&#34;Thursday&#34;,&#34;Friday&#34;,&#34;Saturday&#34;]}},&#34;type&#34;:&#34;chart&#34;,&#34;fonts&#34;:&#34;Cardo&#34;,&#34;debug&#34;:false},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Don’t worry, it’s quite boring – you’re not missing out on anything.&lt;/span&gt;But no, in all seriousness, I thought I’d take the chance to explore &lt;a href=&#34;https://en.wikipedia.org/wiki/B%C3%A9zier_curve&#34;&gt;Bezier curves&lt;/a&gt; while I’m mucking about with some fun looking theming. Bezier curves came up at work, the details of which I don’t really know whether or not I can share. So, I won’t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bezier &amp;lt;- function(t, p) {
  # A simple Bezier curve
  x = (1 - t) * (1 - t) * p[1, 1] + 2 * (1 - t) * t * p[2,1] + t * t * p[3,1]
  y = (1 - t) * (1 - t) * p[1, 2] + 2 * (1 - t) * t * p[2,2] + t * t * p[3,2]
  
  return(c(x,y))
}

points &amp;lt;- matrix(c(0, 5, 1, 0, 2, 3), ncol = 2)
line &amp;lt;- matrix(bezier(0:100/100, points), ncol=2)
head(line)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]   [,2]
## [1,] 0.0000 0.0000
## [2,] 0.0991 0.0399
## [3,] 0.1964 0.0796
## [4,] 0.2919 0.1191
## [5,] 0.3856 0.1584
## [6,] 0.4775 0.1975&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Basically, we just plot a bunch of smaller points on a new, continuously defined function that has a nifty little bend in it.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;highchart html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;hc_opts&#34;:{&#34;title&#34;:{&#34;text&#34;:&#34;A Bezier Curve&#34;},&#34;yAxis&#34;:{&#34;title&#34;:{&#34;text&#34;:null}},&#34;credits&#34;:{&#34;enabled&#34;:false},&#34;exporting&#34;:{&#34;enabled&#34;:false},&#34;plotOptions&#34;:{&#34;series&#34;:{&#34;turboThreshold&#34;:0},&#34;treemap&#34;:{&#34;layoutAlgorithm&#34;:&#34;squarified&#34;},&#34;bubble&#34;:{&#34;minSize&#34;:5,&#34;maxSize&#34;:25}},&#34;annotationsOptions&#34;:{&#34;enabledButtons&#34;:false},&#34;tooltip&#34;:{&#34;delayForDisplay&#34;:10},&#34;chart&#34;:{&#34;type&#34;:&#34;line&#34;},&#34;series&#34;:[{&#34;data&#34;:[[0,0],[0.0991,0.0399],[0.1964,0.0796],[0.2919,0.1191],[0.3856,0.1584],[0.4775,0.1975],[0.5676,0.2364],[0.6559,0.2751],[0.7424,0.3136],[0.8271,0.3519],[0.91,0.39],[0.9911,0.4279],[1.0704,0.4656],[1.1479,0.5031],[1.2236,0.5404],[1.2975,0.5775],[1.3696,0.6144],[1.4399,0.6511],[1.5084,0.6876],[1.5751,0.7239],[1.64,0.76],[1.7031,0.7959],[1.7644,0.8316],[1.8239,0.8671],[1.8816,0.9024],[1.9375,0.9375],[1.9916,0.9724],[2.0439,1.0071],[2.0944,1.0416],[2.1431,1.0759],[2.19,1.11],[2.2351,1.1439],[2.2784,1.1776],[2.3199,1.2111],[2.3596,1.2444],[2.3975,1.2775],[2.4336,1.3104],[2.4679,1.3431],[2.5004,1.3756],[2.5311,1.4079],[2.56,1.44],[2.5871,1.4719],[2.6124,1.5036],[2.6359,1.5351],[2.6576,1.5664],[2.6775,1.5975],[2.6956,1.6284],[2.7119,1.6591],[2.7264,1.6896],[2.7391,1.7199],[2.75,1.75],[2.7591,1.7799],[2.7664,1.8096],[2.7719,1.8391],[2.7756,1.8684],[2.7775,1.8975],[2.7776,1.9264],[2.7759,1.9551],[2.7724,1.9836],[2.7671,2.0119],[2.76,2.04],[2.7511,2.0679],[2.7404,2.0956],[2.7279,2.1231],[2.7136,2.1504],[2.6975,2.1775],[2.6796,2.2044],[2.6599,2.2311],[2.6384,2.2576],[2.6151,2.2839],[2.59,2.31],[2.5631,2.3359],[2.5344,2.3616],[2.5039,2.3871],[2.4716,2.4124],[2.4375,2.4375],[2.4016,2.4624],[2.3639,2.4871],[2.3244,2.5116],[2.2831,2.5359],[2.24,2.56],[2.1951,2.5839],[2.1484,2.6076],[2.0999,2.6311],[2.0496,2.6544],[1.9975,2.6775],[1.9436,2.7004],[1.8879,2.7231],[1.8304,2.7456],[1.7711,2.7679],[1.71,2.79],[1.6471,2.8119],[1.5824,2.8336],[1.5159,2.8551],[1.4476,2.8764],[1.3775,2.8975],[1.3056,2.9184],[1.2319,2.9391],[1.1564,2.9596],[1.0791,2.9799],[1,3]]}]},&#34;theme&#34;:{&#34;colors&#34;:[&#34;#737373&#34;,&#34;#D8D7D6&#34;,&#34;#B2B0AD&#34;,&#34;#8C8984&#34;],&#34;chart&#34;:{&#34;style&#34;:{&#34;fontFamily&#34;:&#34;Cardo&#34;}},&#34;xAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;},&#34;yAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;,&#34;tickWidth&#34;:1,&#34;gridLineColor&#34;:&#34;transparent&#34;},&#34;legend&#34;:{&#34;enabled&#34;:false}},&#34;conf_opts&#34;:{&#34;global&#34;:{&#34;Date&#34;:null,&#34;VMLRadialGradientURL&#34;:&#34;http =//code.highcharts.com/list(version)/gfx/vml-radial-gradient.png&#34;,&#34;canvasToolsURL&#34;:&#34;http =//code.highcharts.com/list(version)/modules/canvas-tools.js&#34;,&#34;getTimezoneOffset&#34;:null,&#34;timezoneOffset&#34;:0,&#34;useUTC&#34;:true},&#34;lang&#34;:{&#34;contextButtonTitle&#34;:&#34;Chart context menu&#34;,&#34;decimalPoint&#34;:&#34;.&#34;,&#34;downloadJPEG&#34;:&#34;Download JPEG image&#34;,&#34;downloadPDF&#34;:&#34;Download PDF document&#34;,&#34;downloadPNG&#34;:&#34;Download PNG image&#34;,&#34;downloadSVG&#34;:&#34;Download SVG vector image&#34;,&#34;drillUpText&#34;:&#34;Back to {series.name}&#34;,&#34;invalidDate&#34;:null,&#34;loading&#34;:&#34;Loading...&#34;,&#34;months&#34;:[&#34;January&#34;,&#34;February&#34;,&#34;March&#34;,&#34;April&#34;,&#34;May&#34;,&#34;June&#34;,&#34;July&#34;,&#34;August&#34;,&#34;September&#34;,&#34;October&#34;,&#34;November&#34;,&#34;December&#34;],&#34;noData&#34;:&#34;No data to display&#34;,&#34;numericSymbols&#34;:[&#34;k&#34;,&#34;M&#34;,&#34;G&#34;,&#34;T&#34;,&#34;P&#34;,&#34;E&#34;],&#34;printChart&#34;:&#34;Print chart&#34;,&#34;resetZoom&#34;:&#34;Reset zoom&#34;,&#34;resetZoomTitle&#34;:&#34;Reset zoom level 1:1&#34;,&#34;shortMonths&#34;:[&#34;Jan&#34;,&#34;Feb&#34;,&#34;Mar&#34;,&#34;Apr&#34;,&#34;May&#34;,&#34;Jun&#34;,&#34;Jul&#34;,&#34;Aug&#34;,&#34;Sep&#34;,&#34;Oct&#34;,&#34;Nov&#34;,&#34;Dec&#34;],&#34;thousandsSep&#34;:&#34; &#34;,&#34;weekdays&#34;:[&#34;Sunday&#34;,&#34;Monday&#34;,&#34;Tuesday&#34;,&#34;Wednesday&#34;,&#34;Thursday&#34;,&#34;Friday&#34;,&#34;Saturday&#34;]}},&#34;type&#34;:&#34;chart&#34;,&#34;fonts&#34;:&#34;Cardo&#34;,&#34;debug&#34;:false},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Neato. What a great Bezier curve. Thanks to &lt;a href=&#34;https://stackoverflow.com/a/5634528&#34;&gt;xan&lt;/a&gt; on StackOverflow for providing the pseudocode for that Bezier curve.&lt;/p&gt;
&lt;p&gt;Now, I’d like to ask you to hang out with that curve for a little while and consider it’s elegance, if you would be so kind.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Corporate Debt</title>
      <link>/2017/09/12/corporate-debt/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/12/corporate-debt/</guid>
      <description>


&lt;p&gt;This strikes me as &lt;a href=&#34;https://www.ft.com/content/46027dd2-8f6c-11e7-9084-d0c17942ba93&#34;&gt;a strange use of corporate cash&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thirty US companies together have more than $800bn of fixed-income investments, according to a Financial Times analysis of their most recent filings with the US Securities and Exchange Commission.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s a lot of money. You might be wondering who those thirty companies are, and it’s probably who you’d expect – tech companies with far too much cash sitting around collecting dust. Apple, Alphabet, and Microsoft are cited.&lt;/p&gt;
&lt;p&gt;This strikes me as a huge problem (&lt;a href=&#34;https://www.economist.com/news/business-and-finance/21722809-their-excuses-doing-so-dont-add-up-tech-firms-hoard-huge-cash-piles&#34;&gt;I’m not the only one&lt;/a&gt;): fixed income investing is really useful if you’re trying to meet fixed obligations, like an insurer or pension fund might, or for hedges on inflation and foreign exchange. It’s certainly not something research and development companies should be doing. They should be piling that cash into R&amp;amp;D, acquisitions, buybacks, literally anything other than fixed income investments.&lt;/p&gt;
&lt;p&gt;I’d also be more than willing to take it. You know, for the team.&lt;/p&gt;
&lt;p&gt;We’ll close on this happy note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The emergence of US companies as a leading investor in corporate debt alongside traditional asset managers comes at a time many in the market express concern about a bond market bubble that could be vulnerable to bursting should inflation and economic growth accelerate.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>The American Dream</title>
      <link>/2017/09/03/the-american-dream/</link>
      <pubDate>Sun, 03 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/03/the-american-dream/</guid>
      <description>&lt;p&gt;Edmund Phelps writing at &lt;a href=&#34;https://www.project-syndicate.org/commentary/recalling-the-american-dream-by-edmund-s--phelps-2017-08?utm_source=Project+Syndicate+Newsletter&amp;amp;utm_campaign=e8bef8a0bc-sunday_newsletter_3_9_2017&amp;amp;utm_medium=email&amp;amp;utm_term=0_73bad5b7d8-e8bef8a0bc-104329629&#34;&gt;Project Syndicate&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What made the American Dream distinctive was neither the hope of winning the lottery nor of being buoyed by national market forces or public policy. It was the hope of achieving things, with all that that entails: drawing on one’s personal knowledge, trusting one’s intuition, venturing into the unknown. It reflected the deep need of these Americans to have the experience of succeeding at something: a craftsman’s gratification at seeing his mastery result in better work, or a merchant’s satisfaction at seeing “his ship come in.” It was success that mattered, not relative success (would anyone want to be the sole achiever?). And the process may have mattered more than the success.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>The Flash Crash</title>
      <link>/2017/08/26/the-flash-crash/</link>
      <pubDate>Sat, 26 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/26/the-flash-crash/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://www.afajof.org/details/journalArticle/10546091/The-Flash-Crash-HighFrequency-Trading-in-an-Electronic-Market.html&#34;&gt;A paper&lt;/a&gt; came out in the April 2017 edition of the &lt;em&gt;Journal of Finance&lt;/em&gt; that I found to be absolutely fascinating. Normally I’d just put a link up on Twitter and move on, but leaving Facebook and Twitter have limited my ability to scream about cool papers. I thought I’d do a brief post about the paper and talk about why it’s interesting to me.&lt;/p&gt;
&lt;p&gt;The abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event—the Flash Crash—where a large automated selling program was rapidly executed in the E‐mini S&amp;amp;P 500 stock index futures market. Using audit trail transaction‐level data for the E‐mini on May 6 and the previous three days, we find that the trading pattern of the most active nondesignated intraday intermediaries (classified as High‐Frequency Traders) did not change when prices fell during the Flash Crash.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The flash crash&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; has always been an interesting subject, because it illustrates a lesson that anyone who is involved with markets knows – if you sell a ton of stuff, the prices go down. But perhaps more importantly, it demonstrates the highly interconnected nature of modern financial markets.&lt;/p&gt;
&lt;p&gt;During the flash crash, an institutional trader initiated an algorithm to sell an incredibly large amount of E-mini shares, a futures contract on the S&amp;amp;P 500. The algorithm used was a fairly simplistic strategy that aimed to trade 9% of the past minute’s volume.&lt;/p&gt;
&lt;p&gt;I have a lot of feelings on this kind of strategy. It’s not especially tactful, it has no regard to price or timing, and fails to adapt to changing conditions. If I’m aware of someone trading a tremendous amount of shares every minute (and assuming I can devise their strategy before they complete the trade), I might try to game the system by increasing the volume traded in the previous minute – either by simply churning a position or taking large long positions – and then using the foreknowledge that a large trade is about to come to take advantage. Particularly for a trade the size that the institution was trying to make (around $4.9 billion), this is a really crappy way to do it.&lt;/p&gt;
&lt;p&gt;Back to the paper. It’s well worth the read. They study audit trail level data for four days during and before the flash crash in the E-mini market, and their interesting finding is that high-frequency traders didn’t really change their trading behavior during the event. This is in contrast to a lot of the murmurings commonly bandied about in regards to the flash crash, where everyone mutters something to the effect of “HFTs got out, liquidity dried up, etc.”&lt;/p&gt;
&lt;p&gt;The authors note that market makers, not high-frequency traders, altered their inventory holding behavior in response to changing prices. Interesting.&lt;/p&gt;
&lt;p&gt;Give it a read.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You can read more about the flash crash in the &lt;a href=&#34;https://www.sec.gov/news/studies/2010/marketevents-report.pdf&#34;&gt;SEC’s report&lt;/a&gt; on the matter.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>English Pronunciation</title>
      <link>/2017/08/16/english-pronunciation/</link>
      <pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/16/english-pronunciation/</guid>
      <description>&lt;p&gt;My last name tends to throw a lot of people off, because it has far too many f&amp;rsquo;s and not enough real letters. While doing a bit of research on how to tell people how to best pronounce my name (it&amp;rsquo;s &amp;ldquo;fye-fur&amp;rdquo;, not &amp;ldquo;fiffer&amp;rdquo;), I found a &lt;a href=&#34;http://ncf.idallen.com/english.html&#34;&gt;neat little poem&lt;/a&gt; designed to showcase some of the strange ways English pronounces words. I sent it out to a couple of my friends from Norway and Greece and they politely declined to try a live reading. I can see why; I messed up about 15-20 words while reading it aloud.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an excerpt:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Compare alien with Italian,
Dandelion and battalion.
Sally with ally, yea, ye,
Eye, I, ay, aye, whey, and key.
Say aver, but ever, fever,
Neither, leisure, skein, deceiver.
Heron, granary, canary.
Crevice and device and aerie.&lt;/p&gt;

&lt;p&gt;Face, but preface, not efface.
Phlegm, phlegmatic, ass, glass, bass.
Large, but target, gin, give, verging,
Ought, out, joust and scour, scourging.
Ear, but earn and wear and tear
Do not rhyme with here but ere.
Seven is right, but so is even,
Hyphen, roughen, nephew Stephen,
Monkey, donkey, Turk and jerk,
Ask, grasp, wasp, and cork and work.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Give it a shot, and see how well you can do with the whole poem. It&amp;rsquo;s certainly fun to do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perfect Numbers</title>
      <link>/2017/08/15/perfect-numbers/</link>
      <pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/15/perfect-numbers/</guid>
      <description>


&lt;p&gt;I just started reading a book called &lt;a href=&#34;https://books.google.co.uk/books/about/How_to_Prove_it.html?id=murSjwEACAAJ&amp;amp;redir_esc=y&amp;amp;hl=en&#34;&gt;How to Prove It: A Structured Approach&lt;/a&gt;, a book all about writing and understanding proofs. My unconventional background in mathematics means that I have a fair bit of terror when it comes to proofs; they have often come up on more quantitative finance courses in a tangential manner, as the proofs are often unnessecary. The fact remains that I am terrified by them, and wanted to get over my fear of proofs.&lt;/p&gt;
&lt;p&gt;In the process, I read a bit on &lt;a href=&#34;https://en.wikipedia.org/wiki/Perfect_number&#34;&gt;perfect numbers&lt;/a&gt;. One of the proofs in the book was Euclid’s proof about infinite primes, and I figured I should write a little bit of code to find perfect numbers, for gits and shiggles.&lt;/p&gt;
&lt;p&gt;Here’s the code to compute &lt;a href=&#34;https://en.wikipedia.org/wiki/Aliquot_sum&#34;&gt;aliquot sums&lt;/a&gt; in Julia, a necessary intermediate step in perfect number evaluation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;using Primes

function aliquot(x::Int64, verbose=false)
    divisors = []
    for i in 1:x-1
        if(x%i == 0)
            append!(divisors, i)
        end
    end
    if verbose == true print(divisors) end
    return sum(divisors)
end&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a function for computing the next prime number after integer &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function next_perfect(x::Int64, limit=Inf64)
    # Given a number, find the next highest perfect number.
    found = false
    while(found==false)
        # You can set a computational limit with the &amp;#39;limit&amp;#39; argument.
        if (x &amp;gt;= limit) break end
        if aliquot(x) == x
            print(x, &amp;quot; is the next perfect number.&amp;quot;)
            found=true
            break
        end
        x += 1
    end
    return x
end&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Brief Paper on Cointegration in Crude Oil</title>
      <link>/2017/08/06/a-brief-paper-on-cointegration-in-crude-oil/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/06/a-brief-paper-on-cointegration-in-crude-oil/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve just finished putting the final touches on my final paper for my master&amp;rsquo;s degree, and I thought I&amp;rsquo;d write up a bit about the process. You can get the fancy LaTeX version &lt;a href=&#34;https://drive.google.com/file/d/0B6yUWclvz_SNREhObUxEVjFESWs/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My final summer course for the master&amp;rsquo;s program was all about Energy Finance, taught by an econometrician&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:de644491&#34;&gt;&lt;a href=&#34;#fn:de644491&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Commensurate with the professor&amp;rsquo;s experience, the final paper was along the same lines. We were allowed to choose between three different project options. Here&amp;rsquo;s the one I picked:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Are the prices of WTI and Brent co-integrated? Is it possible to profitably trade the price differential? First, summarize the results of the empirical literature. Then conduct your own analysis. Carefully explain your methodology.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This was easily the most difficult amongst the three options, because you had to design a trading strategy. For many of my colleagues who like programming less, it proved difficult to design a system to evaluate a trading system. But I thought it might be a fun challenge (and it was!) to do so.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d read the paper if you&amp;rsquo;re particularly interested in the specifics, but I thought I&amp;rsquo;d include a little bit more about the backend. I don&amp;rsquo;t have the chance to get into it in the text, but I basically wrote an entire trading system to measure transaction costs, portfolio value, etc. That was what took the most time. I spent more time debugging the transaction system than writing the paper, and I don&amp;rsquo;t actually talk about it in the paper.&lt;/p&gt;

&lt;p&gt;I also got pretty deep into &lt;a href=&#34;https://en.wikipedia.org/wiki/Cointegration&#34;&gt;cointegration&lt;/a&gt;, which is a fascinating concept. Basically, two series that are non-stationary&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:stationary&#34;&gt;&lt;a href=&#34;#fn:stationary&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; can have a &lt;em&gt;cointegrating vector&lt;/em&gt; which makes a linear combination of the series stationary. This has some pretty interesting knock-on effects:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If the combined series is stationary, you know that the series crosses the mean frequently, and that it is bound to revert. In crude oil prices, this means that the spread between two assets (like Brent and WTI crude oils) cannot remain too far from the x-intercept for too long.&lt;/li&gt;
&lt;li&gt;The variance is (theoretically&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:variance&#34;&gt;&lt;a href=&#34;#fn:variance&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;) bound to a constant state, so you can moderately achieve returns without excess variance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The bulk on my trading strategy relies on these two factors &amp;ndash; if the cointegrated series is far from zero (or close to zero) then it will &lt;em&gt;eventually&lt;/em&gt; return to the mean. I had some pretty neat returns, and actually beat the S&amp;amp;P in and out of sample.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m really proud of the paper, and hope that maybe someone might get to make use of the strategy I developed. Maybe when I&amp;rsquo;m older and actually have capital, I&amp;rsquo;ll do it, though by then the cointegrating relationship may have changed.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:de644491&#34;&gt;These dudes tend to make everything more fun.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:de644491&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:stationary&#34;&gt;This means that the series is essentially random and has no trend, has a mean of zero, and a constant variance.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:stationary&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:variance&#34;&gt;I say theoretically, because empirically, the variance is not constant. Particularly the period between 2008 and 2013 or so was especially volatile, and the period between 1994 and 2005 was extremely placid.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:variance&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Julia</title>
      <link>/2017/07/05/julia/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/05/julia/</guid>
      <description>&lt;p&gt;As I&amp;rsquo;m wrapping up my master&amp;rsquo;s degree, I have somehow managed to find a large amount of time to pursue personal interests. One of those interests is &lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt;, a technical computing language with C-comparable speed. I&amp;rsquo;m not exactly sure where I stumbled on it, but it stuck with me. Of course, the best way to learn something is to do something &lt;em&gt;cool&lt;/em&gt; with it, and &lt;a href=&#34;https://fivethirtyeight.com/features/pick-a-number-any-number/&#34;&gt;FiveThiryEight&amp;rsquo;s Riddler&lt;/a&gt; often tends to supply great cannon fodder for programming. This past week&amp;rsquo;s one was a computationally difficult one:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;From Itay Bavly, a chain-link number problem:&lt;/p&gt;

&lt;p&gt;You start with the integers from one to 100, inclusive, and you want to organize them into a chain. The only rules for building this chain are that you can only use each number once and that each number must be adjacent in the chain to one of its factors or multiples. For example, you might build the chain:&lt;/p&gt;

&lt;p&gt;4, 12, 24, 6, 60, 30, 10, 100, 25, 5, 1, 97&lt;/p&gt;

&lt;p&gt;You have no numbers left to place after 97, leaving you with a finished chain of length 12.&lt;/p&gt;

&lt;p&gt;What is the longest chain you can build?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There really doesn&amp;rsquo;t appear to be an easy answer to the problem &amp;ndash; my brother noted this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Friend of mine says that traversing a directed graph is NP-Complete, so brute-force is the way to do it. Probably&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:a14987c3&#34;&gt;&lt;a href=&#34;#fn:a14987c3&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I thought it seemed like a perfect time to try out Julia. The past two weeks or so I&amp;rsquo;ve been idly combing through Julia&amp;rsquo;s &lt;a href=&#34;https://docs.julialang.org/en/stable/&#34;&gt;fantastic documentation&lt;/a&gt;, and I&amp;rsquo;ve been really impressed by the syntax&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:110a8281&#34;&gt;&lt;a href=&#34;#fn:110a8281&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and ease at which you can handle very fast processes.&lt;/p&gt;

&lt;p&gt;What I wanted to do was basically try and brute force the problem. Here&amp;rsquo;s my pseudocode.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Pick a random number.&lt;/li&gt;
&lt;li&gt;Pick a valid number to follow it.&lt;/li&gt;
&lt;li&gt;Repeat until you can&amp;rsquo;t find a number.&lt;/li&gt;
&lt;li&gt;Do steps 1-3 with new chains, discarding the shortest chain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Mathematically, it&amp;rsquo;s very simple to define what&amp;rsquo;s a multiple and what&amp;rsquo;s a factor, here&amp;rsquo;s two functions that do that. &lt;code&gt;valid&lt;/code&gt; is a function where you pass an &lt;code&gt;x&lt;/code&gt; and a &lt;code&gt;y&lt;/code&gt; and return &lt;code&gt;true&lt;/code&gt; if &lt;code&gt;x&lt;/code&gt; can be followed by &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Test if x can be followed by y&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; valid(x, y, limit)
	&lt;span style=&#34;color:#75715e&#34;&gt;# Determine if y is a multiple of x&lt;/span&gt;
	mul &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; multiples(x, limit) &lt;span style=&#34;color:#75715e&#34;&gt;# Get multiples of x&lt;/span&gt;
	index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; findin(mul, y) &lt;span style=&#34;color:#75715e&#34;&gt;# Find if y is in the list of x&amp;#39;s multiples&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; [] &lt;span style=&#34;color:#75715e&#34;&gt;# If the index isn&amp;#39;t zero&lt;/span&gt;
		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

	&lt;span style=&#34;color:#75715e&#34;&gt;# Now determine if y is a factor of x&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Multiples generates a list of multiples and returns it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; multiples(x, limit)
	vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;limit
		&lt;span style=&#34;color:#75715e&#34;&gt;#print(i, &amp;#34;\n&amp;#34;)&lt;/span&gt;
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; x)
			append!(vals, i)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; vals &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
		print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;No multiples of &amp;#34;&lt;/span&gt;, x, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vals
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These two functions are called by &lt;code&gt;makechain&lt;/code&gt;, which picks the first number&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:f0d5da3f&#34;&gt;&lt;a href=&#34;#fn:f0d5da3f&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, and then tests if subsequent random numbers are valid. When it runs out of valid numbers, it spits out the answer.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; makechain(limit&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int64&lt;/span&gt;)
  possible &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Array&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;limit)
  first &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand(possible)
  remove &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; getindex(possible, first)
  deleteat!(possible,remove)

  chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [first]

	&lt;span style=&#34;color:#75715e&#34;&gt;# Pick a random number.&lt;/span&gt;
	&lt;span style=&#34;color:#75715e&#34;&gt;# Check if that number is valid.&lt;/span&gt;
	&lt;span style=&#34;color:#75715e&#34;&gt;# If it isn&amp;#39;t pick a new one, until they&amp;#39;re all gone.&lt;/span&gt;
	testPosition &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; possible
	shuffle!(testPosition)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; testPosition
		v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; valid(chain[&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;], i, limit)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
			append!(chain, i)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; chain
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, the final function just runs &lt;code&gt;makechain&lt;/code&gt; a bunch of times and finds the longest chain it can.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; find_longest(iterations&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int64&lt;/span&gt;, limit&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
	longest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;iterations
		chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; makechain(limit)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; length(chain) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; length(longest)
			longest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; chain
		&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; longest
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;My biggest output was something like 27 integers long after building 10 million chains, which was far below the 77 found by two other contestants. One guy apparently solved it with some nifty combinatorics software.&lt;/p&gt;

&lt;p&gt;Even though I didn&amp;rsquo;t get the right answer, I had a lot of fun working with Julia for the first time and I&amp;rsquo;m looking forward to finding neat things to do with it. Also, Julia is &lt;strong&gt;wicked fast&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:a14987c3&#34;&gt;Later, this was confirmed by &lt;a href=&#34;https://fivethirtyeight.com/features/is-this-bathroom-occupied/&#34;&gt;Oliver Roeder at the Riddler&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:a14987c3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:110a8281&#34;&gt;It kind of reads like Python with a bit of Matlab.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:110a8281&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:f0d5da3f&#34;&gt;The function&amp;rsquo;s argument, &lt;code&gt;limit&lt;/code&gt;, allows you to test chains between 1 and any integer.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:f0d5da3f&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle Titanic</title>
      <link>/2017/05/23/kaggle-titanic/</link>
      <pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/23/kaggle-titanic/</guid>
      <description>


&lt;p&gt;I thought I’d start getting into &lt;a href=&#34;www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt; to work on some non-finance data to get a feel for the messiness of real-world information. Kaggle’s introductory competition is about predicting which passengers on the Titanic are going to survive using a handful of features, so let’s launch into mucking about. This post follows a “lab book” style and is quite scattered, as I develop ideas about what to do.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Libraries
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;purrr&amp;#39; was built under R version 3.4.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
# Load data
train &amp;lt;- read_csv(&amp;quot;../../data/Titanic/train.csv&amp;quot;)
test &amp;lt;- read_csv(&amp;quot;../../data/Titanic/test.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s take a look at the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &amp;quot;Braund, Mr. Owen Harris&amp;quot; &amp;quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&amp;quot; &amp;quot;Heikkinen, Miss. Laina&amp;quot; &amp;quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&amp;quot; ...
##  $ Sex        : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &amp;quot;A/5 21171&amp;quot; &amp;quot;PC 17599&amp;quot; &amp;quot;STON/O2. 3101282&amp;quot; &amp;quot;113803&amp;quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  NA &amp;quot;C85&amp;quot; NA &amp;quot;C123&amp;quot; ...
##  $ Embarked   : chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...
##  - attr(*, &amp;quot;spec&amp;quot;)=List of 2
##   ..$ cols   :List of 12
##   .. ..$ PassengerId: list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Survived   : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Pclass     : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Name       : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Sex        : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Age        : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_double&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ SibSp      : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Parch      : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Ticket     : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Fare       : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_double&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Cabin      : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Embarked   : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   ..$ default: list()
##   .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_guess&amp;quot; &amp;quot;collector&amp;quot;
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;col_spec&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What I want to do first is add a couple of features. &lt;a href=&#34;https://www.datacamp.com/community/open-courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic#gs.UrAze5E&#34;&gt;DataCamp’s excellent tutorial&lt;/a&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; on this data set uses &lt;code&gt;Title&lt;/code&gt; and &lt;code&gt;FamilySize&lt;/code&gt;, which I’ll add now. I also thought it might be cool to separate out family names to see if certain families were likely to survive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Do it with test and train, don&amp;#39;t want to reconcile them later.
test &amp;lt;- test %&amp;gt;% 
  mutate(Surname = as.factor(word(test$Name, sep = fixed(&amp;quot;,&amp;quot;))),
         Title = word(test$Name, start = 1, sep = fixed(&amp;quot;.&amp;quot;)))

test$Title &amp;lt;- test$Title %&amp;gt;% 
  str_replace(&amp;quot;[.]&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
  word(start = -1) %&amp;gt;% 
  as.factor(.)

# Remove uncommon titles
uncommon &amp;lt;- test %&amp;gt;% 
  group_by(Title) %&amp;gt;%
  count() %&amp;gt;% 
  filter (n &amp;gt;=5) 

levels(test$Title) &amp;lt;- c(levels(test$Title), &amp;quot;Other&amp;quot;)
test$Title[!(test$Title %in% uncommon$Title)] &amp;lt;- as.factor(&amp;quot;Other&amp;quot;)
test$Title &amp;lt;- droplevels.data.frame(test)$Title

# Update embarkment location to factor
test$Embarked &amp;lt;- as.factor(test$Embarked)

# Gender to factor
test$Sex &amp;lt;- as.factor(test$Sex)

test$FamilySize &amp;lt;- test$Parch + test$SibSp + 1

# Change training dataset
train &amp;lt;- train %&amp;gt;% 
  mutate(Surname = as.factor(word(train$Name, sep = fixed(&amp;quot;,&amp;quot;))),
         Title = word(train$Name, start = 1, sep = fixed(&amp;quot;.&amp;quot;)))

train$Title &amp;lt;- train$Title %&amp;gt;% 
  str_replace(&amp;quot;[.]&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
  word(start = -1) %&amp;gt;% 
  as.factor(.)

# Remove uncommon titles
uncommon &amp;lt;- train %&amp;gt;% 
  group_by(Title) %&amp;gt;%
  count() %&amp;gt;% 
  filter (n &amp;gt;=5) 

levels(train$Title) &amp;lt;- c(levels(train$Title), &amp;quot;Other&amp;quot;)
train$Title[!(train$Title %in% uncommon$Title)] &amp;lt;- as.factor(&amp;quot;Other&amp;quot;)
train$Title &amp;lt;- droplevels.data.frame(train)$Title

# Update embarkment location to factor
train$Embarked &amp;lt;- as.factor(train$Embarked)

# Gender to factor
train$Sex &amp;lt;- as.factor(train$Sex)

train$FamilySize &amp;lt;- train$Parch + train$SibSp + 1

summary(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId       Survived          Pclass          Name          
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##      Sex           Age            SibSp           Parch       
##  female:314   Min.   : 0.42   Min.   :0.000   Min.   :0.0000  
##  male  :577   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  
##               Median :28.00   Median :0.000   Median :0.0000  
##               Mean   :29.70   Mean   :0.523   Mean   :0.3816  
##               3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  
##               Max.   :80.00   Max.   :8.000   Max.   :6.0000  
##               NA&amp;#39;s   :177                                     
##     Ticket               Fare           Cabin           Embarked  
##  Length:891         Min.   :  0.00   Length:891         C   :168  
##  Class :character   1st Qu.:  7.91   Class :character   Q   : 77  
##  Mode  :character   Median : 14.45   Mode  :character   S   :644  
##                     Mean   : 32.20                      NA&amp;#39;s:  2  
##                     3rd Qu.: 31.00                                
##                     Max.   :512.33                                
##                                                                   
##       Surname       Title       FamilySize    
##  Andersson:  9   Dr    :  7   Min.   : 1.000  
##  Sage     :  7   Master: 40   1st Qu.: 1.000  
##  Carter   :  6   Miss  :182   Median : 1.000  
##  Goodwin  :  6   Mr    :517   Mean   : 1.905  
##  Johnson  :  6   Mrs   :125   3rd Qu.: 2.000  
##  Panula   :  6   Rev   :  6   Max.   :11.000  
##  (Other)  :851   Other : 14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s a lot of Anderssons! I wonder if they’re related - let’s check family size by surname.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;% 
  filter(Surname == &amp;quot;Andersson&amp;quot;) %&amp;gt;% 
  select(Name, FamilySize, Survived, SibSp, Parch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 5
##                                                        Name FamilySize
##                                                       &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1                               Andersson, Mr. Anders Johan          7
## 2                           Andersson, Miss. Erna Alexandra          7
## 3                         Andersson, Miss. Ellis Anna Maria          7
## 4              Andersson, Mr. August Edvard (&amp;quot;Wennerstrom&amp;quot;)          1
## 5                      Andersson, Miss. Ingeborg Constanzia          7
## 6                         Andersson, Miss. Sigrid Elisabeth          7
## 7 Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)          7
## 8                        Andersson, Miss. Ebba Iris Alfrida          7
## 9                   Andersson, Master. Sigvard Harald Elias          7
## # ... with 3 more variables: Survived &amp;lt;int&amp;gt;, SibSp &amp;lt;int&amp;gt;, Parch &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are all related - except for Erna and August, and &lt;a href=&#34;https://titanicstory.wordpress.com/2012/04/04/the-entire-andersson-family-was-lost-on-the-titanic/&#34;&gt;the whole family died&lt;/a&gt;. This is a really sad data set.&lt;/p&gt;
&lt;p&gt;Na’s are the bane of any good analysis, and I want to try to remove some of them. Let’s try to pull out as many as we can.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_age &amp;lt;- function(df) {
  # Turns missing values into the average for the column.
  NA2mean &amp;lt;- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
  df[,&amp;#39;Age&amp;#39;] &amp;lt;- lapply(df[,&amp;#39;Age&amp;#39;], NA2mean)
  return(df)
}

clean_embarkment &amp;lt;- function(df) {
  # The most people embarked from &amp;#39;S&amp;#39;, so I&amp;#39;m just setting
  # the two missing values to &amp;#39;S&amp;#39;.
  df[is.na(df[,&amp;#39;Embarked&amp;#39;]), &amp;#39;Embarked&amp;#39;] &amp;lt;- &amp;#39;S&amp;#39;
  return (df)
}

test &amp;lt;- clean_age(test)
train &amp;lt;- clean_age(train)

test &amp;lt;- clean_embarkment(test)
train &amp;lt;- clean_embarkment(train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also want to scale all my features to between 0 and 1, to make processing easier. This also means scrapping the names and turning all numerical values into numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaler &amp;lt;- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

cleanse &amp;lt;- function(df) {
  # Remove character variables
  df &amp;lt;- subset(df, select = -c(Name, Ticket, Cabin, Surname))
  
  # If it&amp;#39;s a prediction set or otherwise, break it out
  if(&amp;#39;Survived&amp;#39; %in% colnames(df)){
    id &amp;lt;- select(df, Survived, PassengerId)
    df &amp;lt;- subset(df, select = -c(Survived, PassengerId))
  } else {
    id &amp;lt;- select(df, PassengerId)
    df &amp;lt;- subset(df, select = -c(PassengerId))
  }
  
  # Convert factors to numbers
  factname = c(&amp;#39;Embarked&amp;#39;, &amp;#39;Title&amp;#39;, &amp;#39;Sex&amp;#39;)
  df[,factname] &amp;lt;- lapply(df[,factname] , as.integer)
  
  # Scale variables
  df &amp;lt;- as.tibble(map(df, na.rm = TRUE, scaler))
  
  # Again, separate by labeled or not
  if(&amp;#39;Survived&amp;#39; %in% colnames(id)){
    df$PassengerId &amp;lt;- id$PassengerId
    df$Survived &amp;lt;- id$Survived
  } else {
    df$PassengerId &amp;lt;- id$PassengerId
  }
  
  return(df)
}

train_scl &amp;lt;- cleanse(train)
test_scl &amp;lt;- cleanse(test)

summary(train_scl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Pclass            Sex              Age             SibSp        
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  
##  1st Qu.:0.5000   1st Qu.:0.0000   1st Qu.:0.2712   1st Qu.:0.00000  
##  Median :1.0000   Median :1.0000   Median :0.3679   Median :0.00000  
##  Mean   :0.6543   Mean   :0.6476   Mean   :0.3679   Mean   :0.06538  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.4345   3rd Qu.:0.12500  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  
##      Parch             Fare            Embarked          Title       
##  Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0.01544   1st Qu.:0.5000   1st Qu.:0.3333  
##  Median :0.0000   Median :0.02821   Median :1.0000   Median :0.5000  
##  Mean   :0.0636   Mean   :0.06286   Mean   :0.7682   Mean   :0.4805  
##  3rd Qu.:0.0000   3rd Qu.:0.06051   3rd Qu.:1.0000   3rd Qu.:0.5000  
##  Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  
##    FamilySize       PassengerId       Survived     
##  Min.   :0.00000   Min.   :  1.0   Min.   :0.0000  
##  1st Qu.:0.00000   1st Qu.:223.5   1st Qu.:0.0000  
##  Median :0.00000   Median :446.0   Median :0.0000  
##  Mean   :0.09046   Mean   :446.0   Mean   :0.3838  
##  3rd Qu.:0.10000   3rd Qu.:668.5   3rd Qu.:1.0000  
##  Max.   :1.00000   Max.   :891.0   Max.   :1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start the analysis with a good old-fashioned logistic regression. Throw everything we’ve got into the pot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit &amp;lt;- glm(Survived ~ Pclass + Sex + Age + SibSp + 
               Parch + Fare + Embarked + Title,
             family = binomial(),
             data = train_scl,
             na.action = na.omit)
summary(logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked + Title, family = binomial(), data = train_scl, 
##     na.action = na.omit)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6460  -0.5874  -0.4168   0.6330   2.4134  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)   4.4881     0.5158   8.701  &amp;lt; 2e-16 ***
## Pclass       -2.1785     0.2789  -7.811 5.66e-15 ***
## Sex          -2.7493     0.1995 -13.783  &amp;lt; 2e-16 ***
## Age          -2.7969     0.6686  -4.183 2.87e-05 ***
## SibSp        -2.7339     0.8755  -3.123  0.00179 ** 
## Parch        -0.5260     0.7104  -0.740  0.45910    
## Fare          0.9325     1.2190   0.765  0.44427    
## Embarked     -0.4345     0.2300  -1.889  0.05891 .  
## Title        -0.8603     0.6672  -1.289  0.19724    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1186.66  on 890  degrees of freedom
## Residual deviance:  783.39  on 882  degrees of freedom
## AIC: 801.39
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basically what the above tells us is that Pretty much everything decreases your chances of living. You start at a high level (the intercept has a coefficient of 4.6) and decrease from there. Men have a sex of 1, and women have a sex of 0, so being a man is a strong predictor of dying. The strongest indicator by far is age - being older decreases your chances of living. Let’s take the testing data set and predict what we think the results are likely to be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we predict using the model
threshold &amp;lt;- 0.5
logit_pred &amp;lt;- predict(logit, newdata = test_scl, type = &amp;#39;response&amp;#39;)
hist(logit_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-12-kaggle-titanic_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit_pred &amp;lt;- ifelse(logit_pred &amp;gt; threshold, 1, 0)
# If we&amp;#39;re missing data, predict 0.
logit_pred[is.na(logit_pred)] &amp;lt;- 0
summary(logit_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.3589  1.0000  1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool. Let’s export it and see what results we get!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all &amp;lt;- data.frame(test$PassengerId, logit_pred)
colnames(all) &amp;lt;- c(&amp;quot;PassengerID&amp;quot;, &amp;quot;Survived&amp;quot;)
write_csv(all, &amp;quot;../../data/Titanic/predictions/logit_prediction.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural Networks&lt;/h1&gt;
&lt;p&gt;After submitting to Kaggle, this method gives me an accuracy of 76%, worse than the random forest method, which gave 79%. Let me see if a neural network is any better.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(neuralnet)
set.seed(91)

# Model the neural network
nnet &amp;lt;- neuralnet(Survived ~ Pclass + Sex + Age + SibSp + 
             Parch + Fare + Embarked + Title,
             hidden = c(2,2,2),
             threshold = 0.035,
             stepmax = 400000000,
             data = train_scl,
             lifesign = &amp;#39;full&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hidden: 2, 2, 2    thresh: 0.035    rep: 1/1    steps:      1000 min thresh: 0.1103394579
##                                                             2000 min thresh: 0.05215387676
##                                                             3000 min thresh: 0.04034919191
##                                                             4000 min thresh: 0.03822729544
##                                                             5000 min thresh: 0.03554090753
##                                                             5122 error: 54.00459 time: 3.79 secs&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict the test set
nnet.c &amp;lt;- compute(nnet, test_scl[,1:8])
nnet.c &amp;lt;- nnet.c$net.result
hist(nnet.c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-12-kaggle-titanic_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet.c &amp;lt;- ifelse(nnet.c &amp;gt; threshold, 1, 0)
# If we&amp;#39;re missing data, predict 0.
nnet.c[is.na(nnet.c)] &amp;lt;- 0

summary(nnet.c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1           
##  Min.   :0.0000000  
##  1st Qu.:0.0000000  
##  Median :0.0000000  
##  Mean   :0.3755981  
##  3rd Qu.:1.0000000  
##  Max.   :1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all &amp;lt;- data.frame(test$PassengerId, nnet.c)
colnames(all) &amp;lt;- c(&amp;quot;PassengerID&amp;quot;, &amp;quot;Survived&amp;quot;)
write_csv(all, &amp;quot;../../data/Titanic/predictions/nnet_prediction.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve run a lot of other computation on a variety of neural networks, with up to five layers and a variety of node amounts - I only ever matched random forest accuracy with a relatively uncomplicated neural network with three layers of two nodes, at 79%. I suspect that for this data set, predicting survival is best suited to other algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I used random forests and decision trees as my first submissions. DataCamp’s tutorial does an excellent job explaining the methodology and code, so you can check out the hyperlink above if you’re interested.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Rise of DIY Quants</title>
      <link>/2017/05/16/the-rise-of-diy-quants/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/16/the-rise-of-diy-quants/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This post first appeared in the May 2017 edition of the Reading University Investment Society newspaper - you can find a copy of the whole paper &lt;a href=&#34;https://www.dropbox.com/s/bn6kukgloae9sr9/May.pdf?dl=0&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Modern finance is a constantly evolving field. In the 1970s after Black-Scholes published their seminal paper, derivatives in their current form (for primitive derivatives are found as far back as in ancient Sumerian culture) became common and ubiquitous. Approximately ten years later, we saw the rise of the first quantitative powerhouse Renaissance Technologies, a firm that to this day makes absurd profits. Even today, the rise of passive investing is driven by mathematics as firms construct smart-beta models and strive for mean-variance portfolio optimization.&lt;/p&gt;

&lt;p&gt;But the past five years have seen another interesting development - the rise of DIY quants. Idle software engineers, physicists, students, hedge fund managers, and even yours truly have engaged in the construction of systematic and automated trading tools with free tools like Quantopian, Quandl, CloudQuant, Numerai, and many others. These platforms combine free data with programming and software development tools, and many offer tutorials on quantitative investment strategies and techniques.&lt;/p&gt;

&lt;p&gt;Take Quantopian for example. In about ten minutes from creating an account, you can have a algorithmized strategy for mean-reversion up and running. You can even hook your algorithm up to two brokers for live trading, either Interactive Brokers or Robinhood. Quantopian offers minute-level equities data, as well as futures prices to trade on, all callable by an easy-to-manage API. Participants on the platform can place their algorithms in competitions, with the prize being capital awarded from Quantopian and its full deployment in the market.&lt;/p&gt;

&lt;p&gt;You can couple your Quantopian strategy with Quandl&amp;rsquo;s vast array of core and alternative data, such as satellite imagery, oil tank storage levels, retailer email receipt data, and other custom datasets.  One could imagine a complex algorithm that locates all oil tankers currently shipping, weights the levels in reserve, and prices equities for oil and shipping as well as oil derivatives. In fact, it&amp;rsquo;s likely that some hedge fund somewhere in the world is already doing such a thing.&lt;/p&gt;

&lt;p&gt;The world is becoming more accepting of this type of behavior. In some ways, it reflects global culture&amp;rsquo;s growing aversion to high finance, as we shift away from large banks and financial institutions and move to passive investing. In the same way, DIY quants are democratizing previously unavailable services and reclaiming sovereignty over their investments.&lt;/p&gt;

&lt;p&gt;This type of investing wouldn&amp;rsquo;t have been able to occur even in the 2000s - few had the skills necessary to write the code, the computers were too slow, the digital infrastructure was lacking, and the data was prohibitively expensive. Python and other high-level languages (often condemned by institutional quantitative investors as being too slow for production) have risen in ubiquity. Even in the late 2000&amp;rsquo;s the difference in speed between Python and a faster language, like C or Fortran, was nearly insurmountable due to the slowness of the computers they ran on. Now, a budding math-geek can run a trading bot on a cloud server from a Chromebook, and utilize thousands of times more computational power than was used to send men to the moon. Lastly, the data is too expensive! In the past, someone who wanted to do what someone in their garage today could do for free would have to pay thousands of dollars to have CDs shipped to their house in order to model the data - now, all you need is a quick API call and you can have world-class data.&lt;/p&gt;

&lt;p&gt;I will note that this type of investing cannot compete with true high-frequency trading outfits. Such firms pay millions to co-locate inside broker&amp;rsquo;s facilities to reduce latency, and often have the ability of paying near-zero explicit transaction costs. DIY quantitative investing should work on minute scale or longer. I personally have a portfolio management algorithm that simple performs mean-variance optimization and rebalances once a month, so I don&amp;rsquo;t have to think about my investments.&lt;/p&gt;

&lt;p&gt;If you think you&amp;rsquo;d like to be a cog in the efficient market, go check out any of the quantitative investment sites! Try and find a signal that nobody else has found. If you do, give me a call – maybe I’ll invest.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Github Pages</title>
      <link>/2017/05/13/github-pages/</link>
      <pubDate>Sat, 13 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/13/github-pages/</guid>
      <description>&lt;p&gt;I recently moved my site away from &lt;a href=&#34;zeit.co&#34;&gt;zeit&lt;/a&gt;, and migrated it to &lt;a href=&#34;pages.github.com&#34;&gt;GitHub Pages&lt;/a&gt;. It&amp;rsquo;s much easier to maintain the site with git than with zeit&amp;rsquo;s &lt;code&gt;now&lt;/code&gt; feature, which is a little too high-powered for my tases.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve also changed the theme to a modified version of &lt;a href=&#34;https://github.com/damiencaselli/paperback&#34;&gt;paperback&lt;/a&gt;. The colors were originally all sepia tone, and I liked the current color scheme and typography.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Credit Risk &amp; Logistic Regression</title>
      <link>/2017/05/01/credit-risk-logistic-regression/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/01/credit-risk-logistic-regression/</guid>
      <description>


&lt;p&gt;I thought I’d do a little bit of analysis to showcase some credit risk analysis, using &lt;strong&gt;logistic regression&lt;/strong&gt;. I’ve pulled this sample loan data from a DataCamp course on &lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-credit-risk-modeling-in-r&#34;&gt;credit risk&lt;/a&gt;. It’s a cool class, you should check it out if you have time. In a later post, I will try this same analysis with a neural network to see if it has better predictive capabilities.&lt;/p&gt;
&lt;p&gt;Here’s a look at the data. You can see we have all kinds of valuable information we can use in determining whether someone is likely to default - people with high rates and low credit scores are more likely to default, while people who own their homes and have had long term employment grades are less likely to default. We also have a column called &lt;code&gt;loan_status&lt;/code&gt;, which is a boolean value indicating whether that particular borrower has defaulted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   loan_status loan_amnt int_rate grade emp_length home_ownership
## 1           0      5000    10.65     B         10           RENT
## 2           0      2400       NA     C         25           RENT
## 3           0     10000    13.49     C         13           RENT
## 4           0      5000       NA     A          3           RENT
## 5           0      3000       NA     E          9           RENT
## 6           0     12000    12.69     B         11            OWN
##   annual_inc age
## 1      24000  33
## 2      12252  31
## 3      49200  24
## 4      36000  39
## 5      48000  24
## 6      75000  28&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;If you’re unfamiliar with logistic regression, that’s alright. What it does (in broad strokes) is allow you to predict a value &lt;strong&gt;between 1 and 0&lt;/strong&gt;, and provide you with a degree of certainty. For example, if we ran a logistic regression on a bunch of variables, and then found relevant coefficients, we could use the features of a particular borrower to determine what level of risk they have. A lender could take appropriate measures with someone with a very low chance (0.02) of default by providing them with lower rates, or by simply not lending to someone with a very high chance of default (0.99).&lt;/p&gt;
&lt;p&gt;Now we should tidy up some of the data and get rid of any rows with &lt;code&gt;NA&lt;/code&gt;s. There are more efficient ways of dealing with this problem, but for our purposes we only lose about 3,000 observations, bringing us to about 25,000 observations. This is enough by most measures to build a rudimentary predictive model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Filter out rows with any NAs.
data &amp;lt;- data[complete.cases(data),]
head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   loan_status loan_amnt int_rate grade emp_length home_ownership
## 1           0      5000    10.65     B         10           RENT
## 3           0     10000    13.49     C         13           RENT
## 6           0     12000    12.69     B         11            OWN
## 7           1      9000    13.49     C          0           RENT
## 8           0      3000     9.91     B          3           RENT
## 9           1     10000    10.65     B          3           RENT
##   annual_inc age
## 1      24000  33
## 3      49200  24
## 6      75000  28
## 7      30000  22
## 8      15000  22
## 9     100000  28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s split our dataset into two pieces, 60/40. This allows us to design a model with the 60% dataset and test it on the 40% dataset. If I was performing more exploratory analysis, I’d split the 40 in half, one for messing around in and the other for testing, but I’m pretty much skipping right to the modeling for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9090)
bound &amp;lt;- floor((nrow(data)/4)*3)
data &amp;lt;- data[sample(nrow(data)),]
train &amp;lt;- data[1:bound,]
test &amp;lt;- data[(bound+1):nrow(data),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to the preliminary model. We can use the R’s built-in functions to handle this. See below a summary of the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- glm(loan_status ~., family = binomial(link=&amp;#39;logit&amp;#39;), data = train)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = loan_status ~ ., family = binomial(link = &amp;quot;logit&amp;quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1467  -0.5358  -0.4416  -0.3374   3.3591  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)         -3.006e+00  2.151e-01 -13.976  &amp;lt; 2e-16 ***
## loan_amnt           -2.241e-06  4.141e-06  -0.541  0.58839    
## int_rate             9.058e-02  2.301e-02   3.936 8.29e-05 ***
## gradeB               3.338e-01  1.084e-01   3.080  0.00207 ** 
## gradeC               4.932e-01  1.569e-01   3.143  0.00167 ** 
## gradeD               5.809e-01  1.995e-01   2.911  0.00360 ** 
## gradeE               5.946e-01  2.505e-01   2.374  0.01760 *  
## gradeF               8.550e-01  3.343e-01   2.558  0.01053 *  
## gradeG               1.242e+00  4.367e-01   2.844  0.00446 ** 
## emp_length           5.405e-03  3.655e-03   1.479  0.13920    
## home_ownershipOTHER  7.172e-01  3.331e-01   2.153  0.03130 *  
## home_ownershipOWN   -1.000e-01  9.608e-02  -1.041  0.29795    
## home_ownershipRENT  -1.647e-02  5.329e-02  -0.309  0.75723    
## annual_inc          -5.325e-06  7.722e-07  -6.896 5.36e-12 ***
## age                 -5.048e-03  3.911e-03  -1.291  0.19685    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13297  on 19177  degrees of freedom
## Residual deviance: 12761  on 19163  degrees of freedom
## AIC: 12791
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we test our accuracy. How well does our model predict loan status? The below code spits out the accuracy when we test our model on the &lt;code&gt;test&lt;/code&gt; dataframe, and we get a result of 89%. Not bad!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- predict(model, newdata = test, type = &amp;quot;response&amp;quot;)
fit &amp;lt;- ifelse(fit &amp;gt; 0.5, 1, 0)
error &amp;lt;- mean(fit != test$loan_status)
print(paste( &amp;quot;Accuracy is: &amp;quot;, 1 - error))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy is:  0.893477240732051&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;a href=&#34;https://datascienceplus.com/perform-logistic-regression-in-r/&#34;&gt;lovely blogpost&lt;/a&gt; recommends plotting the true positive vs. false positives. The code for that is below. The plot shows a nearly straight line, which means we really aren’t especially predictive - the output at the bottom of 0.66 similarly shows the same. We’d like this value to be closer to one to indicate predictive ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)
p &amp;lt;- predict(model, newdata = test, type = &amp;quot;response&amp;quot;)
pr &amp;lt;- prediction(p, test$loan_status)
prf &amp;lt;- performance(pr, measure = &amp;quot;tpr&amp;quot;, x.measure = &amp;quot;fpr&amp;quot;)
plot(prf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-01-credit-risk-part-1_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc &amp;lt;- performance(pr, measure=&amp;quot;auc&amp;quot;)
auc &amp;lt;- auc@y.values[[1]]
auc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6606138&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we’ve skipped a couple of important steps in modeling. The model summary shows a litany of variables that really aren’t that predictive; we need to take them out. We’re going to leave anything with a &lt;code&gt;.&lt;/code&gt; or any number of asterisks (&lt;code&gt;*&lt;/code&gt;) in, because they are significant. A 10% significant will suffice for me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model2 &amp;lt;- glm(loan_status ~ int_rate + grade + emp_length +
                (home_ownership==&amp;#39;OTHER&amp;#39;) + annual_inc + age, 
              family = binomial(link=&amp;#39;logit&amp;#39;), 
              data = train)
summary(model2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = loan_status ~ int_rate + grade + emp_length + (home_ownership == 
##     &amp;quot;OTHER&amp;quot;) + annual_inc + age, family = binomial(link = &amp;quot;logit&amp;quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1355  -0.5361  -0.4424  -0.3373   3.3724  
## 
## Coefficients:
##                                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)                   -3.030e+00  2.117e-01 -14.309  &amp;lt; 2e-16 ***
## int_rate                       8.976e-02  2.300e-02   3.903 9.51e-05 ***
## gradeB                         3.324e-01  1.082e-01   3.071  0.00213 ** 
## gradeC                         4.950e-01  1.569e-01   3.155  0.00161 ** 
## gradeD                         5.807e-01  1.994e-01   2.912  0.00359 ** 
## gradeE                         5.941e-01  2.501e-01   2.375  0.01755 *  
## gradeF                         8.522e-01  3.338e-01   2.553  0.01067 *  
## gradeG                         1.230e+00  4.358e-01   2.822  0.00478 ** 
## emp_length                     5.473e-03  3.593e-03   1.523  0.12767    
## home_ownership == &amp;quot;OTHER&amp;quot;TRUE  7.319e-01  3.316e-01   2.208  0.02728 *  
## annual_inc                    -5.386e-06  6.847e-07  -7.867 3.65e-15 ***
## age                           -5.070e-03  3.911e-03  -1.296  0.19490    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13297  on 19177  degrees of freedom
## Residual deviance: 12762  on 19166  degrees of freedom
## AIC: 12786
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to test accuracy. It basically has yielded no meaningful change in predictive ability - but then again, that’s hard to do. All we’ve done is create a more parsimonious model in line with current thinking in statistics and econometrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- predict(model2, newdata = test, type = &amp;quot;response&amp;quot;)
fit &amp;lt;- ifelse(fit &amp;gt; 0.5, 1, 0)
error &amp;lt;- mean(fit != test$loan_status)
print(paste( &amp;quot;Accuracy is: &amp;quot;, 1 - error))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy is:  0.893477240732051&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we plot the cure we showed before. Again, no real difference, but we can feel better that we have a smaller model with less “junk” floating around.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)
p &amp;lt;- predict(model2, newdata = test, type = &amp;quot;response&amp;quot;)
pr &amp;lt;- prediction(p, test$loan_status)
prf &amp;lt;- performance(pr, measure = &amp;quot;tpr&amp;quot;, x.measure = &amp;quot;fpr&amp;quot;)
plot(prf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-01-credit-risk-part-1_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc &amp;lt;- performance(pr, measure=&amp;quot;auc&amp;quot;)
auc &amp;lt;- auc@y.values[[1]]
auc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.661003&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks for taking the time to read this post. Check out later posts where I use neural networks to look at this same dataset. It’ll be fun for the whole family!&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I made a newspaper</title>
      <link>/2017/05/01/i-made-a-newspaper/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/01/i-made-a-newspaper/</guid>
      <description>&lt;p&gt;A couple of my friends here at the University of Reading came up with an excellent idea to start a financial newspaper. The concept was that the society&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; would collect articles on investing and other things finance and publish them in a monthly paper. Being one of the few native English speakers with an interest in editing and journalism, I was selected to run the paper as Chief Editor.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m very proud of the results, you can find the finished version of the April edition &lt;a href=&#34;https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0&#34;&gt;here&lt;/a&gt;. I wrote a couple articles, about the &lt;a href=&#34;/2017/04/16/2017-05-01-federal-funds-hike/&#34;&gt;federal funds hike&lt;/a&gt; and &lt;a href=&#34;/2017/04/16/2017-05-01-iex-s-crumbling-quote/&#34;&gt;IEX&lt;/a&gt;. I also wrote a lovely editors note, available in the full edition.&lt;/p&gt;

&lt;p&gt;We intend to release at least a few more editions before we all graduate, and we&amp;rsquo;ll see how they turn out.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;That&amp;rsquo;s what they call &amp;ldquo;clubs&amp;rdquo; in the UK.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/2017/04/30/introduction/</link>
      <pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/30/introduction/</guid>
      <description>&lt;p&gt;Hello! I&amp;rsquo;ve just finished building this site with Hugo and &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt;, an excellent R package built up by the folks at RStudio. The first iteration of this site is being hosted by &lt;a href=&#34;https://zeit.co/now&#34;&gt;zeit&lt;/a&gt;, a super interesting cloud company. Deployment takes about a minute, unlike the absolute nightmare that is Google Cloud. They take Dockerfiles as well, something that I haven&amp;rsquo;t had any experience with until lately.&lt;/p&gt;

&lt;p&gt;As this is the first blog post here, I just thought I&amp;rsquo;d lay out a bit of why I built this. Over the years I have accumulated a strange variety of skills, like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Market Microstructure&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCTcsdFvTCGyJ8vSp3iHuTyg&#34;&gt;Improvisational Piano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corporate Finance&lt;/li&gt;
&lt;li&gt;Econometrics&lt;/li&gt;
&lt;li&gt;Programming&lt;/li&gt;
&lt;li&gt;Lighting Design&lt;/li&gt;
&lt;li&gt;Carpentry&lt;/li&gt;
&lt;li&gt;Bookbinding&lt;/li&gt;
&lt;li&gt;Machine Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of those are finance, which is easily my favorite thing to study. At some point I&amp;rsquo;d lke to go back to graduate school and study for a PhD, but at the moment, I&amp;rsquo;m tired of living on student loans. Others are related to my previous career as a professional stagehand, lighting designer, entertainment electrician, and wrench monkey. The piano thing just sort of happened one day.&lt;/p&gt;

&lt;p&gt;My undergraduate degree is in theater arts, and my master&amp;rsquo;s degree is in corporate finance, so it has always been very difficult to demonstrate to employers that I am (i) good at what they&amp;rsquo;re hiring for and (ii) interested in the subject.&lt;/p&gt;

&lt;p&gt;To that end, I hope to produce a series of posts exploring a handful of the things I enjoy. More coming soon.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$$ PV_n=FV_n\left(1+r\right)^n $$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Federal Funds Hike</title>
      <link>/2017/04/16/federal-funds-hike/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/16/federal-funds-hike/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This article first appeared in the Reading University Investment Society&amp;rsquo;s  &lt;a href=&#34;https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0&#34;&gt;April edition&lt;/a&gt; on April 16th, 2017.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Last month, the Federal Reserve raised the short-term interest rate target range by 0.25%, to a range of 0.75% to 1%. This is in line with the central bank’s “slow and steady” approach to rate increases after a decade of historically low interest rates, and is the third such rate increase since June of 2006. The Federal Reserve has further signaled its intentions to complete at least two more quarter-point increases during 2017, with the median projection among Fed officials sitting at 1.4% by year-end. This reflects a positive outlook for the US economy, as wage and job growth continue to display robustness in the face of a volatile global market place. But how likely are rates to continue rising?&lt;/p&gt;

&lt;p&gt;The impending and steady rate increases has caused a dramatic rise in bond issuance, as investors pile in to low rate debt securities. Companies sold a record-high amount of bonds in March to the tune of $414 billion, the Wall Street Journal reported earlier this week. This signals that the market believes the Fed’s intentions to raise rates, lending further evidence to the certainty of rate increases.&lt;/p&gt;

&lt;p&gt;Perhaps a bigger indicator that rates will continue to raise steadily – and may even have an accelerated pace – is the split between US monetary and fiscal policy. As the Federal Reserve undertakes monetary tightening, the Trump administration and the Republican majority have indicated expansionary fiscal policy in preliminary budgets. This includes infrastructure spending, which Trump has stated could be as much as $1 trillion, as well as increased military and defense spending. Each of these are certain to increase money supply in the economy and drive inflation expectations, which may increase Federal Reserve rate pacing.&lt;/p&gt;

&lt;p&gt;Thus, the likelihood of continued rate hikes seems high and potentially increasing. This could signal bad news for both personal income investors who will see the market value of their holdings drop, and institutional investors with actively managed bond funds who will struggle to find profitable investment opportunities. The global impacts will be significant, as the federal funds rate is tightly linked with other interest rates. In fact, one day after the Federal Reserve raised its rates a quarter percent, the People’s Bank of China kept its rates in lock step with its American counterpart.&lt;/p&gt;

&lt;p&gt;The markets are headed towards the normalcy previous generations experienced, with 30-year interest rates above 3% and federal fund rate changes that nobody noticed. Now, when a rate hike is expected, it is talked about endlessly weeks before and after. This is good for savers and pension funds, but bond investors are going to suffer during the transition from almost-free money to rates I haven’t seen since I was 12.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEX&#39;s Crumbling Quote</title>
      <link>/2017/04/16/iexs-crumbling-quote/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/16/iexs-crumbling-quote/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This article first appeared in the Reading University Investment Society&amp;rsquo;s  &lt;a href=&#34;https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0&#34;&gt;April edition&lt;/a&gt; on April 16th, 2017.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Investor’s Exchange, also referred to as IEX, came to prominence in 2014 after Michael Lewis’ book Flash Boys was released. IEX was designed as an exchange to protect lumbering buy side firms from high-frequency traders and other nimble market actors.&lt;/p&gt;

&lt;p&gt;Their first innovation was to treat what is referred to as stale quote arbitrage. In this type of arbitrage, a fast trader could detect when the best bid or offer had changed on one exchange, and then immediately go to another exchange and pick off a midpoint quote before the midpoint updated. To solve this problem, IEX famously built a box in a warehouse with a very long coil of fiber optic cable designed to introduce a 350-microsecond delay into quote updates. Any trader who could detect a midpoint quote before it updated on another exchange could not immediately post an order on IEX, because it would suffer a delay while the exchange updated the midpoint quotes.&lt;/p&gt;

&lt;p&gt;IEX’s dedication to ensuring low transaction costs to its buy side clients is ceaseless, and it has published a new working paper about dealing with another type of arbitrage, which it calls “crumbling quote arbitrage”. The paper, written by Allison Bishop is entitled “&lt;a href=&#34;https://iextrading.com/docs/The%20Evolution%20of%20the%20Crumbling%20Quote%20Signal.pdf?utm_medium=email&amp;amp;utm_source=newsletter&amp;amp;utm_term=170411&amp;amp;utm_campaign=moneystuff&#34;&gt;The Evolution of the Crumbling Quote Signal&lt;/a&gt;” and details the exchange’s cunning way of dealing with an interesting problem.&lt;/p&gt;

&lt;p&gt;Crumbling quotes refer to when the number of exchanges on the national best bid or offer is eroding over time as the market eats up posted volumes. This can and does happen in legitimate market trading, but some predatory firms may intentionally claim all posted volumes and have in place an order to take advantage of IEX’s delay.&lt;/p&gt;

&lt;p&gt;The working paper is fascinating, as Bishop demonstrates IEX’s approach to predicting when a crumbling quote is likely to occur, and allowing orders pegged to the midpoint to exercise discretion when IEX’s “signal” is on. Their new rule was approved by the SEC for use in production, though it has yet to be fully installed exchange-wide.&lt;/p&gt;

&lt;p&gt;IEX has been at the forefront of protecting its client’s interests, and this is simply the next step at the frontier of combating high frequency traders and other technologically advanced market participants. IEX has a powerful advantage over market predators, and that is that it – along with other exchanges – has access to both a greater quantity and quality of data that traders would usually have to pay thousands of dollars a month for. It will certainly be interesting to watch the development and competition between IEX and those it tries to protect its clients from.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;me&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;me&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;
&lt;img src=&#34;/profile.jpg&#34; alt=&#34;A picture of my face.&#34; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;label for=&#34;mn-example&#34; class=&#34;margin-toggle&#34;&gt;&amp;#8853;&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;mn-example&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;It&amp;rsquo;s my jolly face!&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I play with &lt;a href=&#34;https://www.github.com/cpfiffer&#34;&gt;code&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/channel/UCTcsdFvTCGyJ8vSp3iHuTyg?view_as=subscriber&#34;&gt;pianos&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am a PhD student at the &lt;a href=&#34;https://business.uoregon.edu/phd/concentrations/finance&#34;&gt;University of Oregon&lt;/a&gt; studying finance. I have a master&amp;rsquo;s in finance from the &lt;a href=&#34;http://www.icmacentre.ac.uk/&#34;&gt;ICMA Centre&lt;/a&gt; at the &lt;a href=&#34;http://www.henley.ac.uk/&#34;&gt;Henley Business School&lt;/a&gt; (which itself is part of the &lt;a href=&#34;https://www.reading.ac.uk/&#34;&gt;University of Reading&lt;/a&gt;), and a BSc in Technical Theater from &lt;a href=&#34;https://sou.edu/&#34;&gt;Southern Oregon University&lt;/a&gt;. I&amp;rsquo;m a developer for &lt;a href=&#34;https://turing.ml&#34;&gt;Turing.jl&lt;/a&gt;, a probabilistic programming tool.&lt;/p&gt;

&lt;p&gt;My interests are pretty broad in finance, but in general I tend to favor situations where technology is involed (with some classic asset pricing stuff too):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Market microstructure&lt;/li&gt;
&lt;li&gt;Cryptocurrency&lt;/li&gt;
&lt;li&gt;AI in financial markets&lt;/li&gt;
&lt;li&gt;Dynamic asset pricing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More computer science/statistics interests:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Probabilistic programming&lt;/li&gt;
&lt;li&gt;Bayesian methods for big data&lt;/li&gt;
&lt;li&gt;Reinforcement learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Favorite programming languages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt; because it&amp;rsquo;s fast and fun to write!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rust-lang.org/en-US/&#34;&gt;Rust&lt;/a&gt; because someone told me I should learn C and I thought this was cooler.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; because I am a masochist.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.wolfram.com/mathematica/&#34;&gt;Mathematica&lt;/a&gt; because I forget how to take derivatives.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I do however flit between interests like a pixie, so this list is likely not current by the time you are reading it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>