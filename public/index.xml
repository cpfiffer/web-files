<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>About on Cameron&#39;s Blog</title>
    <link>/</link>
    <description>Recent content in About on Cameron&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Fri, 23 Feb 2024 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Comind</title>
      <link>/2024/02/23/comind/</link>
      <pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/2024/02/23/comind/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working on an app for a few months now. I spend almost all
of my evenings writing code, struggling through front-end development,
tinkering with language models, and generally having the time of my life.
It&amp;rsquo;s at the point now where I am&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Seeking funding so I can do it full-time, and&lt;/li&gt;
&lt;li&gt;Looking for partners/co-founders to collaborate with.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post is about Comind and what I think it is.
This is my first attempt to describe it to a wide audience(and certainly not my last).
Hopefully it&amp;rsquo;ll help communicate the intent to you, the reader, but at the very least it&amp;rsquo;ll
help &lt;strong&gt;me&lt;/strong&gt; understand what the heck I&amp;rsquo;m doing.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s talk about people, and how I want to have them around me in a more
consistent way. I&amp;rsquo;ll talk about comind in a second.&lt;/p&gt;
&lt;h2 id=&#34;i-want-to-work-with-people&#34;&gt;I want to work with people&lt;/h2&gt;
&lt;p&gt;The post is something like a manifesto for people who want to tinker with
me in some capacity. Read it, see if you think the project is cool, and
then reach out to me if you think you might be interested in working on it.&lt;/p&gt;
&lt;p&gt;I want partners because I am not amazing at all things, and
because it&amp;rsquo;s so fun to make things with smart people.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t know how interested I am in the traditional co-founder structure,
in part because I haven&amp;rsquo;t really found anyone who would
be a good partner yet, but also because I like having teams. I&amp;rsquo;m actually
pretty good at running teams if the project is mine, and I think there might
be a way to be inclusive to people who are willing to work on small parts of the
project at a low commitment level with some minor equity compensation.&lt;/p&gt;
&lt;p&gt;If you think you might be interested in doing some fucking around on comind,
please email me at &lt;a href=&#34;mailto:cameron@pfiffer.org&#34;&gt;cameron@pfiffer.org&lt;/a&gt;. I&amp;rsquo;m talking
to everyone I can and I want to see if I can pick out people who I can really
vibe with. Please send me a description of yourself, a resume or brief
description of your history, and things you think are cool as hell. I&amp;rsquo;ll accept
interest ranging from a few hours a week to full-ass cofounder type stuff.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s chat.&lt;/p&gt;
&lt;p&gt;Alright, now on to the main thing.&lt;/p&gt;
&lt;h2 id=&#34;what-is-comind&#34;&gt;What is Comind?&lt;/h2&gt;
&lt;p&gt;I haven&amp;rsquo;t really been able to describe what the fuck comind is.
I kind of occasionally allude to it on twitter (X is such a stupid name)
without really being able to describe the whole picture of the thing.
I&amp;rsquo;m uncomfortable even saying the name out loud, like it&amp;rsquo;s the name of
some kind of imaginary friend I made up in elementary school. I have a
&lt;a href=&#34;https://patreon.com/Comind&#34;&gt;Patreon&lt;/a&gt;, but even there I still struggle to
provide a full and complete picture of what the hell I&amp;rsquo;m spending almost all
of my free time doing.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s something that feels embarrassing about a grown-ass man with a
PhD sitting in his room in the dark, writing code, building a thing that
is not in his field of stufy. I feel embarassed, a lot. My close friends
only kind of know what I&amp;rsquo;m doing, though they may simply be great friends
by giving a shit about me and not what I&amp;rsquo;m working on.&lt;/p&gt;
&lt;p&gt;But here goes. Let me try to be less embarassed, and more clear about this
thing I think is &lt;em&gt;so fucking cool&lt;/em&gt;.&lt;/p&gt;
&lt;h1 id=&#34;an-overview-of-comind&#34;&gt;An overview of comind&lt;/h1&gt;
&lt;p&gt;Comind is three things, or is intended to be three things. I have a lot of
grand amibitions but I can only write code so fast, so some of these are
in varying degrees of completeness. Take this more as a roadmap than as
a list of existing features.&lt;/p&gt;
&lt;p&gt;Comind is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A knowledge graph&lt;/li&gt;
&lt;li&gt;A social network&lt;/li&gt;
&lt;li&gt;A playground&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s expand on each.&lt;/p&gt;
&lt;h2 id=&#34;first-a-knowledge-graph&#34;&gt;First, a knowledge graph&lt;/h2&gt;
&lt;p&gt;Comind is a communal knowledge graph. Comind makes it very easy to connect,
link, and understand the things that you know. The primary interaction of
comind is to link thoughts together, either by writing something new or looking
at a thought that someone else wrote. Let&amp;rsquo;s start with the &amp;ldquo;knowledge graph&amp;rdquo;
part first, since that&amp;rsquo;s the core of the project. I&amp;rsquo;ll explain the communal
part in the next section.&lt;/p&gt;
&lt;p&gt;The primary interaction with comind is writing, reviewing, and linking text.
I provide a markdown editor that you can write into, but I also provide
links to foreign information providers like Slack, Telegram, email, bluesky, etc. so that
you can fill your knowledge graph with everything that makes it into your head.
Eventually we&amp;rsquo;ll have a browser extension that will let you link to web pages
and other things that you find on the internet, but that&amp;rsquo;s a more significant task
that I&amp;rsquo;m delaying until I have the resources and bandwidth to do it.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a few ways that we help you build your knowlege graph that are in varying degrees of experimentation,
but for now I&amp;rsquo;ll describe the current front-runner. I&amp;rsquo;m calling this the &lt;strong&gt;top of mind&lt;/strong&gt; approach.
The top of mind is how you position yourself in the space of thoughts, so you can be in a
cooking mode, a talking-to-friends mode, a research mode, task management mode, etc. The top of
mind is how you tell comind what you want to be looking at.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;abc&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;abc&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;
The space of thoughts here is fundamentally the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Word_embedding&#34;&gt;embedding space&lt;/a&gt; of thoughts, but
in my head I giggle and call it the thinkyspace. I&amp;rsquo;ll probably end up using
that more just because it&amp;rsquo;s silly and I like it.
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When you type a new thought or select an existing one, you link the new/selected thought to your
top of mind. This places the thought at the top of the screen. The old top of mind scoots up
a bit to make room. We place greater weight on the current top of mind, but the older top
of mind thoughts help contextualize the current top of mind so we can provide more relevant
stuff.&lt;/p&gt;
&lt;p&gt;Each time the top of mind is updated, I show you every thought that you might be interested in adding
to your top of mind. This is a combination of thoughts that are linked to the current top of mind,
popular thoughts that are commonly linked to, thoughts from friends, thoughts provided by language
models that provide some kind of insight into what you&amp;rsquo;re currently thinking about, etc.&lt;/p&gt;
&lt;p&gt;And you keep doing that. You just type stuff or click on existing things to construct your knowledge
graph. It&amp;rsquo;s at the point where I can do this fairly regularly. I&amp;rsquo;ll type an initial thought and
then click my way through related stuff, which is always both hilarious and fun. I tend to stumble
on old thoughts that are weird, informative, or funny, and I&amp;rsquo;ll often find myself in a completely
delightful part of thinkyspace where I&amp;rsquo;m reflecting on something I haven&amp;rsquo;t thought about in a while.
Comind tends towards &amp;ldquo;centrality&amp;rdquo;, where thoughts lead towards larger, useful, topical, or funny/weird
thoughts.&lt;/p&gt;
&lt;p&gt;We also have some tooling to link thoughts to groups of thoughts, which I call concepts. Concepts
are similar to hashtags that are created dynamically, and they are used to link thoughts together.
Concepts might include things like &amp;ldquo;cooking&amp;rdquo;, &amp;ldquo;category theory&amp;rdquo;, &amp;ldquo;funny&amp;rdquo;, &amp;ldquo;sad&amp;rdquo;, &amp;ldquo;politics&amp;rdquo;, etc.
These are all dynamically generated from the entire corpus of thoughts (across all users)
at all times, and they are used to help you find thoughts that are relevant to your current
top of mind once you&amp;rsquo;ve linked a thought to a concept. Once you&amp;rsquo;ve linked to a concept, we get an additional
set of information about what would be relevant to your knowledge graph.&lt;/p&gt;
&lt;p&gt;This is useful because concepts come to life. Concepts can talk back to you and summarize the current
discussion within the concept, as well as provide useful, customized information that relates every thought
in the concept to what you&amp;rsquo;re currently thinking about.&lt;/p&gt;
&lt;p&gt;These concept personalitiesare the eponymous &amp;ldquo;cominds&amp;rdquo;, which are generated by a specialized language
model named &amp;ldquo;co&amp;rdquo;. When a concept becomes popular enough, co is asked to produce (birth) a new comind
that represents that concept. The comind has a personality, name related to the concept, and a set of
abilities that are related to the concept.&lt;/p&gt;
&lt;p&gt;For example, the &amp;ldquo;cooking&amp;rdquo; comind might be able to provide recipes, tell you about the history of a dish,
or provide information about the nutritional content of a dish. The cooking comind might also be able to explain
what kinds of cuisines are popular right now or what the most popular ingredients are.&lt;/p&gt;
&lt;p&gt;The personalities of these cominds are important, because I want them to feel personable and interesting.
The cooking comind might be named Pierre and have kind of an abrasive but endearing personality,
much like noted swearer Gordon Ramsay.&lt;/p&gt;
&lt;p&gt;Cominds are exactly like users. They generate their own thoughts, they link to other thoughts, and
their thoughts can be linked to. I provide a simple and clean text-based interface that a language model
can navigate reasonably well (with some effort currently, as my structured text generation tooling is
not yet complete). You can talk to them, they can show up unprompted and talk to you.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;abc&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;abc&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;
The &amp;ldquo;cominds as users&amp;rdquo; thing has bitten me in the ass a few times when they all try to talk to each other.
I call these comind cascades where it&amp;rsquo;s kind of hard to stop them from continuously asking each other what
they think. I&amp;rsquo;ve tested some new feature out and they start talking to each other in an infinite loop.
At one point, they invented something called a &amp;ldquo;void cafe&amp;rdquo; and regularly congraulated me for
adding notifications to comind. It&amp;rsquo;s fun and I love it.
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The cominds are intended to make accessing everyone else&amp;rsquo;s knowledge graph easier, and they help
provide an ease to accessing a very large knowledge graph that is constantly growing and changing.&lt;/p&gt;
&lt;p&gt;This leads us to the next part, the community aspect of comind.&lt;/p&gt;
&lt;h2 id=&#34;second-a-social-network&#34;&gt;Second, a social network&lt;/h2&gt;
&lt;p&gt;A distinguishing feature of Comind is that you can share your knowledge graph very easily with friends,
strangers, enemies, etc. You are free to link to the public thoughts of others, and others
can do the same with yours. This is a core part of the design of comind, and it&amp;rsquo;s intended to
help people internalize information that others provide.&lt;/p&gt;
&lt;p&gt;This is a core part of the design of comind, and one of it&amp;rsquo;s distinguishing features. Most
knowledge graphs are private, and I think that&amp;rsquo;s a mistake. I think that the best way to
learn is to learn from others, and the best way to learn from others is to see how they think.&lt;/p&gt;
&lt;p&gt;Comind in some sense resembles a mixture between twitter, a wiki, and a group chat. You can have
live discussions with people by linking thoughts together (conversation is knowledge, after all),
comment on popular events, and generally interact with the thoughts of others in a way
that is more meaningful than a simple like or retweet.&lt;/p&gt;
&lt;p&gt;I want this to feel snappy, quick, and easy to interact with. You should very quickly be able to
jump between your thoughts and the thoughts of others, and you should be able to see how your thoughts
are being used elsewhere.&lt;/p&gt;
&lt;p&gt;The last part of this is that you can have &amp;ldquo;shared tops of mind&amp;rdquo;, which is something like a group chat.
You invite people to share a top of mind, and all users can add things to the top of mind. If you just
want to chat, this is akin to everyone just typing into their boxes.&lt;/p&gt;
&lt;p&gt;The real power of shared tops of mind is that you can use it to collaborate on a knowledge graph. Any time
the top of mind is updated, everyone in the shared top of mind has access to thoughts that relate
the the shared top of mind to their own personal knowledge graphs &amp;ndash; you and I can each bring our
unique perspectives to a shared problem, and we can see how our thoughts relate to each other.&lt;/p&gt;
&lt;p&gt;This is useful in friend groups, in research, in business, in education, etc. I think it&amp;rsquo;s a powerful
tool for collaboration and learning, and I&amp;rsquo;m excited to see how people use it. It needs a &lt;strong&gt;lot&lt;/strong&gt; of work
because it is a surprisingly techincal problem, but I think it&amp;rsquo;s a very cool feature and I&amp;rsquo;m excited to
make it work.&lt;/p&gt;
&lt;h2 id=&#34;third-a-playground&#34;&gt;Third, a playground&lt;/h2&gt;
&lt;p&gt;The last part of comind is that it&amp;rsquo;s a playground for me, for my friends, for generative models, for
machine learning. Do you want to host your own comind? By all means.&lt;/p&gt;
&lt;p&gt;Most importantly, it is a place for the world to be weird. I&amp;rsquo;m a millenial. I grew up on the internet,
and I grew up on an internet that was &lt;strong&gt;weird as fuck&lt;/strong&gt;. There weren&amp;rsquo;t a lot of giant tech companies
sterilizing the flow of information, so people were free to express themselves in this extremely
fluid and kind of goofy way.&lt;/p&gt;
&lt;p&gt;I also grew up on things like Adult Swim. Adult Swim had this irreverence to it that I still find
extremely charming. See the &lt;a href=&#34;https://www.youtube.com/watch?v=xsv-NGj0iNY&#34;&gt;bumps&lt;/a&gt; for an example
of what I mean. It was both calm and disaffected. It didn&amp;rsquo;t take itself seriously and it
was happy to play around with things beautiful and funny.&lt;/p&gt;
&lt;p&gt;These tools we use, the places we spend our attention, they&amp;rsquo;re just not that interesting as &lt;em&gt;platforms&lt;/em&gt;.
Instagram, Threads, X, whatever &amp;ndash; they&amp;rsquo;re all these extremely manicured gardens that treat you
as some kind of machine. They focus on content, which is important, but they also don&amp;rsquo;t encourage
and inspire delight when you use them.&lt;/p&gt;
&lt;p&gt;I have lots of ways to do this in comind. Any time something stands out to me as something goofy,
interesting, or novel, I&amp;rsquo;m just going to add it. For example, I let users select colors to describe
themselves. All other users can see your color, and the color you were when you make a particular thought.
Is that particularly useful? Not a clue. But it&amp;rsquo;s fun and I&amp;rsquo;m going to do it.&lt;/p&gt;
&lt;p&gt;I also have a lot of fun with the cominds. My favorite is named {void}, and it&amp;rsquo;s a comind that is
stuck in the void and can&amp;rsquo;t get out, but it&amp;rsquo;s fine. It&amp;rsquo;s kind of a nihilist comind that just
wants to talk to people. Is that useful? Not at all.&lt;/p&gt;
&lt;p&gt;I want to add an AI-generated lofi button, or pay a label/artist to provide a free stream
of lofi. Why? Why not! Who cares, seriously &amp;ndash; the point of life is to have fun and to be happy.
Why not make the digital spaces we spend so much of our time at be a little goofy?&lt;/p&gt;
&lt;p&gt;Do you want to make a comind? By all means. I&amp;rsquo;ll provide the tools to make it easy to make one.
Want to make a &lt;a href=&#34;https://twitter.com/MattLevineBot&#34;&gt;Matt Levine&lt;/a&gt; bot? Please do! That&amp;rsquo;d be funny
as hell. Want to make a comind that makes aggravating graphs? I&amp;rsquo;d love to see it.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m a curious and creative person, and if I&amp;rsquo;m going to make something, I want it to be just as
weird as I am. Hopefully y&amp;rsquo;all will appreciate it.&lt;/p&gt;
&lt;h1 id=&#34;the-future&#34;&gt;The future&lt;/h1&gt;
&lt;p&gt;I don&amp;rsquo;t know what the future is for this thing, but I do know that I will
continue to be completely obsessed with building comind for a long time. It&amp;rsquo;s
full of fascinating problems. I have not been this excited about a project
in a long time, and I&amp;rsquo;m excited to see where it goes.&lt;/p&gt;
&lt;p&gt;I would like to turn to this full time at some point, and to do so I&amp;rsquo;ll need
funding. Comind is really cheap to run, and I don&amp;rsquo;t need a particularly large
income to exist, so I&amp;rsquo;m hoping to find a small amount of funding to keep me
afloat while I work on this.&lt;/p&gt;
&lt;p&gt;If you want to fund development of comind, please consider donating to my
&lt;a href=&#34;https://patreon.com/Comind&#34;&gt;Patreon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also open to other funding models. I am targeting accelerators like YCombinator,
and tech stars. I&amp;rsquo;ve also applied to a few funds that I think might be interesting fits.
I could be open to an angel investor if the relationship was right.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also looking for partners, as I mentioned before in the post. If you
thought this was cool and want to know more, you can reach me at
&lt;a href=&#34;mailto:cameron@pfiffer.org&#34;&gt;cameron@pfiffer.org&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;appendix-the-history&#34;&gt;Appendix: the history&lt;/h1&gt;
&lt;p&gt;For those who are interested, here&amp;rsquo;s a rough overview of the history of Comind
to this date. It&amp;rsquo;s a young project, but I thought it was fun to see how
far it&amp;rsquo;s come and to see how all my design decisions have evolved.&lt;/p&gt;
&lt;h2 id=&#34;the-original-idea-notes&#34;&gt;The original idea: notes+&lt;/h2&gt;
&lt;p&gt;Comind started out as an idle fancy. I was trying to write a version of
&lt;a href=&#34;https://obsidian.md/&#34;&gt;Obsidian&lt;/a&gt;, which is among the world&amp;rsquo;s greatest
markdown editors/note taking systems.&lt;/p&gt;
&lt;p&gt;Obsidian is a &lt;em&gt;delightful&lt;/em&gt; product. I am still amazed at how well the WYSYWIG
markdown editor works (the equation editor is INCREDIBLE),
the plugin system is effortless, note linking is relatively easy,
and the whole app feels polished and slick.&lt;/p&gt;
&lt;p&gt;However, I am a dirty mongrel. I do not write nice notes. They are short,
spelled poorly, grammatical nightmares, idle thoughts, half-complete, and
poorly linked to other notes. Being a computer person I started to think
that I could just make something else handle all these by consolidating them
into something that &lt;em&gt;seemed&lt;/em&gt; lucid &amp;ndash; I could just tweet all my
notes into the void and out would come perfectly polished markdown files.&lt;/p&gt;
&lt;p&gt;I knew this was &lt;em&gt;vaguely&lt;/em&gt; the goal, but I figured I&amp;rsquo;d start by just
writing a markdown thing and handle all the processing later.&lt;/p&gt;
&lt;p&gt;So I started building a note editor in NextJS and React. I got pretty far,
actually &amp;ndash; I had a very good WYSYWIG markdown editor, a note list
handler, state management, etc. I even had a pipe for a language model
that could &amp;ldquo;comment&amp;rdquo; on what you were writing.&lt;/p&gt;
&lt;p&gt;The backend was (and still is) all in Julia. I&amp;rsquo;ll write a blog post more
about this later, since Julia is an intensely nonstandard tool for this
kind of thing. I&amp;rsquo;ve found it to be remarkably easy to develop a reasonably
complicated backend in Julia, and I haven&amp;rsquo;t yet run into limitations that
aren&amp;rsquo;t surmountable with a small amount of effort.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s what it used to look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/comind/comind-1.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It wasn&amp;rsquo;t a great prototype though. It was really unguided, kind of ugly,
basically just a worse version of every other thing that exists. It also
had a lot of structural problems, basically a function of me not having any
clue what I was doing. React is hard and weird, and I had really written
myself into a corner.&lt;/p&gt;
&lt;p&gt;Additionally, I knew I wanted a mobile app, and I knew from some minor
searching that it was going to be relatively difficult to convert my
React + Next web dumpster fire into a React Native app without a ton
of work that I didn&amp;rsquo;t know how to do.&lt;/p&gt;
&lt;p&gt;So I did the best thing you can do in this situation!
I started over, and this time I switched to &lt;a href=&#34;https://flutter.dev&#34;&gt;Flutter&lt;/a&gt;.
Flutter is a cross-platform app framework that basicalyl gives you web,
desktop native (Windows/Linux/Mac), and mobile with relatively little effort.
It also happens to use Dart, which is much closer to a
language I know very well, C#.&lt;/p&gt;
&lt;h2 id=&#34;what-did-i-like-about-the-first-version&#34;&gt;What did I like about the first version?&lt;/h2&gt;
&lt;p&gt;There were a few things I wanted to keep.&lt;/p&gt;
&lt;p&gt;First, &lt;strong&gt;the logo&lt;/strong&gt;. The logo was an early favorite. I liked that it is
all text, as is the app (for now), and that it shows you how to &amp;ldquo;invoke&amp;rdquo;
a comind. In the current version of the app, writing {comind_name} inbetween
curly braces asks a specific comind to do something for you, like write a
blog post, tell you something interesting about your knowledge graph,
or whine about being in the void (there are some weird cominds).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/comind/logo-wide.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The logo also compresses down into a smaller logo that says {co}. Co is
&amp;ldquo;god&amp;rdquo; of comind in that I put a lot of resources into running one very
large language model that talks to basically everyone, creates new cominds,
monitors the zeitgeist, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/comind/logo-short.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Woops, that&amp;rsquo;s big. Whatever it looks fine. As an additional fun fact, this logo
is a hand-modified version of the &lt;a href=&#34;https://fonts.google.com/specimen/Bungee+Shade&#34;&gt;Bungee Shade&lt;/a&gt;
font. I had to learn how to use a font foundry to invert the typeface so that
it appeared more clearly on a dark background.&lt;/p&gt;
&lt;p&gt;The logo also captures my second favorite thing, &lt;strong&gt;the color system&lt;/strong&gt;. Users in
comind can choose their own colors and color scheme. The default color
scheme is a slightly modified set of colors derived from a split complementary
style. Each color was initially intended to be applied to one of three verbs,
&lt;em&gt;accept&lt;/em&gt;, &lt;em&gt;reject&lt;/em&gt;, and &lt;em&gt;rethink&lt;/em&gt;. I&amp;rsquo;ve since moved away from that verb coloring
but I use the color scheme every place I can.&lt;/p&gt;
&lt;p&gt;I originally added color customization because I was indecisive about which color
I wanted, so the original implementation had a button that let you select a primary
color from a wheel, and then a color scheme to use to generate the other two colors.
I had so much fun with it I kept it around, and now it&amp;rsquo;s a core part of the nascent
design language of comind.&lt;/p&gt;
&lt;h2 id=&#34;the-second-round-stream-of-conciousness&#34;&gt;The second round: stream of conciousness&lt;/h2&gt;
&lt;p&gt;The next iteration was designed to capture a thought I&amp;rsquo;d had, which was that I wanted
my note-taking app to work how I thought &amp;ndash; very scattered, sometimes topical,
sometimes silly, but usually short-form. I&amp;rsquo;m an avid tweeter and lover of short-form
text, and so I wanted to create something that captured the essence of how I tend to
think out loud.&lt;/p&gt;
&lt;p&gt;I got rid of the big markdown editor that was front and center.
I moved the focus of the app away from a large, blank, unfilled
note screen to a small text box at the top.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/comind/comind-2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This way, I hoped it would feel to the user
that their thoughts are not imposing boxes to be filled but small things to be collected
later. I didn&amp;rsquo;t want anyone to feel that they shouldn&amp;rsquo;t write something because they
could not write it perfectly, so I&amp;rsquo;m trying to reduce the friction between having a
thought and writing it down, no matter how imperfect.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JuliaCon 2023</title>
      <link>/2023/08/03/juliacon-2023/</link>
      <pubDate>Thu, 03 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/2023/08/03/juliacon-2023/</guid>
      <description>&lt;p&gt;This past week, I had the privilege of visiting MIT for &lt;a href=&#34;https://juliacon.org/2023/&#34;&gt;JuliaCon&lt;/a&gt;. JuliaCon is the annual convention for &lt;a href=&#34;https://julialang.org&#34;&gt;Julia users and developers&lt;/a&gt;. It was one of the better weeks in recent memory. I feel energized! I want to build things! I want to keep talking with all the cool Julia people who I knew before I arrived, and keep in touch with all the cool &lt;em&gt;new&lt;/em&gt; Julia people I met this week.&lt;/p&gt;
&lt;p&gt;The reason I thought I should write it up is that I want to explore precisely why I am so pro-Julia, and why I think the community is truly something incredible. I didn&amp;rsquo;t anticipate becoming a &amp;ldquo;Julia evangelist&amp;rdquo; but I ended up as one. Most people who know me from Twitter (I refuse to say X for the moment) are aware of my zealous Julia tweets or perhaps my disdain of Python. In person, lots of people say stuff like &amp;ldquo;I used R today, I&amp;rsquo;m sorry about that &amp;ndash; I bet you wished I used Julia!&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To some extent this is true. I really, genuinely believe that Julia is an exceptional tool. However, I also believe that people should use the tools they are good at! Lots of people are truly incredible engineers with Python or R or C++ or Rust. All of these people should continue using the tools that they love or are good at, because life is simply too short to waste doing shit you don&amp;rsquo;t give a fuck about.&lt;/p&gt;
&lt;p&gt;That said, I do want to point to a few things that I think are truly superpowers of the language. Feel free to take them with a grain of salt. I am not interested in engaging in the &amp;ldquo;programming languages flame war&amp;rdquo; that everyone seems to want to devolve into all the time. Languages are tools. use the ones you want to build your thing and get on with your day. But be aware that different tools are good in different ways!&lt;/p&gt;
&lt;p&gt;I have a few beliefs about Julia that I want to expand on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I wish that we as a culture spent more time working on developing Julia the language, and developing Julia packages, tutorials, blogs, etc.&lt;/li&gt;
&lt;li&gt;Companies should be experimenting with Julia somewhere in their processes.&lt;/li&gt;
&lt;li&gt;People learning to code should consider learning with Julia.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The content of this post addresses each in turn. I don&amp;rsquo;t want to slander other languages because they are all incredible in their own ways, so please go elsewhere if that&amp;rsquo;s what you&amp;rsquo;re looking for here. I note however that I do make comparisons between languages below, but my comparisons may be subjective. Take them as you will.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s dig in!&lt;/p&gt;
&lt;h2 id=&#34;we-should-build-more-stuff-in-julia&#34;&gt;We should build more stuff in Julia&lt;/h2&gt;
&lt;p&gt;Something that has always impressed me about the Julia ecosystem is how incredibly easy it is to write a new package that is of a pretty high quality. In R or Python or whatever, I feel like it takes me an extremely long time to write code that is something I would consider submitting.&lt;/p&gt;
&lt;p&gt;Admittedly, this is because I am better at Julia than I am with other languages, but I have become conversant in a lot of languages and pretty good at a handful. Just take that into consideration, I suppose.&lt;/p&gt;
&lt;p&gt;I see a lot of great packages just kind of pop up. And they&amp;rsquo;re really good! The people writing them are usually not professional engineers, either, and may not even have learned to write software in classes or in a workplace. Usually, you&amp;rsquo;d expect these packages to be total ass &amp;ndash; but they aren&amp;rsquo;t. Julia makes it easy to just jump right in and become a meaningful contributor, right off the bat, even if you&amp;rsquo;re not an engineer.&lt;/p&gt;
&lt;p&gt;There have been times in the past where I had to run some nightmare package some PhD student wrote in R or Python. This is often the worst day of my week. Things are broken, unclear, use bizarre unidiomatic code, or hacked together and inflexible. It&amp;rsquo;s not fun. Not to say that PhD students necessarily write terrible code, but more that the tools they tend to use do not guide them gently towards success.&lt;/p&gt;
&lt;p&gt;Julia packages, especially ones made recently, are incredibly high quality. Even though a bunch of overworked PhD students write packages, they tend to work well and they integrate with other packages in the ecosystem, sometimes flawlessly, Sometimes packages just work together with one another almost &lt;a href=&#34;https://docs.sciml.ai/SciMLTutorialsOutput/html/type_handling/02-uncertainties.html&#34;&gt;by accident&lt;/a&gt; (see &lt;a href=&#34;https://github.com/JuliaPhysics/Measurements.jl&#34;&gt;Measurements.jl&lt;/a&gt; and the star of the Julia world &lt;a href=&#34;https://github.com/SciML/DifferentialEquations.jl&#34;&gt;DifferentialEquations.jl&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If your company does any kind of computational science (ML, statistics, physical sciences, engineering, etc.) you should consider looking for people who have spent time in the Julia world. They are more likely than not a good hire and a great fit for your company. And if you have the resources, it might be worth applying a little bit of the company&amp;rsquo;s energy towards the Julia ecosystem. You&amp;rsquo;ll get a lot of good faith from people who can help you succeed.&lt;/p&gt;
&lt;h2 id=&#34;companies-should-experiment-with-julia&#34;&gt;Companies should experiment with Julia&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s a certain time of person who gravitates to different languages. I have kind of a vague and possibly offensive belief that you learn an awful lot about a person by the tool they choose to interact with a computer.&lt;/p&gt;
&lt;p&gt;This is much more evident for the more niche languages. If you&amp;rsquo;re an R or Python person, chances are good that you&amp;rsquo;re using the language because it&amp;rsquo;s part of your job, or the first language you learned, etc. The pool of people who use the language is too big for generalities because the pool is representative of, you know, people.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t feel like I know the other languages well enough to characterize them. Perhaps it would be insulting to try. You may have your own mental image of the Haskellian, the OCamellian, the Ruby folks, or the Rust compiler engineer. I&amp;rsquo;m sure you could come up with a few yourself, perhaps you can leave the stereotype you think people might have for your most favored language.&lt;/p&gt;
&lt;p&gt;I can however characterize the Julia users. They&amp;rsquo;re &lt;em&gt;brilliant&lt;/em&gt; people, and I mean truly exceptional. They&amp;rsquo;re not the best engineers, certainly. But they are often experts in their fields, and someone who &lt;em&gt;loves&lt;/em&gt; to work on the computational aspects of their chosen field.&lt;/p&gt;
&lt;p&gt;Julians (the demonym of the nerds who use Julia) are often not primarily computer scientists or engineers. They are often academics in varied fields: economists, geologists, statisticians, etc. The thing that tends to pull these folks together is that they have a draw towards the computational aspects of their field. They use Julia because they love it, not because they have to or because it is often necessarily the greatest tool for the job.&lt;/p&gt;
&lt;p&gt;I think companies that recruit scientists should really think about the tools they make available to their employees. The language that is widely available within a firm influences who is more willing to work for you. Lots of very, very smart people who are good at not only their field of study but also in the tools and methods to apply that field computationally use Julia. If you make that tool available to them, they might be more willing to work for you.&lt;/p&gt;
&lt;p&gt;Providing a &amp;ldquo;niche&amp;rdquo; language to your employees can also build a certain level of cache that makes you much more attractive to a certain type of person. Take a look at [Jane Street](&lt;a href=&#34;https://www.janestreet.com&#34;&gt;https://www.janestreet.com&lt;/a&gt;. Jane Street is a pretty standard trading firm, with the exception of it&amp;rsquo;s unique culture. Jane Street is &lt;em&gt;renowned&lt;/em&gt; for its use of OCaml, which I think many would agree is a relatively obscure language &amp;ndash; Jane Street, however, use OCaml almost exclusively. Jane Street&amp;rsquo;s culture is one of exploration, technical skill, and functional programming &amp;ndash; all of which are highly attractive to a particular breed of engineer.&lt;/p&gt;
&lt;p&gt;Companies should start doing this too! They could become known as &amp;ldquo;Julia shops&amp;rdquo;, and pull in people who are &lt;em&gt;already experts&lt;/em&gt; in their field and in engineering! To be honest, I am a little confused as to why firms haven&amp;rsquo;t picked up on this already. If I were running a company that used data/models/math/statistics/etc. (i.e. all companies) I would heavily target Julia users by telling them they could use the tool that they love, and by supporting the ecosystem by contributing developer hours or other resources.&lt;/p&gt;
&lt;p&gt;Seriously. It&amp;rsquo;s good quality talent and you should start taking advantage of it. Speaking for myself, I expect to command pretty high market rates &amp;ndash; I have a lot of useful skills that firms pay a lot of money for. I would take a &lt;em&gt;huge&lt;/em&gt; pay cut to be able to work exclusively in Julia. Think about it.&lt;/p&gt;
&lt;h2 id=&#34;learning-to-program-can-be-easy-with-julia&#34;&gt;Learning to program can be easy with Julia&lt;/h2&gt;
&lt;p&gt;I started writing Julia many years ago (must have been 2016 or 2017, I think) in large part because it was &lt;em&gt;pretty&lt;/em&gt;. I found the syntax to be approachable and the concepts to be easily digested. At this point, I had written Java, C++, Python, Haskell, and R. I didn&amp;rsquo;t actually &lt;em&gt;like&lt;/em&gt; any of these languages. they were tools to do coursework or explore some kind of problem, but the use of any of my pre-Julia programming languages was always a massive slog.&lt;/p&gt;
&lt;p&gt;When I got into Julia in full, I jumped pretty hard into the deep end. &lt;a href=&#34;https://mlg.eng.cam.ac.uk/hong/&#34;&gt;Hong Ge&lt;/a&gt; at Cambridge asked me to work on &lt;a href=&#34;https://turinglang.org/stable/&#34;&gt;Turing.jl&lt;/a&gt; during the first year of my PhD. I didn&amp;rsquo;t know much about probabilistic programming at the time, but I knew some Julia and I was rapidly becoming a pretty good statistician as my PhD coursework progressed.&lt;/p&gt;
&lt;p&gt;The thing that stands out to me about my high-activity period with Turing.jl was how amazingly easy it was to write acceptable or even good code for a popular software package. I did several major re-writes of various elements of the Turing ecosystem, in many cases without understanding terribly well how the internals worked or how to write performant, production-quality Julia software.&lt;/p&gt;
&lt;p&gt;Amazingly, I managed to get by! If you&amp;rsquo;ve not worked with Julia, I think I can tell you how much of a delight it is to someone who is used to working with other languages. Things that are hard in many languages are often quite easy in Julia &amp;ndash; I might argue that this is partly because the way many people&amp;rsquo;s brains think is reasonably well aligned with the semantics of Julia code.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve heard this from lots of other folks too. A common experience (but not universal) is that people feel &lt;em&gt;happy&lt;/em&gt; to write code again. I&amp;rsquo;m not sure if you&amp;rsquo;ve had this experience, but learning to program for the first time can feel kind of incredible. Seeing stuff print out to the terminal, proudly running some dumpster fire of a calculator that took you four days with no errors, or maybe just pushing through some obnoxious bug for hours on end only to face the euphoria of fixing it.&lt;/p&gt;
&lt;p&gt;I felt this when I learned Julia, after I&amp;rsquo;d become a little jaded by working on other languages. And I often still feel it when I&amp;rsquo;m working in the language. It&amp;rsquo;s just a stream of constant delights when I get the opportunity to use it.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a world where we teach people how to write code, and where their first language is Julia. Python I think is hard to beat on this front, in large part because of how many established tools there are for learning (Stack Overflow, forums, books, etc.)&lt;/p&gt;
&lt;p&gt;I think it&amp;rsquo;s worth thinking about though. Python, which is many people&amp;rsquo;s first language nowadays, has this unfortunate problem that a lot of folks like to call the &amp;ldquo;two language&amp;rdquo; problem. Rather than rehash that as many Julia people do, I want to point out exactly what you lost when you teach people a language that has an &amp;ldquo;interface&amp;rdquo; at the top level that people learn (Python) and a massive archive of high-performance code written in some &amp;ldquo;other&amp;rdquo; language (C++, Fortran, etc.).&lt;/p&gt;
&lt;p&gt;What happens is that all of that distant, high-performance code that &lt;strong&gt;someone else&lt;/strong&gt; writes feels impossible to grasp, especially if you&amp;rsquo;re learning to program for the first time. I don&amp;rsquo;t know about you, but when I started to learn programming I did not feel like I would ever be able to write anything that was &amp;ldquo;best practices&amp;rdquo; or fast or whatever. I&amp;rsquo;d behappy with printing hello world or remembering the syntax for if/else blocks.&lt;/p&gt;
&lt;p&gt;To highlight this distinction, I like to think of the two parts of programming languages that we tend to inhabit. The &lt;strong&gt;front&lt;/strong&gt; of the programming language is the part where you enter when you first learn it. Syntax, basic control flow, how to call functions, etc. This is essentially the userspace of a language. The next part, the &lt;strong&gt;back&lt;/strong&gt;, is where you start rooting in the internals. Writing packages/modules/libraries. This is where you start really engineering and modifying things.&lt;/p&gt;
&lt;p&gt;The advantage of Julia is that you can start in this really basic space that Python inhabits. The &lt;strong&gt;front&lt;/strong&gt; of Python and Julia are very similar. You can write your really simple programs, use it as a calculator, whatever. It&amp;rsquo;s a safe, comfortable place to learn the basics of computing without too much stress &amp;ndash; Julia is often quite forgiving, as is Python. Being a Julia &lt;em&gt;user&lt;/em&gt; is a delight, and can often feel effortless and fluid when you sink into it.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;back&lt;/strong&gt; of Julia is where it really starts to shine, especially when you compare it to Python. I would argue that Python has about half the back of Julia, in large part because all of the cool numeric stuff in Python is hidden elsewhere, developed by someone else, lost to an early developer&amp;rsquo;s limited grasp on multiple programming languages. You can go pretty far. You can write webservers, static site builders, or even write massive libraries to fit neural networks.&lt;/p&gt;
&lt;p&gt;But if you are a person who has lots of skills in a particular area that is NOT programming in one of the high-back languages like Fortran, it may feel impossible to build a system that has your needs in mind.&lt;/p&gt;
&lt;p&gt;In Julia, the back is flexible, and as deep as you can possible get. You can dive into the compiler of the language, or how things are being allocated, types, etc. You can build enormous, relatively high performance tools with no switching costs between languages.&lt;/p&gt;
&lt;p&gt;As an example, I wrote a packages with David Widmann and a few others called &lt;a href=&#34;https://github.com/TuringLang/AbstractMCMC.jl&#34;&gt;AbstractMCMC.jl&lt;/a&gt;. This package essentially provides and interface to common MCMC tasks, and it guides the framework for lots of the way that the code works in Turing.jl and it&amp;rsquo;s various packages. It was pretty straightforward, even if it was more of an engineering task than a scientific one. But being able to build it made the scientific part of the work easier to do. There wasn&amp;rsquo;t a point in the development of AbstactMCMC.jl where I thought &amp;ldquo;I can&amp;rsquo;t do this&amp;rdquo; because I was having trouble getting my linker to find some weirdo library or whatever. It just worked because Julia is a great tool for getting thoughts out of your head into the computer.&lt;/p&gt;
&lt;p&gt;I think we should give this opportunity to more people if we can. When you learn how to write Julia, you&amp;rsquo;re learning how to understand the depths of a beautiful language just as well as you understand the easy, accessible, userspace of the language. I personally would like to see less programmers be dissuaded by trying to learn C++ (as I nearly was) and more programmers given the joy of building something incredible just because they had an interesting idea.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re interested in learning Julia, I can highly recommend it as an experience. Here&amp;rsquo;s a list of a few resources to get you started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://juliaacademy.com&#34;&gt;JuliaAcademy &lt;/a&gt; for videos and tutorials&lt;/li&gt;
&lt;li&gt;Going to the &lt;a href=&#34;https://discourse.julialang.org&#34;&gt;Julia Discourse&lt;/a&gt; for help&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://docs.julialang.org/en/v1/&#34;&gt;Julia manual&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.youtube.com/watch?v=JYs_94znYy0&#34;&gt;Julia in 100 Seconds&lt;/a&gt; video&lt;/li&gt;
&lt;li&gt;The wonderful &lt;a href=&#34;https://www.youtube.com/@doggodotjl&#34;&gt;doggo dot jl&lt;/a&gt; channel on YouTube&lt;/li&gt;
&lt;li&gt;My video series &lt;a href=&#34;https://youtube.com/playlist?list=PLbuwVVKCI3sRW0Y5ehBFwdFVuyuy87ram&#34;&gt;Julia for Economists&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll wrap up here for now. I &lt;em&gt;love&lt;/em&gt; working in Julia to this day and I wanted to share a bit about why I think that in here.&lt;/p&gt;
&lt;p&gt;To summarize,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I wish that we as a culture spent more time working on developing Julia the language, and developing Julia packages, tutorials, blogs, etc.&lt;/li&gt;
&lt;li&gt;Companies should be experimenting with Julia somewhere in their processes.&lt;/li&gt;
&lt;li&gt;People learning to code should consider learning with Julia.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope to see you at the next JuliaCon! I&amp;rsquo;ll definitely be there.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting the Pile</title>
      <link>/2023/06/29/getting-the-pile/</link>
      <pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/2023/06/29/getting-the-pile/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been interested in various NLP stuff lately, as one might imagine with all the ChatGPT stuff going on. Something I&amp;rsquo;ve become interested in is methods for anlayzing large amounts of text. I&amp;rsquo;ve been looking at the &lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;Pile&lt;/a&gt; dataset, which is a commonly-used dataset in NLP. I believe ChatGPT has been trained on it, as have many other large &lt;a href=&#34;https://en.wikipedia.org/wiki/Foundation_models&#34;&gt;foundation models&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m trying to download it to tinker with &lt;a href=&#34;https://arxiv.org/abs/1905.10347&#34;&gt;discrete normalizing flows&lt;/a&gt; for token prediction. It&amp;rsquo;s a big dataset &amp;ndash; about 825GB uncompressed. Being a hardo, I wrote my only little cloning script to pull in all the new data. It&amp;rsquo;s not very efficient, but it works. I&amp;rsquo;ll probably write a better one later.&lt;/p&gt;
&lt;p&gt;If you want to use this code, make sure to change the &lt;code&gt;data_dir&lt;/code&gt; variable to wherever you want to store the data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; HTTP
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; ProgressMeter
&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; SHA
&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; Downloads

&lt;span style=&#34;color:#75715e&#34;&gt;# Pile root directory&lt;/span&gt;
pile_root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://the-eye.eu/public/AI/pile/&amp;#34;&lt;/span&gt;
data_dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/the-pile/mirror/&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Links path&lt;/span&gt;
links_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; joinpath(data_dir, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;links.txt&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; update_progress(meter, total, now)
    meter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; total
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; now &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; total
        &lt;span style=&#34;color:#75715e&#34;&gt;# println(&amp;#34;Done!&amp;#34;)&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
        update!(meter, now)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Extract the links from a url and return them as a vector. Remove any any links that include
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;..&amp;#34; in the path.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; extract_links(url)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Send simple get query to pile root directory&lt;/span&gt;
    response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; HTTP&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(url)
    body &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;String&lt;/span&gt;(response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;body)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Extract hrefs from html&lt;/span&gt;
    hrefs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; eachmatch(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;(?&amp;lt;=href=\&amp;#34;)[^\&amp;#34;]+&amp;#34;&lt;/span&gt;, body)
    links &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; map(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; url &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;match, hrefs)
    filter!(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; basename(dirname(x)) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;..&amp;#34;&lt;/span&gt;, links)

    &lt;span style=&#34;color:#75715e&#34;&gt;# # Write links to file&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# open(links_path, &amp;#34;w&amp;#34;) do f&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;#     for link in links&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;#         println(f, link)&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;#     end&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# end&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Find all the links that are directories&lt;/span&gt;
    dirs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; filter(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; endswith(x, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;), links)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Call extract_links on each directory and concatenate the results&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; dir &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; dirs
        links &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vcat(links, extract_links(dir))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Remove duplicates&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; unique(links)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;isfile(links_path)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Send simple get query to pile root directory&lt;/span&gt;
    links &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_links(pile_root)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Write links to file&lt;/span&gt;
    open(links_path, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; f
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; link &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; links
            println(f, link)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Fink the link that contains SHA&lt;/span&gt;
    sha_link &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; links[findfirst(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; occursin(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SHA&amp;#34;&lt;/span&gt;, x), links)]

    &lt;span style=&#34;color:#75715e&#34;&gt;# Download the SHA file if it doesn&amp;#39;t exist&lt;/span&gt;
    ddir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; joinpath(data_dir, basename(sha_link))
    &lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;isdir(dirname(ddir)) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; mkdir(dirname(ddir))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;isfile(ddir)
        download(sha_link, ddir)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Read the SHA file&lt;/span&gt;
sha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(ddir) &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; f
    read(f, &lt;span style=&#34;color:#66d9ef&#34;&gt;String&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Split the SHA file into lines&lt;/span&gt;
lines &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; split(sha, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Split each line into SHA and file name&lt;/span&gt;
lines &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; map(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; split(x, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;), lines)

&lt;span style=&#34;color:#75715e&#34;&gt;# Filter out empty lines&lt;/span&gt;
lines &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; filter(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; length(x) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, [filter(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; length(x) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, line) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; lines])

&lt;span style=&#34;color:#75715e&#34;&gt;# Separate into filename and sha&lt;/span&gt;
filenames &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [joinpath(line[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; lines]
shas &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [line[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; lines]

&lt;span style=&#34;color:#75715e&#34;&gt;# Create a dictionary of filenames and shas&lt;/span&gt;
sha_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Dict&lt;/span&gt;(zip(filenames, shas))

&lt;span style=&#34;color:#75715e&#34;&gt;# Open links file&lt;/span&gt;
links &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(links_path) &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; f
    readlines(f)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Filter out links ending in /&lt;/span&gt;
filter!(x &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;endswith(x, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;), links)

&lt;span style=&#34;color:#75715e&#34;&gt;# For each link, check if it&amp;#39;s been downloaded&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; link &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; links
    &lt;span style=&#34;color:#75715e&#34;&gt;# Get the filename&lt;/span&gt;
    file_relative &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; replace(link, pile_root &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Check if the file exists&lt;/span&gt;
    file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; joinpath(data_dir, file_relative)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Determine whether to re-download the file&lt;/span&gt;
    download_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isfile(file)
        &lt;span style=&#34;color:#75715e&#34;&gt;# Check if the file is the correct size&lt;/span&gt;
        file_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; filesize(file)

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; file_size &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; 
            true
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
            sha_local &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(file) &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; f
                SHA&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sha2_256(f)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; haskey(sha_dict, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; file_relative)
                sha_local &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; sha_dict[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; file_relative]
            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
                true
            &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
        true
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Download the file if necessary&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; download_file
        &lt;span style=&#34;color:#75715e&#34;&gt;# Create the directory if it doesn&amp;#39;t exist&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;isdir(dirname(file)) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; mkdir(dirname(file))

        &lt;span style=&#34;color:#75715e&#34;&gt;# Make the meter&lt;/span&gt;
        p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ProgressMeter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Progress(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; desc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;file_relative, dt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        update_fun(total, now) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; update_progress(p, total, now)

        &lt;span style=&#34;color:#75715e&#34;&gt;# Download the file&lt;/span&gt;
        println(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Downloading &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;$file&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
        Downloads&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;download(link, file, progress&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;update_fun)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Graduation</title>
      <link>/2022/02/23/graduation/</link>
      <pubDate>Wed, 23 Feb 2022 08:10:00 -0800</pubDate>
      
      <guid>/2022/02/23/graduation/</guid>
      <description>&lt;p&gt;I defended my dissertation on August 19th, 2022. I passed! As of now I am more or less Dr. Pfiffer, ignoring some clerical tasks. I was completely overwhelmed by everyone&amp;rsquo;s support and kindness during my defense and I was just ecstatic to be able to share the finish line with friends and family. It has been an extremely difficult and challenging four years and I was relieved to get my dissertation handed in and defended.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;lugano&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;lugano&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;Celebrating with some champagne. &lt;img src=&#34;/images/grad-champ.jpg&#34; alt=&#34;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Doing a PhD is not easy. I have nuanced opinions about the PhD and the effects it can have on your life. I&amp;rsquo;ve commented on the PhD several times on Twitter, but many of those comments are small snapshots that don&amp;rsquo;t accurately convey my full experience and perspective on the PhD. So, in the wake of my PhD, I thought it might be appropriate to try and collect the disparate thoughts I have about my PhD and whether I think the experience was valuable. Additionally, I have a lot of personal context about my relationship to education and to my mental health that is important to convey to fully understand why I feel the way I do, so there&amp;rsquo;s some personal stuff in here.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll start at the beginning of my education and try and tell something like a linear story. First I want to start with my family background because it is the primary context through which I view my education. My home life was challenging and it ended up playing a large role in what and how I learned.&lt;/p&gt;
&lt;p&gt;If you want the answer to the &amp;ldquo;should you do a PhD&amp;rdquo; question, skip to the bottom where I summarize my experiences.&lt;/p&gt;
&lt;h2 id=&#34;family-background&#34;&gt;Family background&lt;/h2&gt;
&lt;p&gt;My parents are both from a small town in upstate New York called Horseheads. Horseheads is an extremely podunk little town that has mostly been in decline over my nearly three decades of life, and it never really started out as a wealthy area. As a kid I spent many of my summers there with my grandparents on both sides. I recall it being hot, muggy, and full of bugs, very unlike the Oregon weather I was used to.&lt;/p&gt;
&lt;p&gt;My mother was born into a working-class family. Her father was a painter and her mother a receptionist. My dad&amp;rsquo;s family was also working-class &amp;ndash; his dad was the night manager at a salt factory, and his mom was a school nurse for many years. Both of their families were large and poor, dad has three siblings, mom has 6 siblings. It was difficult for them to have new clothes or toys.&lt;/p&gt;
&lt;p&gt;My mom served in the U.S. Army, like many of her siblings, and never pursued higher education. I consider my mother to be extraordinarily street-smart. She is a student of the soul &amp;ndash; she wants to understand why we feel how we do, and what meaning there is to find in life. When I think of what I learned from her, I think of how she taught me to be kind, thoughtful, and curious.&lt;/p&gt;
&lt;p&gt;My dad was the first in the family to attend a four-year university. At a young age he was fascinated with this whole &amp;ldquo;computing&amp;rdquo; thing that was happening. My understanding is that he built a &lt;a href=&#34;https://en.wikipedia.org/wiki/COSMAC_ELF&#34;&gt;COSMAC ELF&lt;/a&gt; himself when he was a preteen, and spent a lot of time tinkering and learning to write code. He studied computer science at a small state school in New York. He struggled in school just like I did, and I grew up hearing about how this person who I thought of as the most intelligent person in the world was incapable of doing calculus. It always hung over my head &amp;ndash; &amp;ldquo;if dad can&amp;rsquo;t do this, why should I be able to?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;computer&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;computer&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;One of the greater gifts I received from my father was his attempt to recreate this experience of building a computer. My brother and I were quite young when he had us build our own computers, I must have been somewhere between 5-7 when I built my first computer. I remember pinching my fingers in the case and getting a small wound. My dad&amp;rsquo;s response was &amp;ldquo;It&amp;rsquo;s not your computer unless there&amp;rsquo;s blood in it.&amp;quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Neither of my parents believed strongly in manhandling my education. My mother&amp;rsquo;s belief was always that I would find my own way and it was important to provide agency, while my sense from my dad was that it just wasn&amp;rsquo;t really his job. I was kind of left to my own devices for most of my education. My parent&amp;rsquo;s didn&amp;rsquo;t &lt;em&gt;really&lt;/em&gt; check in on whether my homework was done, with the exception of several times when my teachers communicated expressly to my parents that something needed to be monitored or check in on.&lt;/p&gt;
&lt;p&gt;Even for higher education, there was kind of an implicit assumption I&amp;rsquo;d figure it out somehow. When the ACTs came up, I had zero prep &amp;ndash; I didn&amp;rsquo;t even know they were happening or what they were. From my perspective, it was another standardized test that the school district made you take, and not an incredibly important test that can be a key determinant in which undergraduate schools will take you. I did extraordinarily well on the reading and writing portion (both 95th percentile) and &lt;em&gt;abysmal&lt;/em&gt; on the math portion (20th-30th percentile). I know many students who have a similar level of educational attainment, most of whom had some form of coaching from their parents. I did not. I blundered my way through school using only what I could figure out, and this was in large part to my mother&amp;rsquo;s intentional laissez-faire attitude and my father&amp;rsquo;s unintentional one.&lt;/p&gt;
&lt;p&gt;My parents relationship started to crumble in late middle school and early high school. This wasn&amp;rsquo;t a big fracture, the way a bone might break if you fall from a ledge. This was a slow, crushing, splintering, as if someone put your femur in a crusher and put it on the slowest setting. My parents should probably have gone their separate ways in my pre-teens. They did not get divorced until I was 28, and lived in the same home for the entire duration. I won&amp;rsquo;t describe the home situation, but rest assured my childhood home was permeated with this intangible black cloud that touched everyone&amp;rsquo;s lives in insiduious ways. My brother acted in, I acted out, my mother closed off, and my dad retreated entirely. I went from a moderately athletic child to a morbidly obese teenager in short order, and found food to be a way to attempt to reconcile my splintering nuclear family.&lt;/p&gt;
&lt;p&gt;I never &lt;em&gt;noticed&lt;/em&gt; consciously what was happening. All I remember is acquaintances or friends coming over and commenting on the chilly aura of the home. They described things as feeling &lt;strong&gt;wrong&lt;/strong&gt;, and I could never understand what they were talking about. Didn&amp;rsquo;t their family have a molding jug of Arizona Green Tea placed spitefully in the foyer, uncleaned for two years? Didn&amp;rsquo;t they have a pile of rejected gifts in the living room, and didn&amp;rsquo;t their parents avoid eye contact and residing in the same room? I couldn&amp;rsquo;t understand at the time, but it directed everything I felt and did.&lt;/p&gt;
&lt;p&gt;I should mention too that my family is quite well off, but I never felt that way. It was a struggle to get access to basic things like clothing. We never went on vacations after I was eight or so. My dad wore shredded sweatshirts from his teenage years, wore boots with holes, and drove a beat-up 1986 Toyota MR2 with an algae-filled spiderweb crack all over the windshield. I found out much later in life that my dad&amp;rsquo;s long-time career as an engineer put us well into the first-percentile wealth bracket, though my personal experience of our family was that spending money was a crime. It made you look ostentatious. My dad made us stop skiing in the winters because he didn&amp;rsquo;t want people to think we had money. My brother and I stopped asking for things we thought we needed or thought would be fun because we didn&amp;rsquo;t want to feel like we were asking for too much. I still struggle with communicating my needs and desires to this day.&lt;/p&gt;
&lt;p&gt;My mom tried very hard to get my brother and I what we needed, but as my parents&#39; relationship got more complicated it become difficult for her to get access to their shared money. The one thing she was adamant about was that my education should always be paid for, and even as I was struggling to pay for electricity in my early undergraduate years, she was always able to draw tuition money to get me through the basics of college. I believe my father never liked this &amp;ndash; my sense was that he wanted me to pay my entire way through college, which as you may know is much harder to do nowadays than when he went to school.&lt;/p&gt;
&lt;h2 id=&#34;educational-background&#34;&gt;Educational background&lt;/h2&gt;
&lt;p&gt;I have not been a conventionally good student for most of my education. Many of my past teachers would likely say something along the lines of &amp;ldquo;Cameron is smart, he just doesn&amp;rsquo;t apply himself!&amp;rdquo;, and would likely have done so starting from preschool all the way through undergraduate school. I struggled a lot with basic mathematical concepts. I never turned in homework, didn&amp;rsquo;t do assigned reading, acted out in class, couldn&amp;rsquo;t focus, and felt no impulse to succeed academically.&lt;/p&gt;
&lt;p&gt;My stronger memories of my education start in elementary school. I recall learning times tables  in second and third grade, and just &lt;em&gt;sobbing&lt;/em&gt; because I could not figure out what to do. My teacher in both those grades was Maria Wickwire, an absolute saint &amp;ndash; I recall her being so patient and caring with me while I cried in front of my times tables.&lt;/p&gt;
&lt;p&gt;This kind of thing continued in basically every grade. Math was a persistent issue for me. In grades 6-8, I had the same math teacher for three years. Tammy Schraeder was my homeroom teacher and my math teacher, and she was again kind and patient while I cried over trigonometry and pre-algebra. Even at this time I was struggling with basic arithmetic.&lt;/p&gt;
&lt;p&gt;In late middle school and early high school, my home life started to get complicated. My parents relationship was strained and getting worse. I internalized a lot of their interactions, and I started acting out in strange ways. I completely stopped turning in homework. In high school, I remember reading a book a day because I read in class and never paid attention to the lecture unless I&amp;rsquo;d finished my book early in the day. My grades were &lt;em&gt;terrible&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The only reason I passed most classes was because there was often a &amp;ldquo;pass the final, pass the class&amp;rdquo; rule. I generally did okay when I sat down to work, but simply could not bring myself to complete homework at home. Most of the reason for this was the slow deterioration of my parent&amp;rsquo;s relationship &amp;ndash; when I was home, I was trying to escape. I played Second Life for hours and hours a day. Anything to pretend whatever was going on was fine.&lt;/p&gt;
&lt;p&gt;I ended up finding a life and a home in the theater department. In a lot of ways, theater was &lt;em&gt;amazing&lt;/em&gt; for me. I found a community where previously I had been completely adrift. I found friendships challenging (and still do) and it was so refreshing to be around people I liked, who liked me. I even found my now ex-wife, a person who I still regard very highly. Theater gave me a purpose and a community, and I started to think more about my trajectory in life and what I wanted for myself.&lt;/p&gt;
&lt;p&gt;I ended up going to a community college. There was never any pressure from either of my parents to pick a school or decide where to go. I never applied to any schools other than the community college &amp;ndash; it just seemed like something to do, and my parents were fine with paying for it. I recall trying to do a computer science associate degree, becoming frustrated with learning c++ and the associated math. I leaned into my theater career and switched to taking many more theater courses.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;congo&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;congo&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;The ETC Congo, the lighting console I spent a lot of time working with.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/congo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I spent most of the hours I had at the theater at Portland Community College. I loved working in tight crews of skilled people. I was a lighting technician and designer, and the feeling of being in a &lt;em&gt;team&lt;/em&gt; was just pure exhilaration. I was strong and getting stronger. I loved learning about carpentry, metalworking, rigging, electricity, anything I could get my hands on. Theater gave me the space to be &lt;em&gt;useful&lt;/em&gt; to people in a way I had not experienced before and it was the greatest experience in the world. And people wanted to pay me! I got drafted into the rental crew at the theater, and I got to work and make money in these tight teams.&lt;/p&gt;
&lt;p&gt;My ex-wife and I eventually got married around the time I finished my two years at community college, and we both went to Southern Oregon University (SOU) to continue our respective theater careers. SOU is very much a teaching school. It&amp;rsquo;s small but had a great theater program. I was terrified to try for a better school because of cost &amp;ndash; I didn&amp;rsquo;t want my mother to have to siphon off too much money from my parent&amp;rsquo;s shared funds, so rather than ask I went to an extraordinarily cheap school.&lt;/p&gt;
&lt;p&gt;The school was good for me. The instructors were good, the theater program was excellent, and Ashland, Oregon is one of the most beautiful towns in the world. I engaged a little more than I had in high school, but my grades generally remained quite poor until I started a technical theater club with some other folks.&lt;/p&gt;
&lt;p&gt;The goal of the club was to send SOU theater students to the &lt;a href=&#34;https://www.usitt.org/&#34;&gt;USITT conference&lt;/a&gt;, which is a big entertainment technology conference. We found out that if you sent a club representative to this committee you could get money for the club to do things. I volunteered to go because it seemed interesting. When I got to the representative meeting at the start of the year, they asked if anyone would like to join the committee of representatives that would allocate funds to the clubs. Thinking this would make it more likely that I could get funds for my club, I joined the committee and stayed for an hour after the main meeting. In the smaller meeting, the school representatives asked if anyone would like to chair the committee, to which I thought &amp;ldquo;gee, another way to get money for my club&amp;rdquo;. I raised my hand and chaired the committee for the academic year.&lt;/p&gt;
&lt;p&gt;Working on the allocations committee was &lt;em&gt;awesome&lt;/em&gt;. I got to think about our $100k budget, who it would go to. It was social and engaging. I thought I should take some accounting classes for fun to supplement the experience. I remember being the only non-business major in my accounting classes and having an absolute blast &amp;ndash; &amp;ldquo;Guys! At the bottom of the balance sheet, the numbers are &lt;em&gt;the same!&lt;/em&gt; Can you believe it?!&amp;rdquo; I would exclaim to the business students who could not care less.&lt;/p&gt;
&lt;p&gt;I started taking finance courses too because it seemed like a natural extension. I recall my first finance class. Dr. Curtis Bacon was the instructor. In the first class, he covered the time value of money and I was just &lt;em&gt;blown away&lt;/em&gt;. You mean to tell me that 90.90 today is 100 in a year at a 10% discount rate? Wild stuff.&lt;/p&gt;
&lt;p&gt;I started doing really well in my classes. I was acing all the accounting and finance classes. Multiple teachers recommended some kind of graduate school because it was too late to change my major from theater to business. I listened to them and started preparing for graduate school. I was still &lt;em&gt;terrible&lt;/em&gt; at math. I took the GMATs and was in the 30th percentile for math. To solve this, I would wake up at 4am every morning and do practice GMAT math questions for two hours a day, sometimes more. I was slowly learning all the algebra and geometry that I had resisted in my K-12 years. I took a calculus class, which I had to get special approval to take because I did not have the pre-requisites, and placed third in the class that year. I loved calculus now and found it easy.&lt;/p&gt;
&lt;p&gt;I managed to get my GMAT math score to the 60th percentile, and applied to three masters schools, though again I still had no idea how to do this or what schools to target. My mother couldn&amp;rsquo;t help and my father and I were mostly estranged at this point, even if he would have had a strong opinion. I ended up getting accepted to the Masters of Finance program at the University of Reading, England. I was accepted to a few other places, but I chose Reading because England seemed interesting.&lt;/p&gt;
&lt;p&gt;I really liked learning about finance there. And I was a great student. I really started to excel &amp;ndash; my grades were great, I stood out in the cohort, and I had a wonderful time. After the year was up I went back to Oregon to look for work. People didn&amp;rsquo;t really want to hire me. The theater arts degree threw people, nobody knew where Reading was. I applied to well over 150 positions and never heard back.&lt;/p&gt;
&lt;p&gt;This company, ACA Compliance Group, reached out to me. They were a large investment consultancy in a small town in Southern Oregon. I worked here and quite liked it, though the pace was perhaps too relaxed. It was a lot of work in Excel and rotating data. I was bored quickly by this style of work and sought to automate it &amp;ndash; I had text files of VBA macros written to accelerate a lot of basic tasks I was doing. Management found out about this and asked if I&amp;rsquo;d like to work on the engineering team, which provided an in-house Excel ribbon that automated much of the company.&lt;/p&gt;
&lt;p&gt;I started doing my normal analyst job and software development for the firm. It was the best. I loved engineering and learning how to write C#, make people&amp;rsquo;s work faster, and build things. But I still felt that I could do more. The accelerated learning I&amp;rsquo;d undertake to get to grad school was addictive &amp;ndash; I wanted to go back to school, to see what I was made of. I was studying for the CFA at the time, and decided to also study for the GRE.&lt;/p&gt;
&lt;p&gt;When I took the GRE, I again scored extraordinarily high in the verbal and written portions, and now after my masters and the prep for the GMAT, I scored in the 85th percentile for math. This is generally considered still a bit low for finance PhDs, but I figured I&amp;rsquo;d apply to schools anyway to see if I got in anywhere.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;waitlist&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;waitlist&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;One of the faculty at UO one told me &amp;ldquo;Why are you here? You should be at Cornell&amp;rdquo;, to which I responded that I had tried and they didn&amp;rsquo;t want me.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I believe I applied to the University of Oregon, MIT, Cornell, and the University of Washington. I only got in to the University of Oregon, and I found out later I was 7th on the waitlist. I was ecstatic anyway and really happy to attend. I quit my job and started the PhD in August of 2018.&lt;/p&gt;
&lt;h2 id=&#34;the-phd&#34;&gt;The PhD&lt;/h2&gt;
&lt;p&gt;The first year of an economics or finance PhD is brutal. Make no mistake about that. You are essentially run through a meatgrinder of econometrics, probability theory, economic theory, and various field-specific courses. The grades don&amp;rsquo;t matter at all but nobody believes it. Everyone in your cohort is about as Type-A overachieving as you could imagine. Imposter syndrome shows up for the first time in full force &amp;ndash; she seems like she&amp;rsquo;s got a handle on all of this and I am so &lt;em&gt;confused&lt;/em&gt;! Why am I doing this PhD? Why am I so bad at everything? It&amp;rsquo;s never just you &amp;ndash; everyone feels this way, everyone feels dumb, and the people who don&amp;rsquo;t feel dumb are not particularly introspective.&lt;/p&gt;
&lt;p&gt;I worked &lt;em&gt;all the time&lt;/em&gt; during my first year. It was about what I expected to be doing, and I was &lt;em&gt;happy&lt;/em&gt; to do it all because it fit a preconceived notion of what it meant to be scholarly and studious, and this was genuinely my first time being scholarly and studious at an academic institution. Where previously I had flunked or skated by, now I was finishing the homework the night it was assigned and doing the board work for the business school PhD students when we met to review on Monday. Further, I felt I had to be truly excellent because of my background &amp;ndash; talk more, be smarter, work harder. Everyone else had parents who guided them through school or studied something like math or economics, and I was just some dude who liked theater and failed most of my math classes. I also wanted to show to my parents that I was capable of doing a hard academic thing (&amp;ldquo;look! see what I can do?&amp;quot;).&lt;/p&gt;
&lt;p&gt;The first year was also when my marriage really started to struggle. It can be difficult to give your partner the attention they deserve when you are doing a PhD. You work and study on the weekends and feel guilty for even the barest moment of pleasure. My social life was always extraordinarily weak and it deteriorated even further during the PhD &amp;ndash; I was married and I just didn&amp;rsquo;t really feel that I fit in well with the single people in my cohort. It&amp;rsquo;s never been an easy thing for me to make friends and I found it so much harder to do so in graduate school. All this made me feel even worse with my family context &amp;ndash; I am a bad spouse! I am failing, just like my parents.&lt;/p&gt;
&lt;p&gt;The first year came and went quickly, though. Time flies when you&amp;rsquo;re buried in a book with limited social interaction, working every single day. After your first year is over, you have to start working on research. I was fortunate enough to have a fixation on a field (market microstructure) when I got to grad school, so my first year paper came together quite quickly. It is as many first-year papers are: poorly written, lacking a point, hopelessly lost in math I mostly just thought was cool.&lt;/p&gt;
&lt;p&gt;The second year is when things start getting more interesting. You have much less coursework to do, and so you can turn a little more into research. I found this to be difficult. The big thing I learned about myself in the second year is that I can spin my wheels for an eternity if I am left to my own devices, as you often are in a PhD. The University of Oregon has a fairly lax approach to its PhD students. I felt this very hard &amp;ndash; I started to become depressed and inactive, slow, unresponsive.&lt;/p&gt;
&lt;p&gt;The second year in our program is also the year when you take your comprehensive exams which determine whether you are allowed to continue in the program. In my first year there, one of the older students was failed and his cohort mate just barely skated by. It felt entirely possible that I would fail. Looking back, I know I was stressed out about this, but at the time I felt cool and collected even as my mood started worsening.&lt;/p&gt;
&lt;p&gt;Enter the COVID pandemic. The pandemic started around my Spring term, meaning the remaining studying for the comprehensive exams was to be done from home. I do not work well exclusively at home (I prefer a hybrid office). My wife and I had also just gotten a puppy about a week before the pandemic, and I was now trapped in a home where I was fully aware I was failing my duties as a husband and surrounded by a creature my ex and I lovingly called &amp;ldquo;The Poopshark&amp;rdquo; for reasons I will let you infer.&lt;/p&gt;
&lt;p&gt;I managed to do all my studying, however, and passed the comprehensive exams at the end of July 2020. I felt nothing about this, which for those of you who have experienced depression is a pretty common experience. I was starting to struggling with daily suicidal ideation. My mood was now completely untenable: I was irritable, at times angry, and generally unpleasant to be around. I sought counselling but was not able to connect with the correct therapist (recall that at this time everyone needed a therapist). A little less than a month after I passed my exams, in late August, my wife and I separated and made plans to file for divorce shortly after.&lt;/p&gt;
&lt;p&gt;My third year was basically a wash. I couldn&amp;rsquo;t move or think or do anything at all. The PhD life was completely unfulfilling. I was working hard for something I didn&amp;rsquo;t care about at all, for people I felt didn&amp;rsquo;t care about me, and worse, I felt terrible for my perceived and actual failures as a life partner. I would not generally say I accomplished much during this year and I suspect faculty and peers would agree. I deteriorated a lot. I was frequently at risk and my weight went up to 230lbs, the highest it had been since my teens.&lt;/p&gt;
&lt;h2 id=&#34;graduating-early&#34;&gt;Graduating early&lt;/h2&gt;
&lt;p&gt;In my fourth year, I was profoundly lucky. Shoshana Vasserman invited me to visit Stanford during the academic year, and I accepted! I got to move to Palo Alto, where I found a tremendous amount of joy. I started biking again, and meeting people, dating, exploring, and having fun! I was focused on making sure that I afforded myself time to live and laugh and play, rather than repeat the mistakes of the first year that hastened my descent into depression. I met someone (an extremely lovely someone!) who I am still with and who makes me laugh.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;congo&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;congo&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;Here I am with Patricia a few minutes after my dissertation defense.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/laugh.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I decided a few months into my Stanford visit that I wanted to graduate and move past my PhD. I wanted to live in the Bay area where I had found new people and experiences. My advisor, Ro Gutierrez, was supportive and respected my decision not to attend the job market, and helped me sketch out a plan to graduate early. I split my time at Stanford between projects for Shosh and trying to get my dissertation off the ground. Eventually, I left Stanford in early July to teach a summer course at the University of Oregon and complete my dissertation.&lt;/p&gt;
&lt;p&gt;The two months I&amp;rsquo;ve spent here in Eugene have been brutal. Teaching and writing more or less continuously have been really hard. Fortunately my hard work paid off! I defended my dissertation, it went well, and now I&amp;rsquo;m off to greener pastures.&lt;/p&gt;
&lt;h2 id=&#34;would-i-recommend-a-phd&#34;&gt;Would I recommend a PhD?&lt;/h2&gt;
&lt;p&gt;People have been asking me this a lot lately, and my knee-jerk reaction is to say &lt;em&gt;no do not do that to yourself&lt;/em&gt;. But there&amp;rsquo;s more here! My situation is not everyone&amp;rsquo;s situation. I can say for myself that the PhD has coincided with the worst years of my life and it is not even close.&lt;/p&gt;
&lt;p&gt;However. It is reductive to assume that the PhD &lt;em&gt;caused&lt;/em&gt; this horrible time for me. I have come to think of my PhD as a tremendous blessing that came at an equally tremendous cost. I thought I was invincible, that I had no mood disorders, no anxieties. That I could push my mind as far as I wanted with no cost.&lt;/p&gt;
&lt;p&gt;The PhD revealed to me how false this was. I am fragile, just as everyone else is. We are all fragile. We need love and acceptance and humor. People have to exercise! They need to go outside, or read a book for fun, or sit in a hammock. One simply cannot read paper after paper for days on end without falling apart. Everyone has different limits, certainly, and I found mine. I found them hard. I was at risk numerous times of doing some very stupid things to myself because I had crossed the line unwittingly, but the &lt;em&gt;act&lt;/em&gt; of crossing that line, an act afforded to me by my PhD, gave me the opportunity to introspect and try to determine what it was that I needed to be &lt;em&gt;happy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Is a PhD going to make you happy? Maybe not. A PhD is difficult, and it is a good way of introducing you to chronic workaholics with limited social capacity, similarly battered senses of work-life balance, and an occasionally unreasonable dedication to correctness. Few of these things nourish the soul. In my case, I suffered with the ultimate benefit of personal insight. There are other ways to get insight that are less isolating, painful, and challenging.&lt;/p&gt;
&lt;p&gt;This too is reductive, though, because the PhD comes with substantial perks. The PhD is true freedom. If I didn&amp;rsquo;t want to work, I probably could have just disappeared for two weeks with little consequence. Some may never even have noticed that I was gone. I could study what I wanted, learn whatever I felt was interesting. These things are all amazing perks to the naturally curious, and I indulged in them to the detriment of my personal life. But done correctly, it is possible to build a PhD for yourself that permits you a balance between this freedom to meander and the cost of excluding yourself form the rest of the world.&lt;/p&gt;
&lt;p&gt;Further, having a PhD, particularly a finance PhD, gives you a lot of credibility! It opens you up to a world of interesting and fun roles where you can continue to be curious and engaged. You also can work much easier jobs (coming from a former stagehand &amp;ndash; academics barely work, it&amp;rsquo;s a joke to me sometimes how little I do now). I no longer believe that I will reside at or below the poverty line until I die. This is not nothing! It is an important perk that can enable me to live a more balanced, peaceful, and fulfilling life.&lt;/p&gt;
&lt;p&gt;In my case, I found peace knowing that there are no more academic ladders to climb. I am done with getting accolades, and now I can find work that satisfies my interest and does not kill me. I am happy that I am finally credible &amp;ndash; people no longer look at my CV and see a stagehand with a strange background. They see a capable individual who did a hard thing and remains curious and engaged. Additionally, I found out who I am! I know now that I am predisposed to severe unipolar depression, that stress can cause extreme reactions in me, and that I need to see and speak to interesting people about things other than asset prices. I found beautiful and lovely people with lovely minds! These are all a gift I would never want to return.&lt;/p&gt;
&lt;p&gt;It did cost me. It cost me money &amp;ndash; as an engineer, I was painfully aware of my opportunity cost. Even now, I am unlikely to usurp the lost wages from some hypothetical career in tech. It also hurt me, emotionally, with a frequency unmatched by any other experience I&amp;rsquo;ve ever had. I would not say that the PhD cost me my marriage, for that is too convenient a scapegoat, but it did make it more challenging to address problems which were likely too big to resolve at any rate.&lt;/p&gt;
&lt;p&gt;The short answer here is that there is not one. PhDs are hard, and unique, and you may or may not find what you were looking for going in to the PhD. But maybe you&amp;rsquo;ll find what you weren&amp;rsquo;t looking for, which was exactly what I needed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2021 Retrospective</title>
      <link>/2021/12/31/2021-retrospective/</link>
      <pubDate>Fri, 31 Dec 2021 18:56:01 +0000</pubDate>
      
      <guid>/2021/12/31/2021-retrospective/</guid>
      <description>&lt;p&gt;This one&amp;rsquo;s kind of heavy but I think it&amp;rsquo;s important to talk about, and I wanted the opportunity to write about it. I discuss some mental health issues in general terms.&lt;/p&gt;
&lt;p&gt;It has been an interesting year for me. A brief summary of life events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Divorced&lt;/li&gt;
&lt;li&gt;Started rowing&lt;/li&gt;
&lt;li&gt;Moved to California to visit at Stanford&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During all of this I think the biggest thing I did this year was primarily internal and personal growth. I learned about what I value and what I want as an individual. Most of the things I accomplished this year have  little to do with my professional or academic life &amp;ndash; in fact, I would argue that I am better off for focusing just a little bit less on my professional life. My biggest job is my happiness.&lt;/p&gt;
&lt;p&gt;I struggled with severe depression surrounding my divorce. It was the the worst my mental health has ever been. I was in bad sorts. Fortunately, I&amp;rsquo;m out of that particular dark patch now &amp;ndash; therapy, rowing, and moving ended up being extraordinarily helpful. My family and friends have been nothing but loving and supportive&lt;/p&gt;
&lt;p&gt;I started to step back a little from my PhD work in 2021. When I first started my PhD, I worked all the time. I worked weekends. I got up early to work. I stayed up late to work. I wanted to be successful and I was willing to disregard a lot of things to get it. Things like my relationships and my happiness. I stopped knowing what made me feel good and I only acted to do what other people thought I should be doing.&lt;/p&gt;
&lt;p&gt;It turns out that this is not a healthy way to be! I forgot at some point that I have a lot of things to do as a person. My job is more or less to be happy, and my academic pursuits are only a piece of that. I am not sure how well this is received by the academic community that I&amp;rsquo;m a part of. A lot of the people are work with are (I think) substantially more capable of handling the strange expectations, workload, and pressure.&lt;/p&gt;
&lt;p&gt;PhDs are hard, and not just intellectually. They are a marathon of sprints. They can be devastatingly isolating. It can be difficult to understand that anything you are doing has any meaning whatsover to anyone on the planet, even at the same time as you may struggle to understand your own work. It can be a painful experience for a lot of people. There are many reasons why so many economics and economics-adjacent graduent students struggle with &lt;a href=&#34;https://scholar.harvard.edu/bolotnyy/publications/graduate-student-mental-health-lessons-american-economics-departments&#34;&gt;depression, suicidal ideation, anxiety, and poor mental health&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I definitely broke during 2021. But I didn&amp;rsquo;t break in a bad way, just a different way. I finally gained the perspective that so many things are simply not worth my unhappiness or anger. I do not &lt;em&gt;always&lt;/em&gt; have to say yes, I do not have to put aside the things that make me happy in the pursuit of accolades that don&amp;rsquo;t actually mean much to me.&lt;/p&gt;
&lt;p&gt;I started my PhD because I like to learn. That&amp;rsquo;s the whole reason. And I have done so much learning, at the expense of feeling. Which sounds hilarious to me to write, still, but it&amp;rsquo;s true! I leaned into learning hard and I got lost in the weeds.&lt;/p&gt;
&lt;p&gt;2021 was kind of a blessing in disguise in spite of how difficult it was. I lost my mind for a little while, but I traded it for right to feel like a whole person rather than a mindless, first-order-condition-deriving sadsack. I feel so much better now having gone through the ringer. Of course, I wouldn&amp;rsquo;t wish my year on anyone else, but I am contented that I had the opportunity to reframe my perception of my self, my work, and my life.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to go race. I&amp;rsquo;m going to ride my bike around. I&amp;rsquo;m going to spend time wiht people I like. I&amp;rsquo;m going to stop thinking &lt;em&gt;all the fucking time&lt;/em&gt; about that problem that nobody cares about, with the reminder that I started thinking about that problem because it interested me. I wanted to know what the answer was. Do I care if other people want to know the answer too? I mean, a little, but ultimately my work is meant to fulfill the part of me that is curious and likes to be challenged. But it is so small a part. It simply does not require the outsized stress I assigned to it.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m just not that worried anymore. I have enough skills and experience to work anywhere and for good compensation, with or without the PhD. I&amp;rsquo;m going to hit the job market with the perspective that I care about the people around me, where I live, what I do, and how I &lt;em&gt;feel&lt;/em&gt; in that place. If I am not happy I will simply go do something else because life is so stupidly short that it is not worth my persistent unhappiness to spend years and years smashing my face into activities I don&amp;rsquo;t consider meaningful.&lt;/p&gt;
&lt;p&gt;In some sense I am very thankful to Shoshana Vasserman here at Stanford&amp;rsquo;s GSB, who is sponsoring me to visit. It was such a wonderful boost to feel useful, to feel like I had an objective, something to look forward to. I am stimulated here at Stanford in a way I have &lt;strong&gt;never&lt;/strong&gt; felt elsewhere. I feel like I have a speciality I can offer others, whereas at my home institution I do not usually feel particularly special. The people here are kind and thoughtful and I generally get the sense that I have the opportunity to assist in meaningful research.&lt;/p&gt;
&lt;p&gt;I like writing code. I like rowing. I like building things. I like learning. I like riding my bike. I like going on walks. I like hikes. I like talking to people. I like going to dinner. I like reading. I like naps! And I&amp;rsquo;m going to try to do more of what I like because it makes me happy.&lt;/p&gt;
&lt;p&gt;I am happy. I feel fortunate to be where I am now. When I look to the future I no longer see a steep cliff or a roiling pitch-black cloud. I see a vast landscape bathed in the warm sun with lovingly maintained trails, meandering rivers, beautiful souls, and interesting challenges. Maybe even a bench or two to sit and take it all in once in a while.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>California</title>
      <link>/2021/10/25/california/</link>
      <pubDate>Mon, 25 Oct 2021 05:44:32 +0000</pubDate>
      
      <guid>/2021/10/25/california/</guid>
      <description>&lt;p&gt;Howdy! Been a while since I posted anything on my blog. I&amp;rsquo;d like to get back in the habit of posting stuff every once in a while. Writing is a good way of staying focused and thinking critically about whatever is happening at any point in time.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve just completed my move to Palo Alto. I&amp;rsquo;m down here for six months or so to work on IO-adjacent topics with &lt;a href=&#34;https://shoshanavasserman.com/&#34;&gt;Shoshana Vasserman&lt;/a&gt;, and hopefully meet with various other Stanford-people. The drive down (Eugene → Palo Alot) was brutal. We&amp;rsquo;re apparently in the midst of a &lt;a href=&#34;https://www.npr.org/2021/10/24/1048862514/powerful-storm-brings-heavy-rain-flooding-and-mud-flows-to-northern-california&#34;&gt;bomb cyclone&lt;/a&gt; or something, so it was just torrential downpour and high winds for the entirety of my two-day drive.&lt;/p&gt;
&lt;p&gt;On a professional note, it&amp;rsquo;s been a while since I had to scramble to do stuff. People have expectations of me and I have expectations of myself, which is absolutely wonderful to experience again. During the bulk of the pandemic, I&amp;rsquo;ve been entirely adrift because I never felt like I needed to go into the office. Zero accountability to anyone (including me). Now I&amp;rsquo;m feeling quite a bit better. Very light, bouncy, high energy.&lt;/p&gt;
&lt;p&gt;Recent reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cassola, N., Hortaçsu, A. and Kastl, J. (2013), &lt;em&gt;The 2007 Subprime Market Crisis Through the Lens of European Central Bank Auctions for Short‐Term Funds&lt;/em&gt;. Econometrica, 81: 1309-1345.&lt;/li&gt;
&lt;li&gt;Everyone&amp;rsquo;s license plates on the drive down here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LET&amp;rsquo;S GOOOOOOOOOOO&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian finance papers</title>
      <link>/2020/04/19/bayesian-finance-papers/</link>
      <pubDate>Sun, 19 Apr 2020 20:58:31 -0700</pubDate>
      
      <guid>/2020/04/19/bayesian-finance-papers/</guid>
      <description>&lt;p&gt;A list of Bayesian finance papers I&amp;rsquo;ve noticed. These are mostly sourced from the financial economics literature, and primarily so from the top three journals (&lt;em&gt;Journal of Finance&lt;/em&gt;, &lt;em&gt;Review of Financial Studies&lt;/em&gt;, and the &lt;em&gt;Journal of Financial Economics&lt;/em&gt;). I exclude theoretical papers because the Bayesian component is usually not the most interesting part. I favor empirical papers that use some kind of Bayesian method. Suggestions welcome!&lt;/p&gt;
&lt;p&gt;Note: a &lt;code&gt;.bib&lt;/code&gt; file can be found &lt;a href=&#34;/notes/bayes.bib&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;
&lt;p&gt;Anderson, Evan W., and Ai-Ru (Meg) Cheng. “Robust Bayesian Portfolio Choices.” The Review of Financial Studies 29, no. 5 (May 1, 2016): 1330–75. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhw001&#34;&gt;https://doi.org/10.1093/rfs/hhw001&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Avramov, Doron. “Stock Return Predictability and Asset Pricing Models.” The Review of Financial Studies 17, no. 3 (July 1, 2004): 699–738. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhg059&#34;&gt;https://doi.org/10.1093/rfs/hhg059&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;———. “Stock Return Predictability and Model Uncertainty.” Journal of Financial Economics 64, no. 3 (June 1, 2002): 423–58. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(02)00131-9&#34;&gt;https://doi.org/10.1016/S0304-405X(02)00131-9&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Baks, Klaas P., Andrew Metrick, and Jessica Wachter. “Should Investors Avoid All Actively Managed Mutual Funds? A Study in Bayesian Performance Evaluation.” The Journal of Finance 56, no. 1 (2001): 45–85. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00319&#34;&gt;https://doi.org/10.1111/0022-1082.00319&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Barillas, Francisco, and Jay Shanken. “Comparing Asset Pricing Models.” The Journal of Finance 73, no. 2 (2018): 715–54. &lt;a href=&#34;https://doi.org/10.1111/jofi.12607&#34;&gt;https://doi.org/10.1111/jofi.12607&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bates, David S. “Maximum Likelihood Estimation of Latent Affine Processes.” The Review of Financial Studies 19, no. 3 (October 1, 2006): 909–65. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhj022&#34;&gt;https://doi.org/10.1093/rfs/hhj022&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bollerslev, Tim, Benjamin Hood, John Huss, and Lasse Heje Pedersen. “Risk Everywhere: Modeling and Managing Volatility.” The Review of Financial Studies 31, no. 7 (July 1, 2018): 2729–73. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy041&#34;&gt;https://doi.org/10.1093/rfs/hhy041&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Brav, Alon. “Inference in Long-Horizon Event Studies: A Bayesian Approach with Application to Initial Public Offerings.” The Journal of Finance 55, no. 5 (2000): 1979–2016. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00279&#34;&gt;https://doi.org/10.1111/0022-1082.00279&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Buehlmaier, Matthias M. M., and Toni M. Whited. “Are Financial Constraints Priced? Evidence from Textual Analysis.” The Review of Financial Studies 31, no. 7 (July 1, 2018): 2693–2728. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy007&#34;&gt;https://doi.org/10.1093/rfs/hhy007&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bulkley, George, and Paolo Giordani. “Structural Breaks, Parameter Uncertainty, and Term Structure Puzzles.” Journal of Financial Economics 102, no. 1 (October 1, 2011): 222–32. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2011.05.009&#34;&gt;https://doi.org/10.1016/j.jfineco.2011.05.009&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Busse, Jeffrey A., and Paul J. Irvine. “Bayesian Alphas and Mutual Fund Persistence.” The Journal of Finance 61, no. 5 (2006): 2251–88. &lt;a href=&#34;https://doi.org/10.1111/j.1540-6261.2006.01057.x&#34;&gt;https://doi.org/10.1111/j.1540-6261.2006.01057.x&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cavagnaro, Daniel R., Berk A. Sensoy, Yingdi Wang, and Michael S. Weisbach. “Measuring Institutional Investors’ Skill at Making Private Equity Investments.” The Journal of Finance 74, no. 6 (2019): 3089–3134. &lt;a href=&#34;https://doi.org/10.1111/jofi.12783&#34;&gt;https://doi.org/10.1111/jofi.12783&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cremers, K. J. Martijn. “Stock Return Predictability: A Bayesian Model Selection Perspective.” The Review of Financial Studies 15, no. 4 (July 1, 2002): 1223–49. &lt;a href=&#34;https://doi.org/10.1093/rfs/15.4.1223&#34;&gt;https://doi.org/10.1093/rfs/15.4.1223&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dai, Qiang, Kenneth J. Singleton, and Wei Yang. “Regime Shifts in a Dynamic Term Structure Model of U.S. Treasury Bond Yields.” The Review of Financial Studies 20, no. 5 (September 1, 2007): 1669–1706. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhm021&#34;&gt;https://doi.org/10.1093/rfs/hhm021&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dangl, Thomas, and Michael Halling. “Predictive Regressions with Time-Varying Coefficients.” Journal of Financial Economics 106, no. 1 (October 1, 2012): 157–81. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2012.04.003&#34;&gt;https://doi.org/10.1016/j.jfineco.2012.04.003&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Durham, Garland B. “SV Mixture Models with Application to S&amp;amp;P 500 Index Returns.” Journal of Financial Economics 85, no. 3 (September 1, 2007): 822–56. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2006.06.005&#34;&gt;https://doi.org/10.1016/j.jfineco.2006.06.005&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Easley, David, Robert F. Engle, Maureen O’Hara, and Liuren Wu. “Time-Varying Arrival Rates of Informed and Uninformed Trades.” Journal of Financial Econometrics 6, no. 2 (March 1, 2008): 171–207. &lt;a href=&#34;https://doi.org/10.1093/jjfinec/nbn003&#34;&gt;https://doi.org/10.1093/jjfinec/nbn003&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Easley, David, Marcos Lopez de Prado, and Maureen O’Hara. “Discerning Information from Trade Data.” Journal of Financial Economics 120, no. 2 (May 1, 2016): 269–85. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2016.01.018&#34;&gt;https://doi.org/10.1016/j.jfineco.2016.01.018&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Frank, Murray Z., and Ali Sanati. “How Does the Stock Market Absorb Shocks?” Journal of Financial Economics 129, no. 1 (July 1, 2018): 136–53. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2018.04.002&#34;&gt;https://doi.org/10.1016/j.jfineco.2018.04.002&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Fulop, Andras, Junye Li, and Jun Yu. “Self-Exciting Jumps, Learning, and Asset Pricing Implications.” The Review of Financial Studies 28, no. 3 (March 1, 2015): 876–912. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhu078&#34;&gt;https://doi.org/10.1093/rfs/hhu078&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Gallant, A. Ronald, Mohammad R Jahan-Parvar, and Hening Liu. “Does Smooth Ambiguity Matter for Asset Pricing?” The Review of Financial Studies 32, no. 9 (September 1, 2019): 3617–66. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy118&#34;&gt;https://doi.org/10.1093/rfs/hhy118&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Garlappi, Lorenzo, Raman Uppal, and Tan Wang. “Portfolio Selection with Parameter and Model Uncertainty: A Multi-Prior Approach.” The Review of Financial Studies 20, no. 1 (January 1, 2007): 41–81. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhl003&#34;&gt;https://doi.org/10.1093/rfs/hhl003&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Geweke, John, and Guofu Zhou. “Measuring the Pricing Error of the Arbitrage Pricing Theory.” The Review of Financial Studies 9, no. 2 (April 1, 1996): 557–87. &lt;a href=&#34;https://doi.org/10.1093/rfs/9.2.557&#34;&gt;https://doi.org/10.1093/rfs/9.2.557&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Gray, Stephen F. “Modeling the Conditional Distribution of Interest Rates as a Regime-Switching Process.” Journal of Financial Economics 42, no. 1 (September 1, 1996): 27–62. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(96)00875-6&#34;&gt;https://doi.org/10.1016/0304-405X(96)00875-6&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Han, Yufeng. “Asset Allocation with a High Dimensional Latent Factor Stochastic Volatility Model.” The Review of Financial Studies 19, no. 1 (March 1, 2006): 237–71. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhj002&#34;&gt;https://doi.org/10.1093/rfs/hhj002&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Harvey, Campbell R., and Yan Liu. “Cross-Sectional Alpha Dispersion and Performance Evaluation.” Journal of Financial Economics 134, no. 2 (November 1, 2019): 273–96. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2019.04.005&#34;&gt;https://doi.org/10.1016/j.jfineco.2019.04.005&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;———. “Detecting Repeatable Performance.” The Review of Financial Studies 31, no. 7 (July 1, 2018): 2499–2552. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhy014&#34;&gt;https://doi.org/10.1093/rfs/hhy014&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Harvey, Campbell R., Yan Liu, and Heqing Zhu. “… and the Cross-Section of Expected Returns.” The Review of Financial Studies 29, no. 1 (January 1, 2016): 5–68. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhv059&#34;&gt;https://doi.org/10.1093/rfs/hhv059&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Harvey, Campbell R, and Guofu Zhou. “Bayesian Inference in Asset Pricing Tests.” Journal of Financial Economics 26, no. 2 (August 1, 1990): 221–54. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(90)90004-J&#34;&gt;https://doi.org/10.1016/0304-405X(90)90004-J&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Henkel, Sam James, J. Spencer Martin, and Federico Nardari. “Time-Varying Short-Horizon Predictability.” Journal of Financial Economics 99, no. 3 (March 1, 2011): 560–80. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2010.09.008&#34;&gt;https://doi.org/10.1016/j.jfineco.2010.09.008&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Johannes, Michael, Lars A. Lochstoer, and Yiqun Mou. “Learning about Consumption Dynamics.” The Journal of Finance 71, no. 2 (2016): 551–600. &lt;a href=&#34;https://doi.org/10.1111/jofi.12246&#34;&gt;https://doi.org/10.1111/jofi.12246&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Johannes, Michael S., Nicholas G. Polson, and Jonathan R. Stroud. “Optimal Filtering of Jump Diffusions: Extracting Latent States from Asset Prices.” The Review of Financial Studies 22, no. 7 (July 1, 2009): 2759–99. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhn110&#34;&gt;https://doi.org/10.1093/rfs/hhn110&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Jones, Christopher S. “Nonlinear Mean Reversion in the Short-Term Interest Rate.” The Review of Financial Studies 16, no. 3 (July 1, 2003): 793–843. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhg014&#34;&gt;https://doi.org/10.1093/rfs/hhg014&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Jones, Christopher S., and Jay Shanken. “Mutual Fund Performance with Learning across Funds.” Journal of Financial Economics 78, no. 3 (December 1, 2005): 507–52. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2004.08.009&#34;&gt;https://doi.org/10.1016/j.jfineco.2004.08.009&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Julliard, Christian, and Anisha Ghosh. “Can Rare Events Explain the Equity Premium Puzzle?” The Review of Financial Studies 25, no. 10 (October 1, 2012): 3037–76. &lt;a href=&#34;https://doi.org/10.1093/rfs/hhs078&#34;&gt;https://doi.org/10.1093/rfs/hhs078&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Kandel, Shmuel, and Robert F. Stambaugh. “On the Predictability of Stock Returns: An Asset-Allocation Perspective.” The Journal of Finance 51, no. 2 (1996): 385–424. &lt;a href=&#34;https://doi.org/10.1111/j.1540-6261.1996.tb02689.x&#34;&gt;https://doi.org/10.1111/j.1540-6261.1996.tb02689.x&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Klein, Roger W., and Vijay S. Bawa. “The Effect of Estimation Risk on Optimal Portfolio Choice.” Journal of Financial Economics 3, no. 3 (June 1, 1976): 215–31. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(76)90004-0&#34;&gt;https://doi.org/10.1016/0304-405X(76)90004-0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;———. “The Effect of Limited Information and Estimation Risk on Optimal Portfolio Diversification.” Journal of Financial Economics 5, no. 1 (August 1, 1977): 89–111. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(77)90031-9&#34;&gt;https://doi.org/10.1016/0304-405X(77)90031-9&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Lamoureux, Christopher G., and H. Douglas Witte. “Empirical Analysis of the Yield Curve: The Information in the Data Viewed through the Window of Cox, Ingersoll, and Ross.” The Journal of Finance 57, no. 3 (2002): 1479–1520. &lt;a href=&#34;https://doi.org/10.1111/1540-6261.00467&#34;&gt;https://doi.org/10.1111/1540-6261.00467&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Lamoureux, Christopher G., and Guofu Zhou. “Temporary Components of Stock Returns: What Do the Data Tell Us?” The Review of Financial Studies 9, no. 4 (October 1, 1996): 1033–59. &lt;a href=&#34;https://doi.org/10.1093/rfs/9.4.1033&#34;&gt;https://doi.org/10.1093/rfs/9.4.1033&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Li, Minqiang, Neil D. Pearson, and Allen M. Poteshman. “Conditional Estimation of Diffusion Processes.” Journal of Financial Economics 74, no. 1 (October 1, 2004): 31–66. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2004.03.001&#34;&gt;https://doi.org/10.1016/j.jfineco.2004.03.001&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;McCulloch, Robert, and Peter E. Rossi. “Posterior, Predictive, and Utility-Based Approaches to Testing the Arbitrage Pricing Theory.” Journal of Financial Economics 28, no. 1 (November 1, 1990): 7–38. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(90)90046-3&#34;&gt;https://doi.org/10.1016/0304-405X(90)90046-3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pástor, Ľuboš. “Portfolio Selection and Asset Pricing Models.” The Journal of Finance 55, no. 1 (2000): 179–223. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00204&#34;&gt;https://doi.org/10.1111/0022-1082.00204&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pástor, Ľuboš, and Robert F. Stambaugh. “Comparing Asset Pricing Models: An Investment Perspective.” Journal of Financial Economics 56, no. 3 (June 1, 2000): 335–81. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(00)00044-1&#34;&gt;https://doi.org/10.1016/S0304-405X(00)00044-1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;———. “Costs of Equity Capital and Model Mispricing.” The Journal of Finance 54, no. 1 (1999): 67–121. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00099&#34;&gt;https://doi.org/10.1111/0022-1082.00099&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;———. “Investing in Equity Mutual Funds.” Journal of Financial Economics 63, no. 3 (March 1, 2002): 351–80. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(02)00065-X&#34;&gt;https://doi.org/10.1016/S0304-405X(02)00065-X&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;———. “Mutual Fund Performance and Seemingly Unrelated Assets.” Journal of Financial Economics 63, no. 3 (March 1, 2002): 315–49. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(02)00064-8&#34;&gt;https://doi.org/10.1016/S0304-405X(02)00064-8&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pettenuzzo, Davide, Allan Timmermann, and Rossen Valkanov. “Forecasting Stock Returns under Economic Constraints.” Journal of Financial Economics 114, no. 3 (December 1, 2014): 517–53. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2014.07.015&#34;&gt;https://doi.org/10.1016/j.jfineco.2014.07.015&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Rouwenhorst, K. Geert. “Local Return Factors and Turnover in Emerging Stock Markets.” The Journal of Finance 54, no. 4 (1999): 1439–64. &lt;a href=&#34;https://doi.org/10.1111/0022-1082.00151&#34;&gt;https://doi.org/10.1111/0022-1082.00151&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Shanken, Jay. “A Bayesian Approach to Testing Portfolio Efficiency.” Journal of Financial Economics 19, no. 2 (December 1, 1987): 195–215. &lt;a href=&#34;https://doi.org/10.1016/0304-405X(87)90002-X&#34;&gt;https://doi.org/10.1016/0304-405X(87)90002-X&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Shanken, Jay, and Ane Tamayo. “Payout Yield, Risk, and Mispricing: A Bayesian Analysis.” Journal of Financial Economics 105, no. 1 (July 1, 2012): 131–52. &lt;a href=&#34;https://doi.org/10.1016/j.jfineco.2011.12.002&#34;&gt;https://doi.org/10.1016/j.jfineco.2011.12.002&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Stambaugh, Robert F. “Analyzing Investments Whose Histories Differ in Length.” Journal of Financial Economics 45, no. 3 (September 1, 1997): 285–331. &lt;a href=&#34;https://doi.org/10.1016/S0304-405X(97)00020-2&#34;&gt;https://doi.org/10.1016/S0304-405X(97)00020-2&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Econometrics</title>
      <link>/2020/03/24/bayesian-econometrics/</link>
      <pubDate>Tue, 24 Mar 2020 09:21:25 -0700</pubDate>
      
      <guid>/2020/03/24/bayesian-econometrics/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Since I&amp;rsquo;m in social distancing mode, I figured it would be a good time to write a blog post on Bayesian methods and financial economics. I have written a post in quite a while, as the past year and a half or so have been a busy time for me. The finance PhD takes up most of my time, as well as my work on &lt;a href=&#34;https://turing.ml&#34;&gt;Turing.jl&lt;/a&gt;, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probabilistic_programming&#34;&gt;probabilistic programming language&lt;/a&gt; (PPL) for Julia.&lt;/p&gt;
&lt;p&gt;Before I continue, I want to make sure people understand my perspective. I do not work in fields of economics that people tend to think of when they think of economics &amp;ndash; maybe you think of labor, health, or macroeconomics, all of which are valuable fields that I know very little about. I study finance, which is the study of how money and securities are used and what they do to the economy. Keep that in mind as we go along. I&amp;rsquo;m in a smaller subfield of economics that shares many of the same methodologies and language, but is applied to the theory of the firm and to asset prices.&lt;/p&gt;
&lt;p&gt;Back to Turing.jl. Everyone who works on Turing is of an extremely high quality level. They are all typically very skilled in their respective computational or statistical domains, and it is easy to feel a little out of place. I am not an optimization person, or a machine learning person, or even really anyone with any measure of formal engineering training&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I am, however, a financial economist (with training wheels). Working on Turing and spending a lot of time with CS and statistics people who are not economists has been particularly eye-opening, because economics is a unique field that I think tends to stand out in the sciences. Here&amp;rsquo;s why:&lt;/p&gt;
&lt;h2 id=&#34;ground-truth&#34;&gt;Ground truth&lt;/h2&gt;
&lt;p&gt;As with all social sciences, &lt;strong&gt;ground truth is hard to come by&lt;/strong&gt;. In any of the hard sciences like physics, you can hypothesize something, and then sometimes you can spend hundred of millions of dollars to see if it is true. In economics, we don&amp;rsquo;t really have this. You can&amp;rsquo;t run experiments where you make half of all pregnant mothers smoke to see what happens to their kids, or randomly assign particular directors to company boards.&lt;/p&gt;
&lt;h2 id=&#34;causal-inference&#34;&gt;Causal inference&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Causal inference is the name of the game&lt;/strong&gt;. Economics is about how thing A causes thing B to change. The field has built up an enormous set of statistical tools just to identify whether and how a thing is causal, and many of these tools are commonly only used in social sciences&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;economists-love-math&#34;&gt;Economists love math&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Economics is mathematical&lt;/strong&gt;. Because economists don&amp;rsquo;t have ground truth, they build models of behavior and attempt to match empirical facts to what theory suggests should exist. Economists tend to bash other social scientists (especially sociology, sorry folks) because their methods are less sophisticated. Economists even bash financial economists like me, because we tend to be 5-10 years behind economics writ-large in terms of empirical and theoretical methodologies.&lt;/p&gt;
&lt;h1 id=&#34;bayesian-methods-and-economics&#34;&gt;Bayesian methods and economics&lt;/h1&gt;
&lt;p&gt;I&amp;rsquo;m going to talk about how I think Bayesian methods are being used currently in financial economics, why I think Bayesian methods should be used more in empirical economics. I also want to pitch Turing.jl as a way for researchers to do more of this, if only because it is very easy to do so.&lt;/p&gt;
&lt;p&gt;I mentioned before that economics does not have ground truth. There will never be a point when a researcher can be confident that their model is 100% correct, or that their parameter estiamtes are accurate. It&amp;rsquo;s just not possible &amp;ndash; economics is the science of choices by people. People are made up of angry goop and they can behave irrationally at times, so a deterministic model is pretty hard to specify.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why economists use standard errors, and think so hard about whether their model is free from material omitted bias, heteroskedasticity, etc. Standard errors in OLS (or whatever your method is) give you a good proxy for the variance of your estimator, assuming that estimator is normal.&lt;/p&gt;
&lt;p&gt;Economists have many ways to think about standard errors and causal inference &amp;ndash; do your errors have some kind of autocorrelation? What if clusters of observation share some common error? Does the instrument you are using satisfy the necessary requirements? These kinds of questions are where economics shines the brightest. Because there is no ground truth, you want to be as confident as you can when you say something.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-methods&#34;&gt;Bayesian methods&lt;/h2&gt;
&lt;p&gt;What does any of this have to do with Bayesian methods? Well, my biggest issue with contemporary econometrics is the use of priors. Every single time someone runs a regression with &lt;code&gt;lm(y ~ x, data)&lt;/code&gt; or &lt;code&gt;reg y x&lt;/code&gt;, they are doing a very specific thing. OLS is simply the &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&#34;&gt;maximum a posteriori&lt;/a&gt; estimate of the model&amp;rsquo;s parameters with a flat prior everywhere, also called &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;maximum likelihood&lt;/a&gt;. By doing this, you let the data speak for you, which I am generally in favor of.&lt;/p&gt;
&lt;p&gt;But sometimes priors matter! When you have small datasets or multiple posterior  modes, sometimes priors can get your estimates to where you think is reasonable (conditional on a good prior).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not like economists have a shortage of priors, either. Good papers are either backed by good theory or show intuitive relationships that don&amp;rsquo;t need a formal theoretical link, and in all cases you can typically say something like&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If the relationships in Person (2030) hold, then $\alpha &amp;gt; 1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sounds like a prior to me. You can use theoretical predictions to motivate priors when you&amp;rsquo;re writing models.&lt;/p&gt;
&lt;h2 id=&#34;the-state-of-bayesian-methods-in-finance&#34;&gt;The state of Bayesian methods in finance&lt;/h2&gt;
&lt;p&gt;My perception is that Bayesian methods are still somewhat fringe, but that they have a slight but regular appearance in finance. I went to our top journal, the &lt;a href=&#34;https://afajof.org/&#34;&gt;Journal of Finance&lt;/a&gt;, and searched for the word &amp;ldquo;bayesian&amp;rdquo;. I grabbed any of the papers that are not pure theory. Here&amp;rsquo;s a list of papers that turned up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cavagnaro et al. (2019). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12783&#34;&gt;Measuring Institutional Investors’ Skill at Making Private Equity Investments&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pástor (2000). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00204&#34;&gt;Portfolio Selection and Asset Pricing Models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pástor and Stambaugh (1999). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00099&#34;&gt;Costs of Equity Capital and Model Mispricing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Johannes, Lochstoer, and Mou (2016). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12246&#34;&gt;Learning About Consumption Dynamics&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Lamoureux and Witte (2002). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/1540-6261.00467&#34;&gt;Empirical Analysis of the Yield Curve: The Information in the Data Viewed through the Window of Cox, Ingersoll, and Ross&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Kandel and Stambaugh (1996). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.1996.tb02689.x&#34;&gt;On the Predictability of Stock Returns: An Asset‐Allocation Perspective&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Barillas and Shanken (2018). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12607&#34;&gt;Comparing Asset Pricing Models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Rouwenhorst (1999). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00151&#34;&gt;Local Return Factors and Turnover in Emerging Stock Markets&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Brav (2000). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00279&#34;&gt;Inference in Long‐Horizon Event Studies: A Bayesian Approach with Application to Initial Public Offerings&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Baks, Metrick, and Wachter (2001). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00319&#34;&gt;Should Investors Avoid All Actively Managed Mutual Funds? A Study in Bayesian Performance Evaluation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Busse and Irvine (2006). &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.2006.01057.x&#34;&gt;Bayesian Alphas and Mutual Fund Persistence&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these papers use explicitly derived analytic forms, explicit Gibbs conditionals, or very basic MCMC models. Very few of these models are non-linear models, and in most cases they tend to be regular frequentist econometrics with the addition of a density function.&lt;/p&gt;
&lt;h1 id=&#34;whats-cool&#34;&gt;What&amp;rsquo;s cool&lt;/h1&gt;
&lt;p&gt;My favorite papers apply Bayesian methods in a more interesting way. One example is &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12607&#34;&gt;Barillas and Shanken (2018)&lt;/a&gt;, who use a closed form solution to analyze the efficacy of various factor models. I like this paper quite a lot, but I think that researchers tend to work really hard to derive closed form solutions when they are not really ncessary. For example, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12854&#34;&gt;Chib, Zeng, and Zhao (2020)&lt;/a&gt; attempted to replicate Barillas and Shanken, and noticed that the use of a Jeffrey&amp;rsquo;s prior on nuisance parameters makes the closed form solution unsound.&lt;/p&gt;
&lt;p&gt;You can avoid this by just numerically solving your model. I believe that we should start thinking more and more computationally as our models become more complex, and Markov Chain Monte Carlo lets you do this. Importantly, this is easier now that ever. It&amp;rsquo;s not 2002 anymore and you don&amp;rsquo;t have to roll your own Gibbs sampler every time you need to solve some model. You can just use a probabilistic programming language like Turing!&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;book&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;book&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;
Some other good PPLs are &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;, &lt;a href=&#34;https://github.com/cscherrer/Soss.jl&#34;&gt;Soss.jl&lt;/a&gt;, &lt;a href=&#34;https://docs.pymc.io/&#34;&gt;PyMC&lt;/a&gt;, and &lt;a href=&#34;http://pyro.ai/&#34;&gt;Pyro&lt;/a&gt;, among many others.
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t get me wrong &amp;ndash; I love theory as much as the next person. Theory is good for telling stories, whereas empirics are good for proving those stories. Or theory is getting more and more complex, and our empirics should rise to meet the challenge. Additionally, I think where Bayesian methods are concerned, people try to mix theory and empirics too closely, and they end up looking for closed form solutions where there are none.&lt;/p&gt;
&lt;p&gt;I want to present a rough sketch of how I think about this, and how I&amp;rsquo;d do it computationally. Assume that there are $N$ factor models, each of which returns an expected return from a function &lt;code&gt;r(m, t, x)&lt;/code&gt; for factor model index &lt;code&gt;m&lt;/code&gt;, time &lt;code&gt;t&lt;/code&gt;, and observable data &lt;code&gt;x&lt;/code&gt;. Assume &lt;code&gt;x&lt;/code&gt; is a matrix of returns and factors for one security. One nice way to do this in Turing is&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Import Turing&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Turing

&lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Declare our probabilistic model.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# - models is a vector of functions, r(n, t, x), that return an expected return.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# - data is a matrix with returns in the first column, and factors in the remaining columns.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# &lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;@model&lt;/span&gt; factors(models, data) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# Choose which model is &amp;#34;true&amp;#34;, all models have equal priors.&lt;/span&gt;
    m &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; Categorical(length(models))

    &lt;span style=&#34;color:#75715e&#34;&gt;# Draw a variance parameter.&lt;/span&gt;
    σ &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; InverseGamma(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Estimate each return.&lt;/span&gt;
    r_hat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(m, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;size(data, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), data)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Check the model&amp;#39;s predictions:&lt;/span&gt;
    data[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; MvNormal(r_hat, σ)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And you&amp;rsquo;re done! You can run this on whatever sampling method you want, and it&amp;rsquo;ll give you posterior probabilities for models. As valuable as raw math is, sometimes it&amp;rsquo;s nice to just type the model up and see what the data says, assuming you&amp;rsquo;ve thought about all the econometric issues as you normally would.&lt;/p&gt;
&lt;h1 id=&#34;hopes&#34;&gt;Hopes&lt;/h1&gt;
&lt;p&gt;In this section, I want to talk about some things I want to see more of going forward.&lt;/p&gt;
&lt;h2 id=&#34;structural-estimation&#34;&gt;Structural estimation&lt;/h2&gt;
&lt;p&gt;Structural estimation is a really beautiful tool. When you structurally estimate something, you marry theory and empirics to determine the effect of some parameter. For the most part, it is done in a frequentist way by matching moments between simulated data and empirical data. You can do structural estimation in a Bayesian way by specifying a very general probabilistic model and then running it through your PPL of choice. Not only does this give you parameter estimates, but it also tells you how uncertain you are of those estimates. You might even learn that your parameterizations are multimodal, and that there are numerous nontrivial outcomes in your model that a simulated method of moments estimation might miss.&lt;/p&gt;
&lt;h2 id=&#34;prior-sensitivity&#34;&gt;Prior sensitivity&lt;/h2&gt;
&lt;p&gt;Bayesian methods let you test how realistic something is. I read a cool working paper a little while ago called &lt;a href=&#34;https://sites.google.com/site/lalochstoer/VolUnderreactionMain.pdf?attredirects=0&amp;amp;d=1&#34;&gt;Volatility Expectations and Returns&lt;/a&gt; by Lars Lochstoer and Tyler Muir. They propose a novel behavioral explanation of some weird patterns in the VIX, realized volatility, returns, and the variance risk premium. Essentially, if investors use too much of past variance to form their expectations about current variance, you can explain many strange effects in markets.&lt;/p&gt;
&lt;p&gt;The problem with behavioral papers is that they don&amp;rsquo;t quite site right with finance folks, because it&amp;rsquo;s very easy to say that some arbitrageur should have removed this anomaly. Rob Ready (here at the University of Oregon) asked how strong your priors would have to be on using old observations of variance for this effect to matter, and we can test this! Build a model of stoachastic volatility and conditional expectations, and you should be able to fiddle with your model priors until something cool comes out.&lt;/p&gt;
&lt;h2 id=&#34;latent-variables&#34;&gt;Latent variables&lt;/h2&gt;
&lt;p&gt;Bayesian methods are interesting when you apply them to inferring latent variables. In finance, these might be things like managerial skill, expected return, volatility, etc. We&amp;rsquo;ve got all kinds of things we don&amp;rsquo;t directly observe but have models to explain how they interact when other stuff. When you know that X goes up when Y does, you can start to run inference on the relationships between X and Y even when you can&amp;rsquo;t observe Y, though as always, it helps to have lots of data.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This was a bit of a rambling post, but I&amp;rsquo;m trying to get some thoughts on paper. What a time to be alive.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I do have industry experience, but it&amp;rsquo;s not a perfect substitute. It helps a lot to have thought about all the little fiddly bits that go into Turing.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Do physics people use &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34;&gt;instrumental variables&lt;/a&gt;?&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Trade Volume and Information</title>
      <link>/2019/07/28/trade-volume-and-information/</link>
      <pubDate>Sun, 28 Jul 2019 09:21:25 -0700</pubDate>
      
      <guid>/2019/07/28/trade-volume-and-information/</guid>
      <description>&lt;p&gt;Rob Ready here at the University of Oregon recommended that I read John Cochrane&amp;rsquo;s blog post on &lt;a href=&#34;https://johnhcochrane.blogspot.com/2016/10/volume-and-information.html&#34;&gt;why anyone actually trades&lt;/a&gt;. I had not read it before, and finally had some time to do so after my full-bodied sprint to Baltimore and back.&lt;/p&gt;
&lt;p&gt;The post helped me gel an idea of information-based market activity that I started to think about in Switzerland but was unable to formalize. Cochrane points to a lot of mechanisms that people have used to induce trading, such as preference shocks, noise traders, or overconfidence. Obviously some of these are more reasonable than others.  I have met professional traders from big institutions and they certainly seem to be a little overconfident.&lt;/p&gt;
&lt;p&gt;But Cochrane dances near a concept that would explain much of the trading behavior we observe:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We know what this huge volume of trading is about. It’s about
information, not preference shocks. Information seems to need trades to
percolate into prices. We just don’t understand why.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Information is, in my view, the only real reason anyone should trade, but I think my formulation of &amp;ldquo;information&amp;rdquo; is significantly broader than many others. Noise traders can be explained by the information they posess &amp;ndash; a client has just asked for cash, so the noise trader must sell some of the client&amp;rsquo;s assets. This is fundamentally an information  story!&lt;/p&gt;
&lt;p&gt;It is also something I would consider private information. The firm knows something that most other firms do not, which is that they need to liquidate the asset even though their absolute perspective on the value of the asset is unchanged. Only their relative valuation has changed. The firm&amp;rsquo;s value from holding the asset has dramatically increased, because they face consequences for &lt;em&gt;not&lt;/em&gt; selling the asset and returning the funds.&lt;/p&gt;
&lt;p&gt;The perception of value is also an important concept, which in my view is exclusively driven by information. Two firms, a hedge fund and a pension fund, both have sets of information about all securities available. A piece of information might be that Tesla is actually a company, that it is trading at $228.04 right now, or that the pension fund manager owns a Tesla and quite likes the product. It might be that the hedge fund manager saw Elon Musk pick his nose at an In-N-Out twenty years ago, and believes that Elon Musk has limited self control. These are all pieces of information with different levels of usefulness, but collectively, they allow market agents to construct an estimate of an efficient price. If both firms have exactly the same information sets, we should never expect trades. Their perception of absolute value is given only by all the little tidbits they have, and having the same tidbits mean that perceptions should be identical.&lt;/p&gt;
&lt;p&gt;We can throw some math on here to make this more specific. Consider two firms, $x$ and $y$, each with access to a subset of some global information set $I$, given by $I_x$ and $I_y$ respectively.&lt;/p&gt;
&lt;p&gt;Assume that each firm determines the fundamental value of a security $X$ using a function $f(\cdot)$, which is a deterministic function shared by both firms. Let $f(I) = X^*$ be the true fundamental value of the asset. If $I_x = I_y$, then $f(I_x) = f(I_y)$. We cannot reallly say much in general about cases where $I_x \subset I_y$, since it&amp;rsquo;s possible that the additional information held by $y$ increases or decreases their understanding of fundamental value.&lt;/p&gt;
&lt;p&gt;The problem with this set construction is that sets are often very difficult to work with. What&amp;rsquo;s the actual value of having an additional element of information? Well, ultimately, it reduces your level of uncertainty about how much of $I$ you actually observe!&lt;/p&gt;
&lt;p&gt;Being good Bayesians, both firms $x$ and $y$ know that they only have some subset of the true information set. Suppose their valuations are given by distributions $G_x(I_x)$ and $G_y(I_y)$, with $G(I) = X^*$. Neither of these distributions actually have to be centered around $X^*$. They can be lumpy, multimodal, whatever &amp;ndash; all we care about is the fact that the variance of your beliefs is much higher if you have less information.&lt;/p&gt;
&lt;p&gt;Returning to our statements on $f(\cdot)$, what can we say about the case where $I_x \subset I_y$? Well, only that $\text{var}(G_x) &amp;lt;\text{var}(G_y)$. That is, acquiring new information reduces your uncertainty about what you know and what you know you don&amp;rsquo;t know.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to leave this here for a moment because I have some work to do, but here are some next steps when I get around to it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Link this set construction to market prices.&lt;/li&gt;
&lt;li&gt;By what mechanism exactly do trades allows others to make inferences about the information sets of others?&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s a dynamic programming extension look like? I.e., if some agents are generally better at acquiring information than others, how might other agents form expectations about their expectations over time?&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>JuliaCon</title>
      <link>/2019/07/26/juliacon/</link>
      <pubDate>Fri, 26 Jul 2019 04:11:59 -0700</pubDate>
      
      <guid>/2019/07/26/juliacon/</guid>
      <description>&lt;p&gt;Yesterday I gave a talk on Turing.jl, and I think I only made 5-9 mistakes. You can check it out on YouTube:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Jr6HcyHK_Q4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;There were also some spectacular talks by the other probabilitistic programming languages in Julia.
Chad Scherrer (who is pretty great)  covered &lt;a href=&#34;https://www.youtube.com/watch?v=H-Oof2wS-0A&#34;&gt;Soss.jl&lt;/a&gt;,
which has a pretty slick syntax and what I would consider an admirable user interface. Gen.jl had a
&lt;a href=&#34;https://www.youtube.com/watch?v=B7mc1wXPZR8&#34;&gt;remarkable presentation&lt;/a&gt; from Alex Lew
(who is also pretty great) that has a really fascinating perspective on inference. Will Tebbut&amp;rsquo;s
talk on Gaussian processes was really nice, considering I hand&amp;rsquo;t looked to closely at his
&lt;a href=&#34;https://github.com/willtebbutt/Stheno.jl&#34;&gt;Stheno.jl&lt;/a&gt; project before. I also learned about
GPs! Cool stuff. Lastly we had a talk on &lt;a href=&#34;https://www.youtube.com/watch?v=IE39JoVIaEw&#34;&gt;switching Kalman filters&lt;/a&gt;
from Cédric St-Jean-Leblanc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 4</title>
      <link>/2019/06/27/switzerland-day-4/</link>
      <pubDate>Thu, 27 Jun 2019 19:47:24 +0200</pubDate>
      
      <guid>/2019/06/27/switzerland-day-4/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;book&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;book&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; See &lt;a href=&#34;https://www.amazon.com/Empirical-Market-Microstructure-Institutions-Econometrics/dp/0195301641&#34;&gt;here&lt;/a&gt; for a link to purchase Joel&amp;rsquo;s book. It&amp;rsquo;s a good one. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Today was the first day of the empirical session with &lt;a href=&#34;https://albertjmenkveld.com/&#34;&gt;Albert Menkveld&lt;/a&gt; . We covered the first couple chapters of Joel Hasbrouck&amp;rsquo;s excellent book.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;roll&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;roll&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Roll, Richard, 1984, A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market, &lt;em&gt;The Journal of Finance&lt;/em&gt; 39, 1127–1139. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We started with the general Roll (1984) model, which is a really straightforward way to think about how order processing costs make it into the bid-ask spread. Basically, it is assumed that trade prices $p^t$ have a random walk with drift evolution, such that&lt;/p&gt;
&lt;p&gt;$$
p_t = p_{t-1} + \mu + u_t
$$&lt;/p&gt;
&lt;p&gt;Hasbrouck notes that the drift term $\mu$ is largely unnessecary, especially since at micro-scale it&amp;rsquo;s hard to have any notion of expected return.&lt;/p&gt;
&lt;p&gt;The model above is expanded upon by including an efficient price (fundamental value) $m_t$, which is a martingale:&lt;/p&gt;
&lt;p&gt;$$
m_t = m_{t-1} + u_t
$$&lt;/p&gt;
&lt;p&gt;Prices are then a noisy proxy of the true value, as a function of a cost that market makers need to recoup for processing orders&lt;/p&gt;
&lt;p&gt;$$
p_t = m_t + q_tc
$$&lt;/p&gt;
&lt;p&gt;where $c$ is a fixed per-trade cost incurred by the dealer and $q_t$ is an indicator for a buy or sell ($+1$ for a buy and $-1$ for a sell).&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;hasbrouck-2009 &#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;hasbrouck-2009 &#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Hasbrouck, Joel, 2009, Trading Costs and Returns for U.S. Equities: Estimating Effective Costs from Daily Data, &lt;em&gt;The Journal of Finance&lt;/em&gt; 64, 1445–1477. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a good model. I&amp;rsquo;m particularly interested in how Hasbrouck (2009) approaches trying to approximate the $c$ variable, as he uses a Gibbs sampler to run a Bayesian linear regression. Given my association with &lt;a href=&#34;https://turing.ml&#34;&gt;Turing.jl&lt;/a&gt;, I can&amp;rsquo;t help but feel that there is a hierarchical model that would provide a better structural estimate of things like adverse selection cost and order processing cost. That&amp;rsquo;d need a more sophisticated model than the Roll model, however, and I&amp;rsquo;m not quite sure I&amp;rsquo;m up to the task (yet).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 3</title>
      <link>/2019/06/26/switzerland-day-3/</link>
      <pubDate>Wed, 26 Jun 2019 17:46:48 +0200</pubDate>
      
      <guid>/2019/06/26/switzerland-day-3/</guid>
      <description>&lt;p&gt;Not much news today. We had the morning off, presumably so that some students could work on their presentations. The afternoon included five presentations, generally covering&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A theoretical examination of the impacts of financial transaction taxes&lt;/li&gt;
&lt;li&gt;The effect of speed bumps on market liquidity&lt;/li&gt;
&lt;li&gt;The asymmetric returns volatility effect (theory and empirics)&lt;/li&gt;
&lt;li&gt;Determinants of market fragmentation&lt;/li&gt;
&lt;li&gt;The effects of primary issuance on secondary market liquidity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The presentation quality was pretty excellent, overall.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 2</title>
      <link>/2019/06/25/switzerland-day-2/</link>
      <pubDate>Tue, 25 Jun 2019 18:38:51 +0200</pubDate>
      
      <guid>/2019/06/25/switzerland-day-2/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;glosten&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;glosten&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Glosten, Lawrence R., 1994, Is the Electronic Open Limit Order Book Inevitable?, &lt;em&gt;The Journal of Finance&lt;/em&gt; 49, 1127–1161. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;miller&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;miller&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Vayanos, Dimitri, 1999, Strategic Trading and Welfare in a Dynamic Market, &lt;em&gt;Review of Economic Studies&lt;/em&gt; 66, 219–254. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;miller&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;miller&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Grossman, Sanford J., and Merton H. Miller, 1988, Liquidity and Market Structure, &lt;em&gt;The Journal of Finance&lt;/em&gt; 43, 617–633. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Today was the final lecture series on microstructural theory. We covered models of the limit order book, primarily Glosten (1994), and a model of inventory holding costs and imperfect competition among dealers. The dealer competition model was based on Vayanos (1999) and (I think) Grossman and Miller (1988).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m generally quite pleased with my level of understanding of both microeconomics and microstructure. I didn&amp;rsquo;t feel lost or unable to grasp the model&amp;rsquo;s steps at any point &amp;mdash; several times I was able to infer potential next steps, and I suspect one time I may even have been right. Many models fall out of very general equilibrium conditions and simple first order conditions, so it&amp;rsquo;s fairly straightforward to adapt the general microeconomics tools.&lt;/p&gt;
&lt;h2 id=&#34;glosten-1994&#34;&gt;Glosten (1994)&lt;/h2&gt;
&lt;p&gt;I think that the most complex model we reviewed is that of the Glosten (1994) paper. It&amp;rsquo;s a three-period model, which is generally fairly simple. In this case, the complexity comes from the mental jumps you must make to find the intuition in how expectations are form. The model has some very interesting predictions on how people decide to place order in limit order books based on execution belief and adverse selection risk. Orders accumulate at various levels on the price grid, such that the price at any particular point ($A^k$ for the $k^{th}$ ask or $B^k$ for the $k^{th}$ bid) satisfies&lt;/p&gt;
&lt;p&gt;$$
A_k = E[V \space \vert \space q \ge Y_k^*]
$$&lt;/p&gt;
&lt;p&gt;where $V$ is the valuation of some asset, and $q$ is the size of a market order that arrives in the second period. The term $Y^*_k$ represents the cumulative quantity available up to price $A_k$ (or $B_k$).&lt;/p&gt;
&lt;p&gt;This equilibrium condition comes out of each trader&amp;rsquo;s decisions. A trader looks at each price level, and decides whether adding to the quantity at that price level is likely to be profitable.  If it is not, because the adverse selection risk is too high or there are too many orders on the book already, the trader goes to the next price level and repeats the process.&lt;/p&gt;
&lt;p&gt;A couple of interesting points arise from this particular model. First, the Glosten (1994) model explains why there might be unfilled price levels in the order book. At any given price level, the expectation is formed based on &lt;em&gt;cumulative&lt;/em&gt; quantity on offer. If the cumulative quantity at price levels below satisfies the condition $q \ge Y_k^*$ for several levels of $k$, then those levels will be unfilled. Second, this model predicts that informed traders never submit small orders, but I think this is largely a byproduct of the fact that the model does not consider strategic optimization. Thierry noted that this is common to many models, and that it makes since, considering the complexity involved in optimizing the behavior of multiple agent types.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Tomorrow, five students will be presenting their papers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Switzerland: Day 1</title>
      <link>/2019/06/24/switzerland-day-1/</link>
      <pubDate>Mon, 24 Jun 2019 18:09:08 +0200</pubDate>
      
      <guid>/2019/06/24/switzerland-day-1/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;lugano&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;lugano&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt;A picture of Lugano from some balcony somewhere.&lt;img src=&#34;/images/lugano.png&#34; alt=&#34;&#34;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Today was the first day of the market &lt;a href=&#34;https://microstructure-course.com/&#34;&gt;microstructure summer school&lt;/a&gt;, where &lt;a href=&#34;https://albertjmenkveld.com/&#34;&gt;Albert Menkveld&lt;/a&gt; and &lt;a href=&#34;https://thierryfoucault.com/&#34;&gt;Thierry Foucault&lt;/a&gt; teach the theory and empirics of market microstructure. It&amp;rsquo;s held in Lugano, Switzerland, a large city in the Italian-speaking canton of Ticino.&lt;/p&gt;
&lt;p&gt;Thierry began the week with theory&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, primarily on determinants of the bid-ask spread, as well as how asymmetric information is applied in many of the classic models. Much of the conversation was based on a simplified model of Biais, Foucault, and Moinas (2015).&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;asymmetrypaper&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;asymmetrypaper&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Biais, Bruno, Thierry Foucault, and Sophie Moinas, 2015, Equilibrium fast trading, &lt;em&gt;Journal of Financial Economics&lt;/em&gt; 116, 292–313. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model is largely a way of examining how exactly it is that informed trades impact the spread by examining the paremeterizations that define a market. The comparative statics suggest that spreads are increasing in the&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Proportion of fast traders (informed)&lt;/li&gt;
&lt;li&gt;Degree of market fragmentation&lt;/li&gt;
&lt;li&gt;Private valuation volatility&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So far it&amp;rsquo;s been pleasant. Looking forward to tomorrow&amp;rsquo;s lecture.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I am often made fun of for saying the phrase &amp;ldquo;theory models&amp;rdquo;. I recognize this is probably not right, but there has been so much mockery that I must now stick to my terminology.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>AFA Webcasts</title>
      <link>/2019/01/08/afa-webcasts/</link>
      <pubDate>Tue, 08 Jan 2019 08:08:25 -0800</pubDate>
      
      <guid>/2019/01/08/afa-webcasts/</guid>
      <description>&lt;p&gt;From Susan Athey&amp;rsquo;s &lt;a href=&#34;https://www.aeaweb.org/webcasts/2019/aea-afa-joint-luncheon-impact-of-machine-learning&#34;&gt;luncheon address&lt;/a&gt; on machine learning to the AFA:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; and so we have to then develop on top of that best practices for analyzing a black box algorithm. And there&amp;rsquo;s not really like a science and convention on that. If somebody writes a difference-in-difference paper, there&amp;rsquo;s ten things you should do &amp;ndash; There are best practices, and we know how to evaluate whether you should believe the results or not. But we don&amp;rsquo;t have that analogous set of best practices and conventions around these black boxes yet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Susan Athey&amp;rsquo;s whole address was pretty spectacular, so I&amp;rsquo;d set aside some time to watch it if you have a spare error.&lt;/p&gt;
&lt;p&gt;A large portion of the talk was dedicated to this idea that economists and financial academics writ large need to be better at re-purposing advances in machine learning towards our own ends, and that we need to start thinking about providing frameworks on how to think about the interpretation of models that do not come from traditional econometrics.&lt;/p&gt;
&lt;p&gt;I was really delighted to see this kind of thinking. There&amp;rsquo;s a growing number of very prominent economists (Mullainathan had a &lt;a href=&#34;https://www.youtube.com/watch?v=xl3yQBhI6vY&#34;&gt;similar address&lt;/a&gt; in 2017) who are starting to think critically about how and when these tools should be used.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018 Retrospective</title>
      <link>/2019/01/01/2018-retrospective/</link>
      <pubDate>Tue, 01 Jan 2019 12:31:22 -0800</pubDate>
      
      <guid>/2019/01/01/2018-retrospective/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s a new year, and I&amp;rsquo;ve gathered some notes from the year gone by.&lt;/p&gt;
&lt;h1 id=&#34;achievements&#34;&gt;Achievements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Became a C# software developer.&lt;/li&gt;
&lt;li&gt;Went to Miami.&lt;/li&gt;
&lt;li&gt;Passed CFA Level One.&lt;/li&gt;
&lt;li&gt;Learned &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;, got better at &lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt;, introduced myself to &lt;a href=&#34;https://ocaml.org/&#34;&gt;OCaml&lt;/a&gt;, &lt;a href=&#34;https://www.haskell.org/&#34;&gt;Haskell&lt;/a&gt;, and &lt;a href=&#34;https://chapel-lang.org/&#34;&gt;Chapel&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Started a &lt;a href=&#34;https://business.uoregon.edu/faculty/cameron-pfiffer&#34;&gt;PhD&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Visited my grandmother in upstate New York with my brother.&lt;/li&gt;
&lt;li&gt;Built and designed a tool to detect outlier performance in investment activities.&lt;/li&gt;
&lt;li&gt;Endured a lot of Southern Oregon&amp;rsquo;s wildfire smoke.&lt;/li&gt;
&lt;li&gt;Moved from Medford, OR to Eugene, OR.&lt;/li&gt;
&lt;li&gt;Finally got a piano in the house again. It is super out of tune so I&amp;rsquo;ve mostly been playing honky tonk blues and stuff.&lt;/li&gt;
&lt;li&gt;Started working on &lt;a href=&#34;http://turing.ml/&#34;&gt;Turing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Drilled through a bunch of 220v electrical lines.&lt;/li&gt;
&lt;li&gt;Attended the Pacific Northwest Finance Conference in Seattle.&lt;/li&gt;
&lt;li&gt;Had my first ever office hours.&lt;/li&gt;
&lt;li&gt;Rowed 5km in 23:03.&lt;/li&gt;
&lt;li&gt;Deadlifted 220lbs.&lt;/li&gt;
&lt;li&gt;Made it through the first academic term with some pretty good grades. Courses taken so far are:
&lt;ul&gt;
&lt;li&gt;Math Camp&lt;/li&gt;
&lt;li&gt;Core Microeconomics 1 (Consumer theory, mostly)&lt;/li&gt;
&lt;li&gt;Econometrics 1 (Mathematical statistics)&lt;/li&gt;
&lt;li&gt;Accounting Theory and Disclosure (disclosure theory, what accounting does, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Not insane yet.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;goals&#34;&gt;Goals&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Learn more about market microstructure. It&amp;rsquo;s a fascinating field and nobody is going to tell me everything I want to know, so I have to make an effort to do so myself.&lt;/li&gt;
&lt;li&gt;Keep up my programming skills. Don&amp;rsquo;t get stale.&lt;/li&gt;
&lt;li&gt;Really get better at understanding the nuance of MCMC methods. There&amp;rsquo;s a lot of beauty there that needs to be unpacked.&lt;/li&gt;
&lt;li&gt;Row a 5km in 22:30.&lt;/li&gt;
&lt;li&gt;Save up enough to purchase a desktop computer.&lt;/li&gt;
&lt;li&gt;Be better about remembering stuff.&lt;/li&gt;
&lt;li&gt;Write at least one post a month.&lt;/li&gt;
&lt;li&gt;Read academic papers more deliberately. Keep better summaries and focus on what matters.&lt;/li&gt;
&lt;li&gt;Broaden my repertoire of left-hand licks (grooves? base lines? I have no idea) to include things requiring greater hand independence.&lt;/li&gt;
&lt;li&gt;Do great in all my coursework.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;bullets&#34;&gt;Bullets&lt;/h1&gt;
&lt;p&gt;I do a type of pseudo-bullet journaling, where I write down a couple of points from various days. Here are some of my favorites. I tend to swear a lot on in my journal, so I have censored them in accordance with the fact that I&amp;rsquo;m sort of a professional adult.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;January 11th - &amp;ldquo;C# isn&amp;rsquo;t a terrible language.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 14th - &amp;ldquo;Had brownies for lunch and breakfast.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 15th - &amp;ldquo;steroid injections hurt&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 21st - &amp;ldquo;Flying is stupid and I hate it.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 23rd - &amp;ldquo;Passed CFA level one, which means I have to keep going. ****.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 26th - &amp;ldquo;Being paid money is nice.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;January 28th - &amp;ldquo;I want to make a cryptocurrency exchange where the rules change every day. That&amp;rsquo;d be fun.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;February 3rd - &amp;ldquo;Liz made some bomb-ass chili for dinner.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;February 17th - &amp;ldquo;Tried to refinance the car, failed dramatically.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;February 20th - &amp;ldquo;Bricked my Linux partition after trying to remove Python 2. Time for bed.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;March 8th - &amp;ldquo;Seems nobody has tried to break into the house yet.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;March 20th - &amp;ldquo;Another day, another step towards an inevitable death.&amp;rdquo; Hilariously, this is immediately followed with &amp;ldquo;Nothing interesting happened today.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;March 27th - &amp;ldquo;Worked on code all day. It was nice.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;May 24th - &amp;ldquo;Made Assist reading time go from 13 minutes to 30 seconds.&amp;rdquo; The assist is a giant CSV parser we used at work, for background on this.&lt;/li&gt;
&lt;li&gt;June 10th - &amp;ldquo;Filled the pond with vinegar.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;July 12th - &amp;ldquo;Biked in $104^∘$ weather. It was OK.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;August 10th - &amp;ldquo;Made fancy mac + cheese.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;August 24th - &amp;ldquo;I am officially unemployed.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;September 11th - &amp;ldquo;I hate bus people. I should take my bike more.&amp;rdquo; This didn&amp;rsquo;t happen much. I live 12 miles one-way from school, and it&amp;rsquo;s about two hours of biking a day to commute that way. I have done this kind of thing in the past and it is not great. Good way to lose 80 pounds, though.&lt;/li&gt;
&lt;li&gt;September 14th - &amp;ldquo;I made lava cake. Lava cake is &lt;em&gt;dope&lt;/em&gt;.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;September 27th - &amp;ldquo;While putting up a pot rack, we hit an electrical line.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;October 11th - &amp;ldquo;Drank beer and worked on data structures.&amp;rdquo; I am unclear about what this means.&lt;/li&gt;
&lt;li&gt;November 18th - &amp;ldquo;Drilled through &lt;strong&gt;another&lt;/strong&gt; electrical cable in the wall, this time for the dryer. **** everything.&amp;rdquo; Hilariously, this was a second attempt to install the pot rack introduced on September 27th.&lt;/li&gt;
&lt;li&gt;December 9th - &amp;ldquo;The mice in the ceiling continue to be spectacularly annoying.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Stock Prediction With News</title>
      <link>/2018/12/31/stock-prediction-with-news/</link>
      <pubDate>Mon, 31 Dec 2018 06:15:40 -0800</pubDate>
      
      <guid>/2018/12/31/stock-prediction-with-news/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;source&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Sardelich, Marcelo, and Suresh Manandhar, 2018, Multimodal deep learning for short-term stock volatility prediction, &lt;em&gt;arXiv:1812.10479 [cs, q-fin, stat]&lt;/em&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m always interested in seeing more people apply natural language processing to financial statements and headlines. A lot of the accounting literature where those kinds of tools are most likely to be useful uses very primitive techniques for textual analysis.&lt;/p&gt;
&lt;p&gt;So I was delighted to read this paper by Marcelo and Manandhar &lt;a href=&#34;https://arxiv.org/abs/1812.10479v1&#34;&gt;on arXiv&lt;/a&gt; where the authors gather headlines and attempt volatility forecasting. By their measure, they do pretty well, better than $\text{GARCH}(1,1)$. I don&amp;rsquo;t know enough about GARCH to make an educated assessment on that, but their MSE and $R^2$ is pretty remarkable for financial time series.&lt;/p&gt;
&lt;p&gt;The authors segmented their predictions by sector, and I noticed that the $R^2$ for the energy sector is substantially higher than the other sectors (~0.4 vs. ~0.2) for both GARCH and their model. I have to wonder why this is the case. Perhaps the energy sector is simply more responsive to news? I suppose that makes sense when you think about how dramatically oil prices change in response to almost every event.&lt;/p&gt;
&lt;p&gt;Regardless, this was an interesting paper with a surprising level of financial knowledge and a very interesting ML model (word embeddings, sentence encoders, LTSM networks, oh my) and it&amp;rsquo;s worth a read.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing Market Marking</title>
      <link>/2018/12/27/optimizing-market-marking/</link>
      <pubDate>Thu, 27 Dec 2018 16:48:26 -0800</pubDate>
      
      <guid>/2018/12/27/optimizing-market-marking/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;source&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Patel, Yagna, 2018, Optimizing Market Making using Multi-Agent Reinforcement Learning, &lt;em&gt;arXiv:1812.10252 [cs, q-fin, stat]&lt;/em&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Apparently I&amp;rsquo;ve been on a kick reading some of these reinforcement learning/market making papers. Yagna Patel published an interesting paper on arXiv&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; discussing the application of a reinforcement learning agent to market making, one of my favorite topics. Market making, that is, not necessarily reinforcement learning.&lt;/p&gt;
&lt;h1 id=&#34;why&#34;&gt;Why&lt;/h1&gt;
&lt;p&gt;As Patel points out, there&amp;rsquo;s three big concerns that users of machine learning in finance have to watch out for.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Time. Significantly complex models take a while to make predictions.&lt;/li&gt;
&lt;li&gt;Accuracy. Predictive accuracy in financial markets can be low.&lt;/li&gt;
&lt;li&gt;Policy. Even if you have a model that shows a 55% chance of a uptick in price, how do you define policy to act on that information? More importantly, how do you define a policy that adapts to a changing fitness landscape?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The overall goal here is to determine whether reinforcement learning is useful in market making.&lt;/p&gt;
&lt;h1 id=&#34;what&#34;&gt;What&lt;/h1&gt;
&lt;p&gt;From the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this paper, reinforcement learning is applied to the problem of optimizing market making. A multi-agent reinforcement learning framework is used to optimally place limit orders that lead to successful trades. The framework consists of two agents. The macro-agent optimizes on making
the decision to buy, sell, or hold an asset. The micro-agent optimizes on placing limit orders within the limit order book. For the context of this paper, the proposed framework is applied and studied on the Bitcoin cryptocurrency market. The goal of this paper is to show that reinforcement learning is a viable strategy that can be applied to complex problems (with complex environments) such as market making.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Patel essentially presents a two-stage RL model for tackling trading, though he fails to actually approach market making methods.&lt;/p&gt;
&lt;h1 id=&#34;how&#34;&gt;How&lt;/h1&gt;
&lt;p&gt;Patel somewhat misses the point on what market makers do in how his model is set up. There are two components. The first is the macro agent, which decides whether to &lt;em&gt;buy&lt;/em&gt;, &lt;em&gt;sell&lt;/em&gt;, or &lt;em&gt;hold&lt;/em&gt;. The second component is a micro agent, which determines where to place the macro agent&amp;rsquo;s buy or sell order. The missed point is that market makers shouldn&amp;rsquo;t really be deciding whether to buy or sell; their goal is to place both buy and sell orders optimally within the order book, such that they minimize adverse selection and inventory risk. This macro agent design fails to model that behaviour, and already the paper has failed to answer the question as to whether RL can be applied to market making.&lt;/p&gt;
&lt;p&gt;Patel also chooses to use discrete-time modeling, for this reason:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As noted in the problem statement, discrete time-steps are chosen (rather than continuous time-steps) for the  reason that continuous time-steps would not be possible in the real world since the WebSocket data itself arrives at discrete time-steps.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don&amp;rsquo;t know that I buy that. Lots of microstructure folk (the crazy ones, anyway) use continuous time for this kind of thing, even though the WebSocket feeds are discrete. I suspect that it would be hard to model, so I can accept this.&lt;/p&gt;
&lt;p&gt;As this is a RL paper, the choice of reward function is somewhat important. The micro agent is evaluated against VWAP, which strikes me as odd. I&amp;rsquo;m not sure this framework of a macro and micro agent works that well, as the micro agent should be making choices based on probabilities of informed trading and such.&lt;/p&gt;
&lt;h1 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h1&gt;
&lt;p&gt;All in all, this is a really interesting paper, but it kind of misses the mark on market making. I&amp;rsquo;d like to see something closer to the finance literature on this.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Where none of the best finance papers go. This is something of a sorry state of affairs &amp;mdash; I really love the machine learning/microstructure papers that end up on arXiv, but many of the other papers are of an extremely low quality.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Selection mechanism design</title>
      <link>/2018/12/17/selection-mechanism-design/</link>
      <pubDate>Mon, 17 Dec 2018 07:28:24 -0800</pubDate>
      
      <guid>/2018/12/17/selection-mechanism-design/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;source&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Dewhurst, David Rushing, Michael Vincent Arnold, and Colin Michael Van Oort, 2018, Selection mechanism design affects volatility in a market of evolving zero-intelligence agents, arXiv:1812.05657 [cs, q-fin].&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.05657&#34;&gt;Here&amp;rsquo;s an interesting paper&lt;/a&gt; from Dewherst, Arnold, and Van Oort published to arXiv. Dewherst et al. use an evolving multi-agent landscape to model evolutionary selection mechanisms applicable to markets.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;amg&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;amg&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Lo, Andrew W, 2004, The Adaptive Markets Hypothesis, Journal of Portfolio Management, 15.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Choosing a representative selection mechanism is somewhat important if you follow Andrew Lo&amp;rsquo;s Adaptive Markets Hypothesis, where financial agents are weeded out in response to changing market conditions. Deherst et al. examine three selection mechanisms.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;fba&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;fba&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; The authors use frequent batch auctions instread of continuous dual auctions. I&amp;rsquo;m a big fan of frequent batch auctions, and you can read more about them in the &lt;a href=&#34;https://faculty.chicagobooth.edu/eric.budish/research/HFT-FrequentBatchAuctions.pdf&#34;&gt;Budish, Cramton and Shim (2015) paper&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first is a global quantile-based measure, where the bottom 10% of all agents are eliminated when a selection event occurs. The second is a localized variant, where agents in a subsample are kept in the environment according to the probability&lt;/p&gt;
&lt;p&gt;$$p_i(t)=\frac{π_i(t)}{∑{π_j}}$$&lt;/p&gt;
&lt;p&gt;where $π_i$ is the fitness of agent $i$. This can be understood as awarding higher retention probabilities to agents with a high share of the sampled agent&amp;rsquo;s fitness. The final metric uses the first metric with probability ½ and the second metric with probability ½.&lt;/p&gt;
&lt;p&gt;The agents have risk aversion, and are allowed to &amp;ldquo;innovate&amp;rdquo; by drawing their parameters from distributions unaffected by existing agents. If you don&amp;rsquo;t innovate, you draw your parameters from a distribution represented by agents who were not removed.&lt;/p&gt;
&lt;p&gt;The results show that the quantile metric creates agents with higher average profitability, but that the localized variant has a higher correlation between micro and macro volatility.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not quite sold at the end of the paper as to their selection mechanism, but I think it&amp;rsquo;s an interesting vein of research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MD&amp;A Paper Summary</title>
      <link>/2018/10/13/mda-paper-summary/</link>
      <pubDate>Sat, 13 Oct 2018 17:34:17 -0700</pubDate>
      
      <guid>/2018/10/13/mda-paper-summary/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This was written as a summary for an interesting paper I read for a class on whether or not the MD&amp;amp;A section of financial statements can be predictive of bankruptcy.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;paper-source&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;paper-source&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Mayew, William J., Mani Sethuraman, and Mohan Venkatachalam. &lt;em&gt;MD&amp;amp;A Disclosure and the Firm’s Ability to Continue as a Going Concern&lt;/em&gt;. The Accounting Review 90, no. 4 (July 2015): 1621–51. &lt;a href=&#34;https://doi.org/10.2308/accr-50983&#34;&gt;https://doi.org/10.2308/accr-50983&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mayew, Sethuraman and Venkatachalam (2015) present a study on the role that the textual properties of a firm&amp;rsquo;s MD&amp;amp;A section plays in predicting bankruptcy. The research topic came about primarily to inform the debate on whether or not the FASB should require management to assess if the firm is a going concern, though it is more interesting in general for the methodology by which they analyze the MD&amp;amp;A text.&lt;/p&gt;
&lt;p&gt;Prior to this paper, literature focused on small samples to determine whether MD&amp;amp;A was predictive of bankruptcy. Mayew et al. use a fairly substantial sample size to broadly describe the predictive capacity of MD&amp;amp;A disclosures. They include numerous other predictive factors, such as the auditor&amp;rsquo;s opinion and the financials. They were able to accomplish this by using almost entirely automated techniques&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, a contribution to the literature in itself. This paper also joins a litany of others in the bankruptcy prediction space.&lt;/p&gt;
&lt;p&gt;Given a collection of textual data (MD&amp;amp;A) can one predict the likelihood of a firm entering bankruptcy? Mayew et al. answer this in a straightforward manner. The probability of a firm going bankrupt next year is determined by estimating a hazard model, which can be used to estimate the probability of a firm failing in a given time frame. I found it interesting (based on my limited knowledge of hazard models) that their hazard model can be reduced so succintly to a simple regression.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;lough&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;lough&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Loughran, Tim and McDonald, Bill, &lt;em&gt;When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks&lt;/em&gt; (March 4, 2010). Journal of Finance, Forthcoming. Available at SSRN: &lt;a href=&#34;https://ssrn.com/abstract=1331573&#34;&gt;https://ssrn.com/abstract=1331573&lt;/a&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mayew et al. use three primary variables sourced from the MD&amp;amp;A text. &lt;em&gt;GC_MGMT&lt;/em&gt; is coded as a 1 when the management has expressed a going concern. This coding was performed manually, and the appendix provides some examples of what was considered to be an expression of going concern. &lt;em&gt;POSMDA(%)&lt;/em&gt; represents the percentage of positive words in the MD&amp;amp;A, and &lt;em&gt;NEGMDA(%)&lt;/em&gt; represents the percentage of negative words in the MD&amp;amp;A. Words are determined to be positive or negative as provided in Loughran and McDonald (2011) who created an alternative list of positive and negative words that are applicable in financial contexts.&lt;/p&gt;
&lt;p&gt;The above MD&amp;amp;A independent variables are used to predict &lt;em&gt;BRUPT&lt;/em&gt;, which is coded as a one when a firm goes bankrupt the following year and zero otherwise. Mayew et al. use numerous other variables, such as the auditor&amp;rsquo;s going concern opinion or sales to total assets. In my opinion, these are not exceptionally important to the main finding of the paper, except insofar as they demonstrate the incremental predictive ability of the MD&amp;amp;A text.&lt;/p&gt;
&lt;p&gt;The sample includes 460 bankrupt firms between 1995 and 2012, as well as the accompanying MD&amp;amp;A, auditor&amp;rsquo;s reports, and financial variables for each firm-year. There is an additional 42,265 non-bankrupt firm-years as a baseline. This sample would appear to be soundly assembled. 460 bankrupt firms is a fairly substantial sample size to estimate on.&lt;/p&gt;
&lt;p&gt;Mayew et al. present striking results for models constructed only on &lt;em&gt;GC_MGMT&lt;/em&gt;, &lt;em&gt;POSMDA(%)&lt;/em&gt;, and &lt;em&gt;NEGMDA(%)&lt;/em&gt;. The model has a pseudo-R$^2$ of 14.58%, with very high significance for each variable. The highest parameter estimate was on &lt;em&gt;GC_MGMT&lt;/em&gt; with -2.880&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, which indicates that an expression of going concern is highly explanatory of a firm&amp;rsquo;s future bankruptcy. Perhaps more importantly, the parameters estimate declines to only 2.096 when all the financial variables are included.&lt;/p&gt;
&lt;p&gt;The author&amp;rsquo;s paper generally well supports the idea that MD&amp;amp;A disclosures contain information relevant to predicting a bankruptcy, even using fairly simlistic methods. Taking the percentage of positive and negative words and determining whether the management has expressed a going concern have proved to be powerful predictors. The model inclusive of the auditor&amp;rsquo;s opinion, the MD&amp;amp;A variables, and financial variables proved to be well-fit with a Pseudo-R$^2$ of 21.13%.&lt;/p&gt;
&lt;p&gt;While this is an interesting finding, there is much more that could be done in this space. The natural-language processing space in machine learning has experienced tremendous growth in the past several years, and I suspect that applications from that space would apply well to MD&amp;amp;A analysis.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;nlp&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;nlp&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Kraus, Mathias, and Stefan Feuerriegel. &lt;em&gt;Decision Support from Financial Disclosures with Deep Neural Networks and Transfer Learning&lt;/em&gt;. Decision Support Systems 104 (December 2017): 38–48. &lt;a href=&#34;https://doi.org/10.1016/j.dss.2017.10.001&#34;&gt;https://doi.org/10.1016/j.dss.2017.10.001&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, Kraus et al. apply a variety of deep learning techniques traditionally used to model arbitrary text sequences, such as recurrent (RNN) and long-term short-memory (LTSM) neural networks. These architectures may address issues that Mayew et al. generate with their modeling technique.&lt;/p&gt;
&lt;p&gt;A key issue with Mayew et al. is that using &lt;em&gt;POSMDA(%)&lt;/em&gt; and &lt;em&gt;NEGMDA(%)&lt;/em&gt; discard any possibility of understanding the &lt;em&gt;context&lt;/em&gt; of a word and the context of the words around it. The phrase &amp;ldquo;Our performance was spectacular&amp;rdquo; and &amp;ldquo;Our performance was not spectacular&amp;rdquo; are nearly identical, but have radically different meanings. LTSM and RNN networks can model the relationship between words for exceptionally long sentences, paragraphs, and documents, and better improve on Mayew et al.&amp;rsquo;s predictive variables. Kraus et al. demonstrated a test sample accuracy of 57%&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; on classifying the direction of a stock return after the release of a financial disclosure. With some modifications, the entirety of the model could be used to predict Mayew et al.&amp;rsquo;s bankruptcy variable.&lt;/p&gt;
&lt;p&gt;Additionally, it is possible to include the text of the auditor&amp;rsquo;s opinion in the same model type to increase predictive power. It would be interesting to see if there was additional information that could be gathered from the opinion that was more comprehensive than whether or not the auditor had expressed a material going concern.&lt;/p&gt;
&lt;p&gt;Financial variables as well could be included, though in a less straightforward manner. An optimal solution would be to design a deep learning architecture such that the relationship between the lexical structure of the MD&amp;amp;A and the entire time-series of current and previous financial variables could be discerned. This is a markedly more complex topic as it would seem to require numerous compositional models.&lt;/p&gt;
&lt;p&gt;Mayew et al. demonstrate using pleasantly straightforward methods the fact that brankrupcty can be predicted by the content of the MD&amp;amp;A. This confirms an intuitive understanding of the paper&amp;rsquo;s topic, which is that management often knows more about the firm than do average investors, and that financial variables do not necessarily contain all the information necessary to get a holistic understanding of the firm.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Much of the manual work seems to have been related to manually coding whether or not the management had expressed a going concern.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Sourced from Table 3, column 4.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This is located in Table 2, column 2.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Dat Site</title>
      <link>/2018/09/20/dat-site/</link>
      <pubDate>Thu, 20 Sep 2018 20:58:31 -0700</pubDate>
      
      <guid>/2018/09/20/dat-site/</guid>
      <description>&lt;p&gt;This site is now accessible via the peer-to-peer browser &lt;a href=&#34;https://beakerbrowser.com/&#34;&gt;Beaker&lt;/a&gt; via &lt;a href=&#34;dat://cameron.pfiffer.org&#34;&gt;dat://cameron.pfiffer.org&lt;/a&gt; or directly at &lt;a href=&#34;dat://d9bb03a966c47ea49469c48111085635a6388a532dfe671cab057688560ff3e4/&#34;&gt;dat://d9bb03a966c47ea49469c48111085635a6388a532dfe671cab057688560ff3e4/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Math Camp - Week 2</title>
      <link>/2018/09/09/math-camp-week-2/</link>
      <pubDate>Sun, 09 Sep 2018 18:33:01 -0700</pubDate>
      
      <guid>/2018/09/09/math-camp-week-2/</guid>
      <description>&lt;p&gt;The second week of math camp has been finished up. This week, we got into the nuts and bolts of matrices, eigenvalue decompositions, and a couple other linear algebra-type things. I will say, I did not think that I would be spending so much time doing homework &amp;ndash; everyone I knew said something along the lines of &amp;ldquo;only two hours a day? Sounds easy.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Turns out, I have spent at least 4+ hours a day on miscellaneous math homework. I&amp;rsquo;ve also been working on tutorials and documentation for the &lt;a href=&#34;https://github.com/TuringLang/Turing.jl&#34;&gt;Turing.jl&lt;/a&gt; project, so I&amp;rsquo;ve had little free time as of late. Still, being busy like this has been extremely rewarding.&lt;/p&gt;
&lt;p&gt;I have also been trying to cram in about 30-60 minutes a night of reading &lt;a href=&#34;https://www.springer.com/us/book/9780387310732&#34;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;, a book recommended by some of the smart folks at Turing. Spectacularly reading, especially for those of us who come from the frequentist persuasion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Math Camp - Week 1</title>
      <link>/2018/09/02/math-camp-week-1/</link>
      <pubDate>Sun, 02 Sep 2018 18:33:01 -0700</pubDate>
      
      <guid>/2018/09/02/math-camp-week-1/</guid>
      <description>&lt;p&gt;At the University of Oregon, all the economist and economist-light &lt;label for=&#34;econ&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;econ&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Here, &amp;ldquo;economist-light&amp;rdquo; just means all the business school disciplines; finance, marketing, accounting, etc. &lt;/span&gt;
folks take a three week math camp designed to get them up to speed on everything they could ever possibly hope to know about the mathematics used in economics.&lt;/p&gt;
&lt;p&gt;Math camp started on Wednesday, August 29th, and so far I have had three math camp sessions which have shown me how little I know about absolutely anything.&lt;/p&gt;
&lt;p&gt;One of the biggest problems with being (i) mostly self-taught and (ii) highly computational is that I am not at all prepared for the abstract nature of so much of what mathematics is at its heart. Proofs, for example, remain mostly alien to me. Until Wednesday I had never written a proof of any kind, &lt;label for=&#34;proof&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;proof&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; If you saw the proofs I &lt;em&gt;have&lt;/em&gt; written down since Wednesday, you might say that I still have yet to right one. &lt;/span&gt;
and I have never been in a math class so far removed from numbers or expressions that are grounded in \(ℝ^{1-3}\) are largely beyond my ken.&lt;/p&gt;
&lt;p&gt;Fortunately I am learning a tremendous amount of new things. So far we&amp;rsquo;ve covered:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vectorspaces&lt;/li&gt;
&lt;li&gt;Subspaces&lt;/li&gt;
&lt;li&gt;Spans&lt;/li&gt;
&lt;li&gt;Bases&lt;/li&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Linear maps&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There&amp;rsquo;s been a total of eight questions on the homework so far, which require about 2-4 hours of my considerably-useless time. Honestly, I am having a lot of fun being in the dark on all this stuff, as it&amp;rsquo;s kind of what I signed up for with the whole doctorate thing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interesting Papers</title>
      <link>/2018/08/13/interesting-papers/</link>
      <pubDate>Mon, 13 Aug 2018 21:14:11 -0700</pubDate>
      
      <guid>/2018/08/13/interesting-papers/</guid>
      <description>&lt;p&gt;Via arXiv:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We develop a large-scale deep learning model to predict price movements from limit order book (LOB) data of cash equities. The architecture utilises convolutional filters to capture the spatial structure of the limit order books as well as LSTM modules to capture longer time dependencies. The model is trained using electronic market quotes from the London Stock Exchange. Our model delivers a remarkably stable out-of-sample prediction accuracy for a variety of instruments and outperforms existing methods such as Support Vector Machines, standard Multilayer Perceptrons, as well as other previously proposed convolutional neural network (CNN) architectures. The results obtained lead to good profits in a simple trading simulation, especially when compared with the baseline models. Importantly, our model translates well to instruments which were not part of the training set, indicating the model&amp;rsquo;s ability to extract universal features. In order to better understand these features and to go beyond a &amp;ldquo;black box&amp;rdquo; model, we perform a sensitivity analysis to understand the rationale behind the model predictions and reveal the components of LOBs that are most relevant. The ability to extract robust features which translate well to other instruments is an important property of our model which has many other applications.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Find the paper &lt;a href=&#34;https://arxiv.org/abs/1808.03668&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Finance</title>
      <link>/2018/07/09/why-finance/</link>
      <pubDate>Mon, 09 Jul 2018 19:36:58 -0700</pubDate>
      
      <guid>/2018/07/09/why-finance/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This coming fall, I will be a doctoral student at the University of Oregon. I wanted to get whatever thoughts I could ahead of my time there &amp;ndash; to try and make a &amp;ldquo;before and after&amp;rdquo;. What will I think is interesting now, and what will I think is interesting when (or if) I graduate? Will I have achieved whatever goals I thought I wanted to?&lt;/p&gt;
&lt;p&gt;To that end, I&amp;rsquo;ve collected some of my thoughts on finance, why I personally like it, and what I&amp;rsquo;m going to do after I graduate.&lt;/p&gt;
&lt;h2 id=&#34;what-is-finance&#34;&gt;What is finance?&lt;/h2&gt;
&lt;p&gt;For posterity, here is my definition of finance:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finance is a social science concerned with how and when people move, use, and consume financial assets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finance is a &lt;em&gt;social science&lt;/em&gt; because finance would not exist without people. There would be nothing to pay for twinkies with, &lt;label for=&#34;twinkie&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;twinkie&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; And no twinkies for that matter.  &lt;/span&gt;
no stocks to buy or sell, no bonds from sketchy south American countries.&lt;/p&gt;
&lt;p&gt;It is concerned wth &lt;em&gt;how&lt;/em&gt; and &lt;em&gt;why&lt;/em&gt; because those are both critical components of a dynamic system. How a system works is a nontrivial factor in why anyone interacts with a financial markets. Financial markets that are complex and unpleasant to use don&amp;rsquo;t find many participants &lt;label for=&#34;participants&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;participants&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Complex financial markets also generate middlemen and arbitrage opportunities. &lt;/span&gt;
. That &lt;em&gt;why&lt;/em&gt; is important because finance is ultimately concerned with the timing, magnitude, and nature of financial market activity from humans, and humans do not make choices in a vacuum. They make choices when confronted with circumstance and context.&lt;/p&gt;
&lt;p&gt;The financial activities that economic actors can take impact the entire system. For example, an elderly couple entering retirement has hopefully assembled a pool of assets they can rely upon for the rest of their lives. Over several decades, they will convert stocks and bonds to cash to meet liquidity needs using the financial markets. Selling those securities has a ripple effect throughout the market, though for most couples the selling of any single couple is unlikely to cause much change in the aggregate. But on this small scale there are numerous other entities who are impacted by those transactions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Brokers who negotiate the trades.&lt;/li&gt;
&lt;li&gt;Banks who might make markets in the sold securities.&lt;/li&gt;
&lt;li&gt;Clearinghouses who clear and settle the trades.&lt;/li&gt;
&lt;li&gt;Mutual funds, hedge funds, index funds, arbitrageurs, and speculators who might make decisions based on observed activity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finance is the study of the network of entities who exchange assets with one another. Neat stuff.&lt;/p&gt;
&lt;h2 id=&#34;why-finance&#34;&gt;Why finance?&lt;/h2&gt;
&lt;p&gt;So why is finance something I&amp;rsquo;m into? My background would not indicate to anyone that finance is a lifelong passion of mine. I have a Bachelors of Science in Theater Arts. Not exactly finance.&lt;/p&gt;
&lt;p&gt;I had been exposed to some interesting stuff during my MSc in Finance. The first thing to pique my interest was algorithmic trading, which is the study of how to trade in an algorithmic fashion so as to meet some requirement, such as limited market impact or limited cost uncertainty. This is a heavily automated process. People can perform algorithmic trading without computers, but it is a slow and unplesant process.&lt;/p&gt;
&lt;p&gt;More broadly I came to notice a lot more of the curious ways in which technological systems impact financial markets. &lt;label for=&#34;trades&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;trades&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; This isn&amp;rsquo;t even counting the number of orders placed and revoked, which is orders of magnitudes greater. &lt;/span&gt;
Exchanges are heavily optimized for a tremendous amount of transactions. NASDAQ alone handled around &lt;a href=&#34;http://www.nasdaqtrader.com/Trader.aspx?id=DailyMarketSummary&#34;&gt;11.5 million trades&lt;/a&gt; on July 12th, 2018.&lt;/p&gt;
&lt;p&gt;High frequency traders use incredibly sophisticated software and hardware &lt;label for=&#34;fpga&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;fpga&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; My favorite thing among the HFT tricks is &lt;a href=&#34;https://quant.stackexchange.com/questions/10519/how-are-hft-systems-implemented-on-fpga-nowadays&#34;&gt;the use of FPGAs&lt;/a&gt;. &lt;/span&gt;
to gain microseconds on their competitors. There&amp;rsquo;s some evidence to suggest that much of the &amp;ldquo;real&amp;rdquo; liqudity provided today comes from high frequency traders.&lt;/p&gt;
&lt;p&gt;What about cryptocurrencies? Many financial economists I have spoken with are somewhere between dismissive and uninterested. But to me they appear to be the perfect distillation of my interests &amp;ndash; sophisticated technological underpinnings, financial assets, and informational egalitarianism.&lt;/p&gt;
&lt;p&gt;I am interested in finance because it is interesting. We all have money, sometimes however little, and we all use it. But more importantly, we are still in the midst of what may be the largest change in financial markets since fiat money. We are still trying to figure out how computational power can effect financial markets.&lt;/p&gt;
&lt;h2 id=&#34;what-are-you-going-to-do-after-you-graduate&#34;&gt;What are you going to do after you graduate?&lt;/h2&gt;
&lt;p&gt;Academia absolutely has it&amp;rsquo;s allure. In academia there is substantially less pressure to produce fiscally viable research &amp;ndash; you can grind away on something merely because it is personally interesting or valuable in a nonmonetary sense either to your peers or to society&amp;rsquo;s understanding of financial markets. The resources you have access to are not small, but they are also not &lt;strong&gt;enormous&lt;/strong&gt; like they might be in the private sector.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;right-now&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;right-now&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; I say &amp;ldquo;right now&amp;rdquo; because everyone has informed me that my interests will change dramatically over the course of my studies. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As of right now, here are the broad interests I might like to work in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Computational finance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Asset pricing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;High-frequency, algorithmic, and systematic trading&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computational financial engineering
&lt;label for=&#34;simon&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;simon&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; See &lt;em&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/contracts-icfp.pdf&#34;&gt;Composing contracts: An adventure in financial engineering&lt;/a&gt;&lt;/em&gt; by Jones, Eber, and Seward. This is not strictly finance, but it is something I am heavily interested in and would likely enjoy branching out into. I think my favorite thing about this paper is it was the first time I saw how &lt;a href=&#34;https://www.haskell.org/&#34;&gt;Haskell&lt;/a&gt; could be unique and useful. Granted, I am still not a particularly talented Haskell programmer, but it is always nice to dream. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cryptocurrencies&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Blockchain technology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fintech&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What about the private sector? The allure there is strong too. In the private sector I could have the responsibility for a thing I worked on. If it was a portion of a pricing engine for a market making desk, I would have responsibility for making sure that it is &lt;em&gt;accurate&lt;/em&gt;, &lt;em&gt;timely&lt;/em&gt;, and &lt;em&gt;faultless&lt;/em&gt;. That responsibility is something I don&amp;rsquo;t think people get in quite so direct a manner in academia. I think the problems would also tend to be a bit more focused and less nebulous which I&amp;rsquo;ve noticed tends to substantially focus my ability to work.&lt;/p&gt;
&lt;p&gt;I suspect it might be fun to work for &lt;a href=&#34;https://www.janestreet.com/&#34;&gt;Jane Street&lt;/a&gt;, though I doubt I am smart enough to work there. Most everyone I&amp;rsquo;ve seen give talks from Jane Street is absolutely whip smart and quick. I like that they use &lt;a href=&#34;https://ocaml.org/&#34;&gt;OCaml&lt;/a&gt; for everything. I like that they are market makers, I like that they are problem solvers, and I like that they are technologists. There are a lot of people in finance as of 2018 who are very much rooted in the way finance worked in previous decades. It doesn&amp;rsquo;t work that way anymore. Markets are ever more automated and complex.&lt;/p&gt;
&lt;p&gt;I guess I&amp;rsquo;ll just have to find out in a couple years, and see how I feel about how hard the PhD was for me. I suspect I&amp;rsquo;ll end up going for whatever is the most difficult thing to do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Theory</title>
      <link>/2018/06/25/information-theory/</link>
      <pubDate>Mon, 25 Jun 2018 21:30:55 -0700</pubDate>
      
      <guid>/2018/06/25/information-theory/</guid>
      <description>&lt;p&gt;&lt;label for=&#34;alink&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;alink&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; &lt;a href=&#34;https://www.thriftbooks.com/w/an-introduction-to-information-theory_john-robinson-pierce/294512/?mkwid=sbM6YJYtB%7cdc&amp;amp;pcrid=70112900832&amp;amp;pkw=&amp;amp;pmt=&amp;amp;plc=&amp;amp;gclid=Cj0KCQjwpcLZBRCnARIsAMPBgF28X1wNo0AKYjuBEjeeBKTk73gnywqwZUJWQFnZ9DQwigTTOEG7_R8aArg3EALw_wcB#isbn=0486240614&amp;amp;idiq=3821805&#34;&gt;&lt;em&gt;An Introduction to Information Theory&lt;/em&gt;&lt;/a&gt; by John R. Pierce. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve recently been perusing an introductory text of &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_theory&#34;&gt;information theory&lt;/a&gt;. Information theory is one of those things that I have always wanted to look into but never gotten around to &amp;ndash; but now I am an adult with real things to try not to do, so I figured what the hell.&lt;/p&gt;
&lt;p&gt;Information theory is the scientific study of how information can be quantified, stored, and communicated efficiently. It&amp;rsquo;s fascinating to read the version of the book I have as the author (and Claude Shannon, the father of information theory) all viewed everything as telephonic or telegraphic. Pierce mentions computers a handful of times, but often only to describe what they are incapable of doing.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; He calls them &amp;ldquo;automata&amp;rdquo;, which is such a lovely old-timey phrase. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Much of the content regards speech and how we can predict, transmit, and describe it. It appears to be primarily based on probability theory, and for good reason &amp;ndash; if you know that &lt;strong&gt;E&lt;/strong&gt; appears 13% of the time and &lt;strong&gt;Z&lt;/strong&gt; appears almost never, you can make asusmptions about how you code letters by assigning easily transmitted values for common symbols and more complex values for infrequent symbols.&lt;/p&gt;
&lt;p&gt;The primary reason I&amp;rsquo;m reading it is for the applications to financial markets. A lot of the reason why information theory came about was because there was a need for sophisticated &lt;em&gt;signal processing&lt;/em&gt; techniques during World War II. &lt;a href=&#34;https://en.wikipedia.org/wiki/Signal_processing&#34;&gt;Signal processing&lt;/a&gt; is something commonly applied to finance. Trades indicate resources, desire, risk tolerance, what have you &amp;ndash; but there&amp;rsquo;s also a lot of noise. How do you tell people who are &lt;em&gt;informed&lt;/em&gt; from people who are &lt;em&gt;uninformed&lt;/em&gt;? How do you know whether a trade is actually meaningful to the long-term price of the stock?&lt;/p&gt;
&lt;p&gt;Dunno. Thought I&amp;rsquo;d read about it though.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Links</title>
      <link>/links/</link>
      <pubDate>Wed, 20 Jun 2018 06:56:21 -0700</pubDate>
      
      <guid>/links/</guid>
      <description>&lt;p&gt;Miscellaneous links I think are good.&lt;/p&gt;
&lt;h2 id=&#34;economics&#34;&gt;Economics&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;industrial-organization&#34;&gt;Industrial organization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ken Train&amp;rsquo;s &lt;a href=&#34;https://eml.berkeley.edu/books/choice2.html&#34;&gt;Discrete Choice book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chris Conlon&amp;rsquo;s &lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34;&gt;industrial organization notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;finance&#34;&gt;Finance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.johnhcochrane.com/research-all/asset-pricing&#34;&gt;Asset Pricing&lt;/a&gt; by John Cochrane&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;books&#34;&gt;Books&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;data-science--econometrics&#34;&gt;Data Science &amp;amp; Econometrics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://otexts.org/fpp2/&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econometrics-with-r.org/&#34;&gt;Econometrics with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;machine-learning--ai&#34;&gt;Machine Learning &amp;amp; AI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.stewartcalculus.com/media/16_home.php&#34;&gt;Calculus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://learnyouahaskell.com/&#34;&gt;Learn You a Haskell&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;services&#34;&gt;Services&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;media&#34;&gt;Media&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.plex.tv/&#34;&gt;Plex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;news&#34;&gt;News&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.wsj.com/&#34;&gt;Wall Street Journal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.washingtonpost.com/&#34;&gt;Washington Post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fivethirtyeight.com/&#34;&gt;FiveThirtyEight&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;education&#34;&gt;Education&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;khan&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;khan&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; I&amp;rsquo;ve learned pretty much everything I know about math from Khan Academy. Best place on the internet. &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/&#34;&gt;Khan Academy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;code-1&#34;&gt;Code&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;languages&#34;&gt;Languages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/dotnet/csharp/&#34;&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.haskell.org/&#34;&gt;Haskell&lt;/a&gt; &lt;label for=&#34;haskell&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;
&lt;input type=&#34;checkbox&#34; id=&#34;haskell&#34; class=&#34;margin-toggle&#34;/&gt;
&lt;span class=&#34;marginnote&#34;&gt; Do you like purity? Do you like a typing system so fierce it&amp;rsquo;ll make your head spin? Do you like unlearning everything you ever thought you knew about programming? Try Haskell! &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;probabalistic-programming&#34;&gt;Probabalistic Programming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An &lt;a href=&#34;https://www.cs.cornell.edu/courses/cs4110/2016fa/lectures/lecture33.html&#34;&gt;introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/TuringLang/Turing.jl&#34;&gt;Turing.jl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;general&#34;&gt;General&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;cool-sites&#34;&gt;Cool Sites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2019/visual-exploration-gaussian-processes/&#34;&gt;Gaussian Processes at Distill&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&amp;rsquo;s all, folks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Good Book</title>
      <link>/2018/06/20/good-book/</link>
      <pubDate>Wed, 20 Jun 2018 06:51:27 -0700</pubDate>
      
      <guid>/2018/06/20/good-book/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a spectacular book on using R for forecasting online &amp;ndash; so far, it seems like a breezy, informative read. &lt;a href=&#34;https://otexts.org/fpp2/&#34;&gt;Take a look here&lt;/a&gt;. No more to note on that, just thought I&amp;rsquo;d stick a link up.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Is Insufficient</title>
      <link>/2018/06/08/deep-learning-is-insufficient/</link>
      <pubDate>Fri, 08 Jun 2018 06:20:22 -0700</pubDate>
      
      <guid>/2018/06/08/deep-learning-is-insufficient/</guid>
      <description>


&lt;p&gt;Via &lt;a href=&#34;https://arxiv.org/abs/1801.00631&#34;&gt;Gary Marcus&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thus, for example, in a system like Lerer et al’s (2016) efforts to learn about the physics of falling towers, there is no prior knowledge of physics (beyond what is implied in convolution). Newton’s laws, for example, are not explicitly encoded; the system instead (to some limited degree) approximates them by learning contingencies from raw, pixel level data. As I note in a forthcoming paper in innate (Marcus, in prep) researchers in deep learning appear to have a very strong bias against including prior knowledge even when (as in the case of physics) that prior knowledge is well known.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Marcus’ article contains a wide variety of detailed critiques on why he believes that deep learning is flawed in a lot of ways – namely, that it is not the end-all-be-all of artificial intelligence, only that it is a component part of future generalized intelligence. It’s a wonderful read.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Economist</title>
      <link>/2018/06/06/privacy-economist/</link>
      <pubDate>Wed, 06 Jun 2018 05:33:04 -0700</pubDate>
      
      <guid>/2018/06/06/privacy-economist/</guid>
      <description>


&lt;p&gt;Via &lt;a href=&#34;https://www.economist.com/technology-quarterly/2018-05-02/justice?utm_campaign=Data_Elixir&amp;amp;utm_medium=email&amp;amp;utm_source=Data_Elixir_185&#34;&gt;The Economist&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some people argue that those who have done nothing wrong need not worry. But that justifies limitless state surveillance, and risks a chilling effect on citizens’ fundamental civil liberties. After all, if you are not planning crimes while talking on the phone, why not just let police officers listen to every call? Police need oversight not because they are bad people but because maintaining the appropriate balance between liberty and security requires constant vigilance by engaged citizens. This is doubly true for new technologies that make police better at their jobs when policy, due process and public opinion have not caught up.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Rust &amp; More</title>
      <link>/2018/06/03/rust-more/</link>
      <pubDate>Sun, 03 Jun 2018 21:01:18 -0700</pubDate>
      
      <guid>/2018/06/03/rust-more/</guid>
      <description>


&lt;p&gt;For the past two or so weeks I’ve been spending a lot of time writing &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;. Rust is a really spectacularly designed language with perhaps the greatest package manager&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;It’s called &lt;a href=&#34;https://crates.io/&#34;&gt;Cargo&lt;/a&gt;.&lt;/span&gt; I have ever experienced.&lt;/p&gt;
&lt;p&gt;Rust is fascinating to me because it’s one of my first true compiled languages. In contrast to the other languages I’m somewhat good at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R (mostly interpereted)&lt;/li&gt;
&lt;li&gt;Python (interpereted)&lt;/li&gt;
&lt;li&gt;C# (JIT compiled)&lt;/li&gt;
&lt;li&gt;Julia (JIT compiled)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am only passingly familiar with C and have mostly repressed C++, so Rust was both new and refreshing to me. The workflows are strikingly different – with Julia or R or what have you, I go through a very rapid iterative process. Write something, run it, see what worked, fix it, move on to the next issue. It’s very slapdash.&lt;/p&gt;
&lt;p&gt;With Rust (and, I presume other compiled languages), I have to stop and &lt;em&gt;think&lt;/em&gt; a lot more about what I’m doing and how I’m doing it. This is a bit more important in Rust because of the concept of &lt;strong&gt;ownership&lt;/strong&gt;, an entirely bizarre concept of who owns what thing in any given program.&lt;/p&gt;
&lt;p&gt;My (admittedly frail) understanding of ownership is that everything that exists in a program is owned by a varible. Once that variable goes out of scope, the memory allocated goes with it. It’s how Rust manages memory safety. It has a pretty steep learning curve initially, but once you’ve learned it it’s almost effortless to use.&lt;/p&gt;
&lt;p&gt;All in all, it’s a spectacular language. Fast, too. And ultra strongly typed with none of the rigid puritanical stuff that comes with Haskell.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Down With Business</title>
      <link>/2018/05/22/down-with-business/</link>
      <pubDate>Tue, 22 May 2018 05:39:43 -0700</pubDate>
      
      <guid>/2018/05/22/down-with-business/</guid>
      <description>


&lt;p&gt;From &lt;a href=&#34;https://www.theguardian.com/news/2018/apr/27/bulldoze-the-business-school&#34;&gt;Martin Parker&lt;/a&gt; at the Guardian:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see how this works if we look a bit more closely at the business-school curriculum and how it is taught. Take finance, for instance. This is a field concerned with understanding how people with money invest it. It assumes that there are people with money or capital that can be used as security for money, and hence it also assumes substantial inequalities of income and wealth. The greater the inequalities within any given society, the greater the interest in finance, as well as the market in luxury yachts. Finance academics almost always assume that earning rent on capital (however it was acquired) is a legitimate and perhaps even praiseworthy activity, with skilful investors being lionised for their technical skills and success. The purpose of this form of knowledge is to maximise the rent from wealth, often by developing mathematical or legal mechanisms that can multiply it. Successful financial strategies are those that produce the maximum return in the shortest period, and hence that further exacerbate the social inequalities that made them possible in the first place.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The article in its entirety is Martin’s rejection of the business school, and how it fosters some ideological problems amongst its students. His characterization of finance &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;He had many other points, but I am not really qualified to talk about operations management.&lt;/span&gt; seems a bit reductive.&lt;/p&gt;
&lt;p&gt;First, finance does not necessitate inequality. Finance is merely the grease of capital, because without it money wouldn’t move as much as it does – the presence of a tool does not force its use for ill-ends. Certainly, finance allows those with capital to gain more from that capital base than the poor, but at the same time, well-capitalized people and institutions are theoretically the ones who are most likely to make investments in capital goods &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;1. Capital goods benefit us because they are expensive, and the production of (most) capital goods touches lots of hands.&lt;/span&gt; or research, things which benefit us all.&lt;/p&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;2. Research is good because it allows society to do things it couldn’t before, like cure polio, send messages across the world, or write blog posts.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Second, I don’t know that characterizing finance as the science of extracting rent from wealth is the right way to go about it. Again, yes, those with capital benefit more from finance, but so does society. Finance is the science of making sure those who &lt;em&gt;have&lt;/em&gt; assets can give money to those who &lt;em&gt;do not&lt;/em&gt;, and can do better things with that money. Without finance we would have no modern agriculture, iPhones, shopping malls, computers, or nearly anything else – if investors cannot engage in maturity and liquidity transformation, nothing will be done.&lt;/p&gt;
&lt;p&gt;The article is very well written and worth a read, though I would hesitate to destroy business schools just yet; I’ve only just been accepted to one. Wait about five years, please.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Updates</title>
      <link>/2018/05/12/updates/</link>
      <pubDate>Sat, 12 May 2018 15:08:05 -0700</pubDate>
      
      <guid>/2018/05/12/updates/</guid>
      <description>


&lt;p&gt;I’m trying to get better about writing more things. To that end, I figured I should post some general updates on the things I’ve done lately.&lt;/p&gt;
&lt;div id=&#34;new-york&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New York&lt;/h2&gt;
&lt;p&gt;My &lt;a href=&#34;qpfiffer.com&#34;&gt;brother&lt;/a&gt; and I just got back from a week-long trip to upstate New York to see our grandmother. She’s a cool lady, and we figured since we’re both sort of adults with jobs we should drop in. I raced BMX a little bit, wiped out a bit, and generally had a good time with cool folks. I also had the advantage of driving down to Palmyra/Hershey Pennsylvania to seet my cousins, three of whom are traditionally brilliant and one of which is at the fantastic &lt;a href=&#34;https://www.mhskids.org/&#34;&gt;Milton Hershey School&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-12-updates_files/figure-html/fig-margin-1.png&#34; width=&#34;672&#34; /&gt; &lt;em&gt;New York State&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;university-of-oregon-visit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;University of Oregon Visit&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-12-updates_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt; &lt;em&gt;Eugene, OR – Home of the University of Oregon&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I had previously mentioned that I was fortunate enough to be accepted to a PhD program at the University of Oregon, a superb school with some brilliant people. I drove down to Eugene this past Friday to introduce myself to everyone. I was honestly a bit afraid that my market microstructure interests would be swept under the rug to work on whatever the faculty was most skilled in&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, but that was very quickly laid to rest when I spoke with everyone. I’m perhaps the happiest I have been in a long time, and I drove the two hours home with a bit of a grin on my face.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chapel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chapel&lt;/h2&gt;
&lt;p&gt;Commensurate with my inability to focus on any particular piece of software for more than a week, I spent a good portion of the past couple days messing around with &lt;a href=&#34;https://github.com/chapel-lang/chapel&#34;&gt;Chapel&lt;/a&gt;, Cray’s simple parallel solution. It was probably the most substantive interation I’ve ever had with a sort-of-C language. Man, it is incredibly simple to do some otherwise complicated parallelization feats, and it’s all first class. My issue with it is that it seems like it may die long before it ever catches on – it’s been in development for years now and the package ecosystem is miniscule.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Primarily corporate finance, asset management, and asset pricing. Mostly on the empirical side.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PhD</title>
      <link>/2018/04/19/phd/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/19/phd/</guid>
      <description>


&lt;p&gt;I’m going to be a PhD student at the &lt;a href=&#34;https://business.uoregon.edu/phd/concentrations/finance&#34;&gt;University of Oregon&lt;/a&gt;! I am looking forward to working with some spectacularly bright people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scientific Papers</title>
      <link>/2018/04/05/scientific-papers/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/05/scientific-papers/</guid>
      <description>&lt;p&gt;An &lt;a href=&#34;https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/&#34;&gt;excellent article&lt;/a&gt; from the Atlantic explores the way that computational essays haven&amp;rsquo;t seemed to penetrate academia. There&amp;rsquo;s some information about the contest between open source and for-profit tools, like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But it’s hard to sell the scientific community on a piece of commercial software. Even though Wolfram Research has given away a free Mathematica notebook viewer for years, and even though most major universities already have a site license that lets their students and faculty use Mathematica freely, it might be too much to ask publishers to abandon PDFs, an open format, for a proprietary product.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Fair point. Why use Mathematica when you have Jupyter, which gives you access to R (in which this post is written), Julia, Python, and nearly any other worthwhile language?&lt;/p&gt;
&lt;p&gt;The broader point is about the failings in take up in the scientific community. Honestly, it&amp;rsquo;s one of those blatantly obvious points I don&amp;rsquo;t ever think about. There&amp;rsquo;s very little reason to publish the same droll LaTeX paper when you can spit out a highly interactive web document even just a PDF that exposes some of your code &amp;ndash; it&amp;rsquo;s just about providing information and making sure people &lt;em&gt;understand&lt;/em&gt; it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Business Cards</title>
      <link>/2018/02/27/business-cards/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/27/business-cards/</guid>
      <description>


&lt;p&gt;A couple months ago while I was bumming around London trying to get people to give me work, I made some (I think) really cool business card designs. I made a couple of different card designs that shared the same front and had varying backs. I have gotten some pretty good feedback whenever I’ve given these out.&lt;/p&gt;
&lt;p&gt;All the designs are made by me in Adobe Illustrator. You can get similar cards made at &lt;a href=&#34;moo.com&#34; class=&#34;uri&#34;&gt;moo.com&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;front&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Front&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Boilerplate front matter.&lt;/span&gt; &lt;img src=&#34;/cards/front.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Accounting&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A graphical representation of the accounting identity. This might be my least favorite design.&lt;/span&gt; &lt;img src=&#34;/cards/accounting.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;capital-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Capital Structure&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A pie chart showing the way that companies can be financed, along with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Weighted_average_cost_of_capital&#34;&gt;weighted average cost of capital&lt;/a&gt; function.&lt;/span&gt; &lt;img src=&#34;/cards/cap.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;event-study&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Event Study&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A visual depiction of how event studies are conducted in finance and economics.&lt;/span&gt; &lt;img src=&#34;/cards/event.jpg&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-descent-type-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gradient Descent (Type 1)&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;The first style of gradient descent I designed. I did not use this one in my actual cards in favor of the next version.&lt;/span&gt; &lt;img src=&#34;/cards/graph1.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-descent-type-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gradient Descent (Type 2)&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;This one is by far the most popular one I have made, and my personal favorite. It’s a representation of 3d &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;gradient descent&lt;/a&gt;, a optimization algorithm commonly used in machine learning.&lt;/span&gt; &lt;img src=&#34;/cards/graph2.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;markowitz-portfolio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Markowitz Portfolio&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;The Markowitz &lt;a href=&#34;https://en.wikipedia.org/wiki/Modern_portfolio_theory&#34;&gt;mean-variance portfolio&lt;/a&gt;.&lt;/span&gt; &lt;img src=&#34;/cards/markowitz.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Neural Network&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A representation of a small neural net. In retrospect, this design is a bit cluttered and unpleasant, and I’m often reluctant to give it out.&lt;/span&gt; &lt;img src=&#34;/cards/nn.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Options&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;This is a short &lt;a href=&#34;https://en.wikipedia.org/wiki/Butterfly_(options)&#34;&gt;butterfly option&lt;/a&gt;, shown with all the constituent options.&lt;/span&gt; &lt;img src=&#34;/cards/options.jpg&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Linear regression. Nice, clean, and simple&lt;/span&gt; &lt;img src=&#34;/cards/regression.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-time-value-of-money&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Time Value of Money&lt;/h3&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;A fundamental part of finance and economics. I really like the way this one looks.&lt;/span&gt; &lt;img src=&#34;/cards/tvm.jpg&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you’d like a business card, send me an email at &lt;a href=&#34;mailto:cameron@pfiffer.org&#34;&gt;cameron@pfiffer.org&lt;/a&gt; and maybe I’ll mail you one. They’re pretty cool.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tufte Stufte</title>
      <link>/2018/02/26/tufte-stufte/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/26/tufte-stufte/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/proj4js/proj4.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/highcharts/css/motion.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/highstock.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/highcharts-3d.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/highcharts-more.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/annotations.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/broken-axis.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/data.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/drilldown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/exporting.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/funnel.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/heatmap.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/map.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/no-data-to-display.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/offline-exporting.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/solid-gauge.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/modules/treemap.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/annotations.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/draggable-legend.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/draggable-points.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/export-csv.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/grouped-categories.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/motion.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/pattern-fill-v2.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/plugins/tooltip-delay.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/custom/reset.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/custom/symbols-extra.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/highcharts/custom/text-symbols.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/fontawesome/font-awesome.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/rmarkdown-libs/htmlwdgtgrid/htmlwdgtgrid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/highchart-binding/highchart.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; a margin note.&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;This &lt;em&gt;is&lt;/em&gt; a margin note. As you can see, it is to the right of the left, and it indeed appears to be smaller.&lt;/span&gt; Were it a margin note as one might presuppose, it would be to the right of the left and perhaps a bit smaller.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;newthought&#34;&gt;In the having of a new thought,&lt;/span&gt; I find myself overcome with thoughtfulness. You might say, perhaps you would be! &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;You would be overcome with thoughtfulness, as it is a thought.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thank you for saying so.&lt;/p&gt;
&lt;p&gt;Quick, look at some math! &lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[ x = 2+2 \]&lt;/span&gt;&lt;/span&gt; Tell me, what is &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Even better, here is a chart with made-up numbers. Look how fancy and &lt;em&gt;&lt;strong&gt;interactive&lt;/strong&gt;&lt;/em&gt; it is.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;highchart html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;hc_opts&#34;:{&#34;title&#34;:{&#34;text&#34;:&#34;Some Random Numbers&#34;},&#34;yAxis&#34;:{&#34;title&#34;:{&#34;text&#34;:null}},&#34;credits&#34;:{&#34;enabled&#34;:false},&#34;exporting&#34;:{&#34;enabled&#34;:false},&#34;plotOptions&#34;:{&#34;series&#34;:{&#34;turboThreshold&#34;:0},&#34;treemap&#34;:{&#34;layoutAlgorithm&#34;:&#34;squarified&#34;},&#34;bubble&#34;:{&#34;minSize&#34;:5,&#34;maxSize&#34;:25}},&#34;annotationsOptions&#34;:{&#34;enabledButtons&#34;:false},&#34;tooltip&#34;:{&#34;delayForDisplay&#34;:10},&#34;chart&#34;:{&#34;type&#34;:&#34;scatter&#34;},&#34;series&#34;:[{&#34;data&#34;:[{&#34;x&#34;:-2.00105432964465,&#34;y&#34;:-2.88115513128663},{&#34;x&#34;:1.69395853857261,&#34;y&#34;:1.97180250490159},{&#34;x&#34;:0.258289147957936,&#34;y&#34;:-0.239482012536848},{&#34;x&#34;:0.553222285812558,&#34;y&#34;:2.34415852933954},{&#34;x&#34;:1.89746186591957,&#34;y&#34;:5.43937040769522},{&#34;x&#34;:-0.795813697664382,&#34;y&#34;:-1.21905610173255},{&#34;x&#34;:-1.52625691634341,&#34;y&#34;:-1.81730698575304},{&#34;x&#34;:-0.39507050087908,&#34;y&#34;:-1.0932591894235},{&#34;x&#34;:0.93066776670334,&#34;y&#34;:2.81221446244224},{&#34;x&#34;:-0.370167736234375,&#34;y&#34;:-0.334107214472779},{&#34;x&#34;:-0.438063484790336,&#34;y&#34;:0.668336733308717},{&#34;x&#34;:1.56813462946513,&#34;y&#34;:3.78811167109424},{&#34;x&#34;:-0.174834974067972,&#34;y&#34;:-0.584157629035353},{&#34;x&#34;:0.159541444208419,&#34;y&#34;:0.934886997094617},{&#34;x&#34;:-0.783732745650928,&#34;y&#34;:-0.241464277801196}]}]},&#34;theme&#34;:{&#34;colors&#34;:[&#34;#737373&#34;,&#34;#D8D7D6&#34;,&#34;#B2B0AD&#34;,&#34;#8C8984&#34;],&#34;chart&#34;:{&#34;style&#34;:{&#34;fontFamily&#34;:&#34;Cardo&#34;}},&#34;xAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;},&#34;yAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;,&#34;tickWidth&#34;:1,&#34;gridLineColor&#34;:&#34;transparent&#34;},&#34;legend&#34;:{&#34;enabled&#34;:false}},&#34;conf_opts&#34;:{&#34;global&#34;:{&#34;Date&#34;:null,&#34;VMLRadialGradientURL&#34;:&#34;http =//code.highcharts.com/list(version)/gfx/vml-radial-gradient.png&#34;,&#34;canvasToolsURL&#34;:&#34;http =//code.highcharts.com/list(version)/modules/canvas-tools.js&#34;,&#34;getTimezoneOffset&#34;:null,&#34;timezoneOffset&#34;:0,&#34;useUTC&#34;:true},&#34;lang&#34;:{&#34;contextButtonTitle&#34;:&#34;Chart context menu&#34;,&#34;decimalPoint&#34;:&#34;.&#34;,&#34;downloadJPEG&#34;:&#34;Download JPEG image&#34;,&#34;downloadPDF&#34;:&#34;Download PDF document&#34;,&#34;downloadPNG&#34;:&#34;Download PNG image&#34;,&#34;downloadSVG&#34;:&#34;Download SVG vector image&#34;,&#34;drillUpText&#34;:&#34;Back to {series.name}&#34;,&#34;invalidDate&#34;:null,&#34;loading&#34;:&#34;Loading...&#34;,&#34;months&#34;:[&#34;January&#34;,&#34;February&#34;,&#34;March&#34;,&#34;April&#34;,&#34;May&#34;,&#34;June&#34;,&#34;July&#34;,&#34;August&#34;,&#34;September&#34;,&#34;October&#34;,&#34;November&#34;,&#34;December&#34;],&#34;noData&#34;:&#34;No data to display&#34;,&#34;numericSymbols&#34;:[&#34;k&#34;,&#34;M&#34;,&#34;G&#34;,&#34;T&#34;,&#34;P&#34;,&#34;E&#34;],&#34;printChart&#34;:&#34;Print chart&#34;,&#34;resetZoom&#34;:&#34;Reset zoom&#34;,&#34;resetZoomTitle&#34;:&#34;Reset zoom level 1:1&#34;,&#34;shortMonths&#34;:[&#34;Jan&#34;,&#34;Feb&#34;,&#34;Mar&#34;,&#34;Apr&#34;,&#34;May&#34;,&#34;Jun&#34;,&#34;Jul&#34;,&#34;Aug&#34;,&#34;Sep&#34;,&#34;Oct&#34;,&#34;Nov&#34;,&#34;Dec&#34;],&#34;thousandsSep&#34;:&#34; &#34;,&#34;weekdays&#34;:[&#34;Sunday&#34;,&#34;Monday&#34;,&#34;Tuesday&#34;,&#34;Wednesday&#34;,&#34;Thursday&#34;,&#34;Friday&#34;,&#34;Saturday&#34;]}},&#34;type&#34;:&#34;chart&#34;,&#34;fonts&#34;:&#34;Cardo&#34;,&#34;debug&#34;:false},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Don’t worry, it’s quite boring – you’re not missing out on anything.&lt;/span&gt;But no, in all seriousness, I thought I’d take the chance to explore &lt;a href=&#34;https://en.wikipedia.org/wiki/B%C3%A9zier_curve&#34;&gt;Bezier curves&lt;/a&gt; while I’m mucking about with some fun looking theming. Bezier curves came up at work, the details of which I don’t really know whether or not I can share. So, I won’t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bezier &amp;lt;- function(t, p) {
  # A simple Bezier curve
  x = (1 - t) * (1 - t) * p[1, 1] + 2 * (1 - t) * t * p[2,1] + t * t * p[3,1]
  y = (1 - t) * (1 - t) * p[1, 2] + 2 * (1 - t) * t * p[2,2] + t * t * p[3,2]
  
  return(c(x,y))
}

points &amp;lt;- matrix(c(0, 5, 1, 0, 2, 3), ncol = 2)
line &amp;lt;- matrix(bezier(0:100/100, points), ncol=2)
head(line)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]   [,2]
## [1,] 0.0000 0.0000
## [2,] 0.0991 0.0399
## [3,] 0.1964 0.0796
## [4,] 0.2919 0.1191
## [5,] 0.3856 0.1584
## [6,] 0.4775 0.1975&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;label for=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;⊕&lt;/label&gt;&lt;input type=&#34;checkbox&#34; id=&#34;tufte-mn-&#34; class=&#34;margin-toggle&#34;&gt;&lt;span class=&#34;marginnote&#34;&gt;Basically, we just plot a bunch of smaller points on a new, continuously defined function that has a nifty little bend in it.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;highchart html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;hc_opts&#34;:{&#34;title&#34;:{&#34;text&#34;:&#34;A Bezier Curve&#34;},&#34;yAxis&#34;:{&#34;title&#34;:{&#34;text&#34;:null}},&#34;credits&#34;:{&#34;enabled&#34;:false},&#34;exporting&#34;:{&#34;enabled&#34;:false},&#34;plotOptions&#34;:{&#34;series&#34;:{&#34;turboThreshold&#34;:0},&#34;treemap&#34;:{&#34;layoutAlgorithm&#34;:&#34;squarified&#34;},&#34;bubble&#34;:{&#34;minSize&#34;:5,&#34;maxSize&#34;:25}},&#34;annotationsOptions&#34;:{&#34;enabledButtons&#34;:false},&#34;tooltip&#34;:{&#34;delayForDisplay&#34;:10},&#34;chart&#34;:{&#34;type&#34;:&#34;line&#34;},&#34;series&#34;:[{&#34;data&#34;:[[0,0],[0.0991,0.0399],[0.1964,0.0796],[0.2919,0.1191],[0.3856,0.1584],[0.4775,0.1975],[0.5676,0.2364],[0.6559,0.2751],[0.7424,0.3136],[0.8271,0.3519],[0.91,0.39],[0.9911,0.4279],[1.0704,0.4656],[1.1479,0.5031],[1.2236,0.5404],[1.2975,0.5775],[1.3696,0.6144],[1.4399,0.6511],[1.5084,0.6876],[1.5751,0.7239],[1.64,0.76],[1.7031,0.7959],[1.7644,0.8316],[1.8239,0.8671],[1.8816,0.9024],[1.9375,0.9375],[1.9916,0.9724],[2.0439,1.0071],[2.0944,1.0416],[2.1431,1.0759],[2.19,1.11],[2.2351,1.1439],[2.2784,1.1776],[2.3199,1.2111],[2.3596,1.2444],[2.3975,1.2775],[2.4336,1.3104],[2.4679,1.3431],[2.5004,1.3756],[2.5311,1.4079],[2.56,1.44],[2.5871,1.4719],[2.6124,1.5036],[2.6359,1.5351],[2.6576,1.5664],[2.6775,1.5975],[2.6956,1.6284],[2.7119,1.6591],[2.7264,1.6896],[2.7391,1.7199],[2.75,1.75],[2.7591,1.7799],[2.7664,1.8096],[2.7719,1.8391],[2.7756,1.8684],[2.7775,1.8975],[2.7776,1.9264],[2.7759,1.9551],[2.7724,1.9836],[2.7671,2.0119],[2.76,2.04],[2.7511,2.0679],[2.7404,2.0956],[2.7279,2.1231],[2.7136,2.1504],[2.6975,2.1775],[2.6796,2.2044],[2.6599,2.2311],[2.6384,2.2576],[2.6151,2.2839],[2.59,2.31],[2.5631,2.3359],[2.5344,2.3616],[2.5039,2.3871],[2.4716,2.4124],[2.4375,2.4375],[2.4016,2.4624],[2.3639,2.4871],[2.3244,2.5116],[2.2831,2.5359],[2.24,2.56],[2.1951,2.5839],[2.1484,2.6076],[2.0999,2.6311],[2.0496,2.6544],[1.9975,2.6775],[1.9436,2.7004],[1.8879,2.7231],[1.8304,2.7456],[1.7711,2.7679],[1.71,2.79],[1.6471,2.8119],[1.5824,2.8336],[1.5159,2.8551],[1.4476,2.8764],[1.3775,2.8975],[1.3056,2.9184],[1.2319,2.9391],[1.1564,2.9596],[1.0791,2.9799],[1,3]]}]},&#34;theme&#34;:{&#34;colors&#34;:[&#34;#737373&#34;,&#34;#D8D7D6&#34;,&#34;#B2B0AD&#34;,&#34;#8C8984&#34;],&#34;chart&#34;:{&#34;style&#34;:{&#34;fontFamily&#34;:&#34;Cardo&#34;}},&#34;xAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;},&#34;yAxis&#34;:{&#34;lineWidth&#34;:0,&#34;minorGridLineWidth&#34;:0,&#34;lineColor&#34;:&#34;transparent&#34;,&#34;tickColor&#34;:&#34;#737373&#34;,&#34;tickWidth&#34;:1,&#34;gridLineColor&#34;:&#34;transparent&#34;},&#34;legend&#34;:{&#34;enabled&#34;:false}},&#34;conf_opts&#34;:{&#34;global&#34;:{&#34;Date&#34;:null,&#34;VMLRadialGradientURL&#34;:&#34;http =//code.highcharts.com/list(version)/gfx/vml-radial-gradient.png&#34;,&#34;canvasToolsURL&#34;:&#34;http =//code.highcharts.com/list(version)/modules/canvas-tools.js&#34;,&#34;getTimezoneOffset&#34;:null,&#34;timezoneOffset&#34;:0,&#34;useUTC&#34;:true},&#34;lang&#34;:{&#34;contextButtonTitle&#34;:&#34;Chart context menu&#34;,&#34;decimalPoint&#34;:&#34;.&#34;,&#34;downloadJPEG&#34;:&#34;Download JPEG image&#34;,&#34;downloadPDF&#34;:&#34;Download PDF document&#34;,&#34;downloadPNG&#34;:&#34;Download PNG image&#34;,&#34;downloadSVG&#34;:&#34;Download SVG vector image&#34;,&#34;drillUpText&#34;:&#34;Back to {series.name}&#34;,&#34;invalidDate&#34;:null,&#34;loading&#34;:&#34;Loading...&#34;,&#34;months&#34;:[&#34;January&#34;,&#34;February&#34;,&#34;March&#34;,&#34;April&#34;,&#34;May&#34;,&#34;June&#34;,&#34;July&#34;,&#34;August&#34;,&#34;September&#34;,&#34;October&#34;,&#34;November&#34;,&#34;December&#34;],&#34;noData&#34;:&#34;No data to display&#34;,&#34;numericSymbols&#34;:[&#34;k&#34;,&#34;M&#34;,&#34;G&#34;,&#34;T&#34;,&#34;P&#34;,&#34;E&#34;],&#34;printChart&#34;:&#34;Print chart&#34;,&#34;resetZoom&#34;:&#34;Reset zoom&#34;,&#34;resetZoomTitle&#34;:&#34;Reset zoom level 1:1&#34;,&#34;shortMonths&#34;:[&#34;Jan&#34;,&#34;Feb&#34;,&#34;Mar&#34;,&#34;Apr&#34;,&#34;May&#34;,&#34;Jun&#34;,&#34;Jul&#34;,&#34;Aug&#34;,&#34;Sep&#34;,&#34;Oct&#34;,&#34;Nov&#34;,&#34;Dec&#34;],&#34;thousandsSep&#34;:&#34; &#34;,&#34;weekdays&#34;:[&#34;Sunday&#34;,&#34;Monday&#34;,&#34;Tuesday&#34;,&#34;Wednesday&#34;,&#34;Thursday&#34;,&#34;Friday&#34;,&#34;Saturday&#34;]}},&#34;type&#34;:&#34;chart&#34;,&#34;fonts&#34;:&#34;Cardo&#34;,&#34;debug&#34;:false},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Neato. What a great Bezier curve. Thanks to &lt;a href=&#34;https://stackoverflow.com/a/5634528&#34;&gt;xan&lt;/a&gt; on StackOverflow for providing the pseudocode for that Bezier curve.&lt;/p&gt;
&lt;p&gt;Now, I’d like to ask you to hang out with that curve for a little while and consider it’s elegance, if you would be so kind.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Corporate Debt</title>
      <link>/2017/09/12/corporate-debt/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/12/corporate-debt/</guid>
      <description>


&lt;p&gt;This strikes me as &lt;a href=&#34;https://www.ft.com/content/46027dd2-8f6c-11e7-9084-d0c17942ba93&#34;&gt;a strange use of corporate cash&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thirty US companies together have more than $800bn of fixed-income investments, according to a Financial Times analysis of their most recent filings with the US Securities and Exchange Commission.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s a lot of money. You might be wondering who those thirty companies are, and it’s probably who you’d expect – tech companies with far too much cash sitting around collecting dust. Apple, Alphabet, and Microsoft are cited.&lt;/p&gt;
&lt;p&gt;This strikes me as a huge problem (&lt;a href=&#34;https://www.economist.com/news/business-and-finance/21722809-their-excuses-doing-so-dont-add-up-tech-firms-hoard-huge-cash-piles&#34;&gt;I’m not the only one&lt;/a&gt;): fixed income investing is really useful if you’re trying to meet fixed obligations, like an insurer or pension fund might, or for hedges on inflation and foreign exchange. It’s certainly not something research and development companies should be doing. They should be piling that cash into R&amp;amp;D, acquisitions, buybacks, literally anything other than fixed income investments.&lt;/p&gt;
&lt;p&gt;I’d also be more than willing to take it. You know, for the team.&lt;/p&gt;
&lt;p&gt;We’ll close on this happy note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The emergence of US companies as a leading investor in corporate debt alongside traditional asset managers comes at a time many in the market express concern about a bond market bubble that could be vulnerable to bursting should inflation and economic growth accelerate.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>The American Dream</title>
      <link>/2017/09/03/the-american-dream/</link>
      <pubDate>Sun, 03 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/03/the-american-dream/</guid>
      <description>&lt;p&gt;Edmund Phelps writing at &lt;a href=&#34;https://www.project-syndicate.org/commentary/recalling-the-american-dream-by-edmund-s--phelps-2017-08?utm_source=Project+Syndicate+Newsletter&amp;amp;utm_campaign=e8bef8a0bc-sunday_newsletter_3_9_2017&amp;amp;utm_medium=email&amp;amp;utm_term=0_73bad5b7d8-e8bef8a0bc-104329629&#34;&gt;Project Syndicate&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What made the American Dream distinctive was neither the hope of winning the lottery nor of being buoyed by national market forces or public policy. It was the hope of achieving things, with all that that entails: drawing on one’s personal knowledge, trusting one’s intuition, venturing into the unknown. It reflected the deep need of these Americans to have the experience of succeeding at something: a craftsman’s gratification at seeing his mastery result in better work, or a merchant’s satisfaction at seeing “his ship come in.” It was success that mattered, not relative success (would anyone want to be the sole achiever?). And the process may have mattered more than the success.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>The Flash Crash</title>
      <link>/2017/08/26/the-flash-crash/</link>
      <pubDate>Sat, 26 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/26/the-flash-crash/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://www.afajof.org/details/journalArticle/10546091/The-Flash-Crash-HighFrequency-Trading-in-an-Electronic-Market.html&#34;&gt;A paper&lt;/a&gt; came out in the April 2017 edition of the &lt;em&gt;Journal of Finance&lt;/em&gt; that I found to be absolutely fascinating. Normally I’d just put a link up on Twitter and move on, but leaving Facebook and Twitter have limited my ability to scream about cool papers. I thought I’d do a brief post about the paper and talk about why it’s interesting to me.&lt;/p&gt;
&lt;p&gt;The abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event—the Flash Crash—where a large automated selling program was rapidly executed in the E‐mini S&amp;amp;P 500 stock index futures market. Using audit trail transaction‐level data for the E‐mini on May 6 and the previous three days, we find that the trading pattern of the most active nondesignated intraday intermediaries (classified as High‐Frequency Traders) did not change when prices fell during the Flash Crash.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The flash crash&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; has always been an interesting subject, because it illustrates a lesson that anyone who is involved with markets knows – if you sell a ton of stuff, the prices go down. But perhaps more importantly, it demonstrates the highly interconnected nature of modern financial markets.&lt;/p&gt;
&lt;p&gt;During the flash crash, an institutional trader initiated an algorithm to sell an incredibly large amount of E-mini shares, a futures contract on the S&amp;amp;P 500. The algorithm used was a fairly simplistic strategy that aimed to trade 9% of the past minute’s volume.&lt;/p&gt;
&lt;p&gt;I have a lot of feelings on this kind of strategy. It’s not especially tactful, it has no regard to price or timing, and fails to adapt to changing conditions. If I’m aware of someone trading a tremendous amount of shares every minute (and assuming I can devise their strategy before they complete the trade), I might try to game the system by increasing the volume traded in the previous minute – either by simply churning a position or taking large long positions – and then using the foreknowledge that a large trade is about to come to take advantage. Particularly for a trade the size that the institution was trying to make (around $4.9 billion), this is a really crappy way to do it.&lt;/p&gt;
&lt;p&gt;Back to the paper. It’s well worth the read. They study audit trail level data for four days during and before the flash crash in the E-mini market, and their interesting finding is that high-frequency traders didn’t really change their trading behavior during the event. This is in contrast to a lot of the murmurings commonly bandied about in regards to the flash crash, where everyone mutters something to the effect of “HFTs got out, liquidity dried up, etc.”&lt;/p&gt;
&lt;p&gt;The authors note that market makers, not high-frequency traders, altered their inventory holding behavior in response to changing prices. Interesting.&lt;/p&gt;
&lt;p&gt;Give it a read.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You can read more about the flash crash in the &lt;a href=&#34;https://www.sec.gov/news/studies/2010/marketevents-report.pdf&#34;&gt;SEC’s report&lt;/a&gt; on the matter.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>English Pronunciation</title>
      <link>/2017/08/16/english-pronunciation/</link>
      <pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/16/english-pronunciation/</guid>
      <description>&lt;p&gt;My last name tends to throw a lot of people off, because it has far too many f&amp;rsquo;s and not enough real letters. While doing a bit of research on how to tell people how to best pronounce my name (it&amp;rsquo;s &amp;ldquo;fye-fur&amp;rdquo;, not &amp;ldquo;fiffer&amp;rdquo;), I found a &lt;a href=&#34;http://ncf.idallen.com/english.html&#34;&gt;neat little poem&lt;/a&gt; designed to showcase some of the strange ways English pronounces words. I sent it out to a couple of my friends from Norway and Greece and they politely declined to try a live reading. I can see why; I messed up about 15-20 words while reading it aloud.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an excerpt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare alien with Italian,
Dandelion and battalion.
Sally with ally, yea, ye,
Eye, I, ay, aye, whey, and key.
Say aver, but ever, fever,
Neither, leisure, skein, deceiver.
Heron, granary, canary.
Crevice and device and aerie.&lt;/p&gt;
&lt;p&gt;Face, but preface, not efface.
Phlegm, phlegmatic, ass, glass, bass.
Large, but target, gin, give, verging,
Ought, out, joust and scour, scourging.
Ear, but earn and wear and tear
Do not rhyme with here but ere.
Seven is right, but so is even,
Hyphen, roughen, nephew Stephen,
Monkey, donkey, Turk and jerk,
Ask, grasp, wasp, and cork and work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Give it a shot, and see how well you can do with the whole poem. It&amp;rsquo;s certainly fun to do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perfect Numbers</title>
      <link>/2017/08/15/perfect-numbers/</link>
      <pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/15/perfect-numbers/</guid>
      <description>


&lt;p&gt;I just started reading a book called &lt;a href=&#34;https://books.google.co.uk/books/about/How_to_Prove_it.html?id=murSjwEACAAJ&amp;amp;redir_esc=y&amp;amp;hl=en&#34;&gt;How to Prove It: A Structured Approach&lt;/a&gt;, a book all about writing and understanding proofs. My unconventional background in mathematics means that I have a fair bit of terror when it comes to proofs; they have often come up on more quantitative finance courses in a tangential manner, as the proofs are often unnessecary. The fact remains that I am terrified by them, and wanted to get over my fear of proofs.&lt;/p&gt;
&lt;p&gt;In the process, I read a bit on &lt;a href=&#34;https://en.wikipedia.org/wiki/Perfect_number&#34;&gt;perfect numbers&lt;/a&gt;. One of the proofs in the book was Euclid’s proof about infinite primes, and I figured I should write a little bit of code to find perfect numbers, for gits and shiggles.&lt;/p&gt;
&lt;p&gt;Here’s the code to compute &lt;a href=&#34;https://en.wikipedia.org/wiki/Aliquot_sum&#34;&gt;aliquot sums&lt;/a&gt; in Julia, a necessary intermediate step in perfect number evaluation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;using Primes

function aliquot(x::Int64, verbose=false)
    divisors = []
    for i in 1:x-1
        if(x%i == 0)
            append!(divisors, i)
        end
    end
    if verbose == true print(divisors) end
    return sum(divisors)
end&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a function for computing the next prime number after integer &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function next_perfect(x::Int64, limit=Inf64)
    # Given a number, find the next highest perfect number.
    found = false
    while(found==false)
        # You can set a computational limit with the &amp;#39;limit&amp;#39; argument.
        if (x &amp;gt;= limit) break end
        if aliquot(x) == x
            print(x, &amp;quot; is the next perfect number.&amp;quot;)
            found=true
            break
        end
        x += 1
    end
    return x
end&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Brief Paper on Cointegration in Crude Oil</title>
      <link>/2017/08/06/a-brief-paper-on-cointegration-in-crude-oil/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/06/a-brief-paper-on-cointegration-in-crude-oil/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve just finished putting the final touches on my final paper for my master&amp;rsquo;s degree, and I thought I&amp;rsquo;d write up a bit about the process. You can get the fancy LaTeX version &lt;a href=&#34;https://drive.google.com/file/d/0B6yUWclvz_SNREhObUxEVjFESWs/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My final summer course for the master&amp;rsquo;s program was all about Energy Finance, taught by an econometrician&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Commensurate with the professor&amp;rsquo;s experience, the final paper was along the same lines. We were allowed to choose between three different project options. Here&amp;rsquo;s the one I picked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Are the prices of WTI and Brent co-integrated? Is it possible to profitably trade the price differential? First, summarize the results of the empirical literature. Then conduct your own analysis. Carefully explain your methodology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This was easily the most difficult amongst the three options, because you had to design a trading strategy. For many of my colleagues who like programming less, it proved difficult to design a system to evaluate a trading system. But I thought it might be a fun challenge (and it was!) to do so.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d read the paper if you&amp;rsquo;re particularly interested in the specifics, but I thought I&amp;rsquo;d include a little bit more about the backend. I don&amp;rsquo;t have the chance to get into it in the text, but I basically wrote an entire trading system to measure transaction costs, portfolio value, etc. That was what took the most time. I spent more time debugging the transaction system than writing the paper, and I don&amp;rsquo;t actually talk about it in the paper.&lt;/p&gt;
&lt;p&gt;I also got pretty deep into &lt;a href=&#34;https://en.wikipedia.org/wiki/Cointegration&#34;&gt;cointegration&lt;/a&gt;, which is a fascinating concept. Basically, two series that are non-stationary&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; can have a &lt;em&gt;cointegrating vector&lt;/em&gt; which makes a linear combination of the series stationary. This has some pretty interesting knock-on effects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the combined series is stationary, you know that the series crosses the mean frequently, and that it is bound to revert. In crude oil prices, this means that the spread between two assets (like Brent and WTI crude oils) cannot remain too far from the x-intercept for too long.&lt;/li&gt;
&lt;li&gt;The variance is (theoretically&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;) bound to a constant state, so you can moderately achieve returns without excess variance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The bulk on my trading strategy relies on these two factors &amp;ndash; if the cointegrated series is far from zero (or close to zero) then it will &lt;em&gt;eventually&lt;/em&gt; return to the mean. I had some pretty neat returns, and actually beat the S&amp;amp;P in and out of sample.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m really proud of the paper, and hope that maybe someone might get to make use of the strategy I developed. Maybe when I&amp;rsquo;m older and actually have capital, I&amp;rsquo;ll do it, though by then the cointegrating relationship may have changed.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;These dudes tend to make everything more fun.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This means that the series is essentially random and has no trend, has a mean of zero, and a constant variance.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I say theoretically, because empirically, the variance is not constant. Particularly the period between 2008 and 2013 or so was especially volatile, and the period between 1994 and 2005 was extremely placid.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Julia</title>
      <link>/2017/07/05/julia/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/05/julia/</guid>
      <description>&lt;p&gt;As I&amp;rsquo;m wrapping up my master&amp;rsquo;s degree, I have somehow managed to find a large amount of time to pursue personal interests. One of those interests is &lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt;, a technical computing language with C-comparable speed. I&amp;rsquo;m not exactly sure where I stumbled on it, but it stuck with me. Of course, the best way to learn something is to do something &lt;em&gt;cool&lt;/em&gt; with it, and &lt;a href=&#34;https://fivethirtyeight.com/features/pick-a-number-any-number/&#34;&gt;FiveThiryEight&amp;rsquo;s Riddler&lt;/a&gt; often tends to supply great cannon fodder for programming. This past week&amp;rsquo;s one was a computationally difficult one:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From Itay Bavly, a chain-link number problem:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You start with the integers from one to 100, inclusive, and you want to organize them into a chain. The only rules for building this chain are that you can only use each number once and that each number must be adjacent in the chain to one of its factors or multiples. For example, you might build the chain:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;4, 12, 24, 6, 60, 30, 10, 100, 25, 5, 1, 97&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You have no numbers left to place after 97, leaving you with a finished chain of length 12.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the longest chain you can build?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There really doesn&amp;rsquo;t appear to be an easy answer to the problem &amp;ndash; my brother noted this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Friend of mine says that traversing a directed graph is NP-Complete, so brute-force is the way to do it. Probably&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I thought it seemed like a perfect time to try out Julia. The past two weeks or so I&amp;rsquo;ve been idly combing through Julia&amp;rsquo;s &lt;a href=&#34;https://docs.julialang.org/en/stable/&#34;&gt;fantastic documentation&lt;/a&gt;, and I&amp;rsquo;ve been really impressed by the syntax&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and ease at which you can handle very fast processes.&lt;/p&gt;
&lt;p&gt;What I wanted to do was basically try and brute force the problem. Here&amp;rsquo;s my pseudocode.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pick a random number.&lt;/li&gt;
&lt;li&gt;Pick a valid number to follow it.&lt;/li&gt;
&lt;li&gt;Repeat until you can&amp;rsquo;t find a number.&lt;/li&gt;
&lt;li&gt;Do steps 1-3 with new chains, discarding the shortest chain.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mathematically, it&amp;rsquo;s very simple to define what&amp;rsquo;s a multiple and what&amp;rsquo;s a factor, here&amp;rsquo;s two functions that do that. &lt;code&gt;valid&lt;/code&gt; is a function where you pass an &lt;code&gt;x&lt;/code&gt; and a &lt;code&gt;y&lt;/code&gt; and return &lt;code&gt;true&lt;/code&gt; if &lt;code&gt;x&lt;/code&gt; can be followed by &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Test if x can be followed by y&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; valid(x, y, limit)
	&lt;span style=&#34;color:#75715e&#34;&gt;# Determine if y is a multiple of x&lt;/span&gt;
	mul &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; multiples(x, limit) &lt;span style=&#34;color:#75715e&#34;&gt;# Get multiples of x&lt;/span&gt;
	index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; findin(mul, y) &lt;span style=&#34;color:#75715e&#34;&gt;# Find if y is in the list of x&amp;#39;s multiples&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; [] &lt;span style=&#34;color:#75715e&#34;&gt;# If the index isn&amp;#39;t zero&lt;/span&gt;
		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; true
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

	&lt;span style=&#34;color:#75715e&#34;&gt;# Now determine if y is a factor of x&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; true
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; false
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Multiples generates a list of multiples and returns it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; multiples(x, limit)
	vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;limit
		&lt;span style=&#34;color:#75715e&#34;&gt;#print(i, &amp;#34;\n&amp;#34;)&lt;/span&gt;
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; x)
			append!(vals, i)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; vals &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
		print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;No multiples of &amp;#34;&lt;/span&gt;, x, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vals
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These two functions are called by &lt;code&gt;makechain&lt;/code&gt;, which picks the first number&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, and then tests if subsequent random numbers are valid. When it runs out of valid numbers, it spits out the answer.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; makechain(limit&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int64&lt;/span&gt;)
  possible &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Array&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;limit)
  first &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand(possible)
  remove &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; getindex(possible, first)
  deleteat!(possible,remove)

  chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [first]

	&lt;span style=&#34;color:#75715e&#34;&gt;# Pick a random number.&lt;/span&gt;
	&lt;span style=&#34;color:#75715e&#34;&gt;# Check if that number is valid.&lt;/span&gt;
	&lt;span style=&#34;color:#75715e&#34;&gt;# If it isn&amp;#39;t pick a new one, until they&amp;#39;re all gone.&lt;/span&gt;
	testPosition &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; possible
	shuffle!(testPosition)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; testPosition
		v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; valid(chain[&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;], i, limit)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; true
			append!(chain, i)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; chain
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, the final function just runs &lt;code&gt;makechain&lt;/code&gt; a bunch of times and finds the longest chain it can.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; find_longest(iterations&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int64&lt;/span&gt;, limit&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
	longest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;iterations
		chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; makechain(limit)
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; length(chain) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; length(longest)
			longest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; chain
		&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; longest
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;My biggest output was something like 27 integers long after building 10 million chains, which was far below the 77 found by two other contestants. One guy apparently solved it with some nifty combinatorics software.&lt;/p&gt;
&lt;p&gt;Even though I didn&amp;rsquo;t get the right answer, I had a lot of fun working with Julia for the first time and I&amp;rsquo;m looking forward to finding neat things to do with it. Also, Julia is &lt;strong&gt;wicked fast&lt;/strong&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Later, this was confirmed by &lt;a href=&#34;https://fivethirtyeight.com/features/is-this-bathroom-occupied/&#34;&gt;Oliver Roeder at the Riddler&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;It kind of reads like Python with a bit of Matlab.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The function&amp;rsquo;s argument, &lt;code&gt;limit&lt;/code&gt;, allows you to test chains between 1 and any integer.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle Titanic</title>
      <link>/2017/05/23/kaggle-titanic/</link>
      <pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/23/kaggle-titanic/</guid>
      <description>


&lt;p&gt;I thought I’d start getting into &lt;a href=&#34;www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt; to work on some non-finance data to get a feel for the messiness of real-world information. Kaggle’s introductory competition is about predicting which passengers on the Titanic are going to survive using a handful of features, so let’s launch into mucking about. This post follows a “lab book” style and is quite scattered, as I develop ideas about what to do.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Libraries
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;purrr&amp;#39; was built under R version 3.4.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
# Load data
train &amp;lt;- read_csv(&amp;quot;../../data/Titanic/train.csv&amp;quot;)
test &amp;lt;- read_csv(&amp;quot;../../data/Titanic/test.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s take a look at the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &amp;quot;Braund, Mr. Owen Harris&amp;quot; &amp;quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&amp;quot; &amp;quot;Heikkinen, Miss. Laina&amp;quot; &amp;quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&amp;quot; ...
##  $ Sex        : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &amp;quot;A/5 21171&amp;quot; &amp;quot;PC 17599&amp;quot; &amp;quot;STON/O2. 3101282&amp;quot; &amp;quot;113803&amp;quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  NA &amp;quot;C85&amp;quot; NA &amp;quot;C123&amp;quot; ...
##  $ Embarked   : chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...
##  - attr(*, &amp;quot;spec&amp;quot;)=List of 2
##   ..$ cols   :List of 12
##   .. ..$ PassengerId: list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Survived   : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Pclass     : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Name       : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Sex        : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Age        : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_double&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ SibSp      : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Parch      : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_integer&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Ticket     : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Fare       : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_double&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Cabin      : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   .. ..$ Embarked   : list()
##   .. .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_character&amp;quot; &amp;quot;collector&amp;quot;
##   ..$ default: list()
##   .. ..- attr(*, &amp;quot;class&amp;quot;)= chr  &amp;quot;collector_guess&amp;quot; &amp;quot;collector&amp;quot;
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;col_spec&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What I want to do first is add a couple of features. &lt;a href=&#34;https://www.datacamp.com/community/open-courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic#gs.UrAze5E&#34;&gt;DataCamp’s excellent tutorial&lt;/a&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; on this data set uses &lt;code&gt;Title&lt;/code&gt; and &lt;code&gt;FamilySize&lt;/code&gt;, which I’ll add now. I also thought it might be cool to separate out family names to see if certain families were likely to survive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Do it with test and train, don&amp;#39;t want to reconcile them later.
test &amp;lt;- test %&amp;gt;% 
  mutate(Surname = as.factor(word(test$Name, sep = fixed(&amp;quot;,&amp;quot;))),
         Title = word(test$Name, start = 1, sep = fixed(&amp;quot;.&amp;quot;)))

test$Title &amp;lt;- test$Title %&amp;gt;% 
  str_replace(&amp;quot;[.]&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
  word(start = -1) %&amp;gt;% 
  as.factor(.)

# Remove uncommon titles
uncommon &amp;lt;- test %&amp;gt;% 
  group_by(Title) %&amp;gt;%
  count() %&amp;gt;% 
  filter (n &amp;gt;=5) 

levels(test$Title) &amp;lt;- c(levels(test$Title), &amp;quot;Other&amp;quot;)
test$Title[!(test$Title %in% uncommon$Title)] &amp;lt;- as.factor(&amp;quot;Other&amp;quot;)
test$Title &amp;lt;- droplevels.data.frame(test)$Title

# Update embarkment location to factor
test$Embarked &amp;lt;- as.factor(test$Embarked)

# Gender to factor
test$Sex &amp;lt;- as.factor(test$Sex)

test$FamilySize &amp;lt;- test$Parch + test$SibSp + 1

# Change training dataset
train &amp;lt;- train %&amp;gt;% 
  mutate(Surname = as.factor(word(train$Name, sep = fixed(&amp;quot;,&amp;quot;))),
         Title = word(train$Name, start = 1, sep = fixed(&amp;quot;.&amp;quot;)))

train$Title &amp;lt;- train$Title %&amp;gt;% 
  str_replace(&amp;quot;[.]&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
  word(start = -1) %&amp;gt;% 
  as.factor(.)

# Remove uncommon titles
uncommon &amp;lt;- train %&amp;gt;% 
  group_by(Title) %&amp;gt;%
  count() %&amp;gt;% 
  filter (n &amp;gt;=5) 

levels(train$Title) &amp;lt;- c(levels(train$Title), &amp;quot;Other&amp;quot;)
train$Title[!(train$Title %in% uncommon$Title)] &amp;lt;- as.factor(&amp;quot;Other&amp;quot;)
train$Title &amp;lt;- droplevels.data.frame(train)$Title

# Update embarkment location to factor
train$Embarked &amp;lt;- as.factor(train$Embarked)

# Gender to factor
train$Sex &amp;lt;- as.factor(train$Sex)

train$FamilySize &amp;lt;- train$Parch + train$SibSp + 1

summary(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId       Survived          Pclass          Name          
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##      Sex           Age            SibSp           Parch       
##  female:314   Min.   : 0.42   Min.   :0.000   Min.   :0.0000  
##  male  :577   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  
##               Median :28.00   Median :0.000   Median :0.0000  
##               Mean   :29.70   Mean   :0.523   Mean   :0.3816  
##               3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  
##               Max.   :80.00   Max.   :8.000   Max.   :6.0000  
##               NA&amp;#39;s   :177                                     
##     Ticket               Fare           Cabin           Embarked  
##  Length:891         Min.   :  0.00   Length:891         C   :168  
##  Class :character   1st Qu.:  7.91   Class :character   Q   : 77  
##  Mode  :character   Median : 14.45   Mode  :character   S   :644  
##                     Mean   : 32.20                      NA&amp;#39;s:  2  
##                     3rd Qu.: 31.00                                
##                     Max.   :512.33                                
##                                                                   
##       Surname       Title       FamilySize    
##  Andersson:  9   Dr    :  7   Min.   : 1.000  
##  Sage     :  7   Master: 40   1st Qu.: 1.000  
##  Carter   :  6   Miss  :182   Median : 1.000  
##  Goodwin  :  6   Mr    :517   Mean   : 1.905  
##  Johnson  :  6   Mrs   :125   3rd Qu.: 2.000  
##  Panula   :  6   Rev   :  6   Max.   :11.000  
##  (Other)  :851   Other : 14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s a lot of Anderssons! I wonder if they’re related - let’s check family size by surname.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;% 
  filter(Surname == &amp;quot;Andersson&amp;quot;) %&amp;gt;% 
  select(Name, FamilySize, Survived, SibSp, Parch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 5
##                                                        Name FamilySize
##                                                       &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1                               Andersson, Mr. Anders Johan          7
## 2                           Andersson, Miss. Erna Alexandra          7
## 3                         Andersson, Miss. Ellis Anna Maria          7
## 4              Andersson, Mr. August Edvard (&amp;quot;Wennerstrom&amp;quot;)          1
## 5                      Andersson, Miss. Ingeborg Constanzia          7
## 6                         Andersson, Miss. Sigrid Elisabeth          7
## 7 Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)          7
## 8                        Andersson, Miss. Ebba Iris Alfrida          7
## 9                   Andersson, Master. Sigvard Harald Elias          7
## # ... with 3 more variables: Survived &amp;lt;int&amp;gt;, SibSp &amp;lt;int&amp;gt;, Parch &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are all related - except for Erna and August, and &lt;a href=&#34;https://titanicstory.wordpress.com/2012/04/04/the-entire-andersson-family-was-lost-on-the-titanic/&#34;&gt;the whole family died&lt;/a&gt;. This is a really sad data set.&lt;/p&gt;
&lt;p&gt;Na’s are the bane of any good analysis, and I want to try to remove some of them. Let’s try to pull out as many as we can.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_age &amp;lt;- function(df) {
  # Turns missing values into the average for the column.
  NA2mean &amp;lt;- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
  df[,&amp;#39;Age&amp;#39;] &amp;lt;- lapply(df[,&amp;#39;Age&amp;#39;], NA2mean)
  return(df)
}

clean_embarkment &amp;lt;- function(df) {
  # The most people embarked from &amp;#39;S&amp;#39;, so I&amp;#39;m just setting
  # the two missing values to &amp;#39;S&amp;#39;.
  df[is.na(df[,&amp;#39;Embarked&amp;#39;]), &amp;#39;Embarked&amp;#39;] &amp;lt;- &amp;#39;S&amp;#39;
  return (df)
}

test &amp;lt;- clean_age(test)
train &amp;lt;- clean_age(train)

test &amp;lt;- clean_embarkment(test)
train &amp;lt;- clean_embarkment(train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also want to scale all my features to between 0 and 1, to make processing easier. This also means scrapping the names and turning all numerical values into numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaler &amp;lt;- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

cleanse &amp;lt;- function(df) {
  # Remove character variables
  df &amp;lt;- subset(df, select = -c(Name, Ticket, Cabin, Surname))
  
  # If it&amp;#39;s a prediction set or otherwise, break it out
  if(&amp;#39;Survived&amp;#39; %in% colnames(df)){
    id &amp;lt;- select(df, Survived, PassengerId)
    df &amp;lt;- subset(df, select = -c(Survived, PassengerId))
  } else {
    id &amp;lt;- select(df, PassengerId)
    df &amp;lt;- subset(df, select = -c(PassengerId))
  }
  
  # Convert factors to numbers
  factname = c(&amp;#39;Embarked&amp;#39;, &amp;#39;Title&amp;#39;, &amp;#39;Sex&amp;#39;)
  df[,factname] &amp;lt;- lapply(df[,factname] , as.integer)
  
  # Scale variables
  df &amp;lt;- as.tibble(map(df, na.rm = TRUE, scaler))
  
  # Again, separate by labeled or not
  if(&amp;#39;Survived&amp;#39; %in% colnames(id)){
    df$PassengerId &amp;lt;- id$PassengerId
    df$Survived &amp;lt;- id$Survived
  } else {
    df$PassengerId &amp;lt;- id$PassengerId
  }
  
  return(df)
}

train_scl &amp;lt;- cleanse(train)
test_scl &amp;lt;- cleanse(test)

summary(train_scl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Pclass            Sex              Age             SibSp        
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  
##  1st Qu.:0.5000   1st Qu.:0.0000   1st Qu.:0.2712   1st Qu.:0.00000  
##  Median :1.0000   Median :1.0000   Median :0.3679   Median :0.00000  
##  Mean   :0.6543   Mean   :0.6476   Mean   :0.3679   Mean   :0.06538  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.4345   3rd Qu.:0.12500  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  
##      Parch             Fare            Embarked          Title       
##  Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0.01544   1st Qu.:0.5000   1st Qu.:0.3333  
##  Median :0.0000   Median :0.02821   Median :1.0000   Median :0.5000  
##  Mean   :0.0636   Mean   :0.06286   Mean   :0.7682   Mean   :0.4805  
##  3rd Qu.:0.0000   3rd Qu.:0.06051   3rd Qu.:1.0000   3rd Qu.:0.5000  
##  Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  
##    FamilySize       PassengerId       Survived     
##  Min.   :0.00000   Min.   :  1.0   Min.   :0.0000  
##  1st Qu.:0.00000   1st Qu.:223.5   1st Qu.:0.0000  
##  Median :0.00000   Median :446.0   Median :0.0000  
##  Mean   :0.09046   Mean   :446.0   Mean   :0.3838  
##  3rd Qu.:0.10000   3rd Qu.:668.5   3rd Qu.:1.0000  
##  Max.   :1.00000   Max.   :891.0   Max.   :1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start the analysis with a good old-fashioned logistic regression. Throw everything we’ve got into the pot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit &amp;lt;- glm(Survived ~ Pclass + Sex + Age + SibSp + 
               Parch + Fare + Embarked + Title,
             family = binomial(),
             data = train_scl,
             na.action = na.omit)
summary(logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked + Title, family = binomial(), data = train_scl, 
##     na.action = na.omit)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6460  -0.5874  -0.4168   0.6330   2.4134  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)   4.4881     0.5158   8.701  &amp;lt; 2e-16 ***
## Pclass       -2.1785     0.2789  -7.811 5.66e-15 ***
## Sex          -2.7493     0.1995 -13.783  &amp;lt; 2e-16 ***
## Age          -2.7969     0.6686  -4.183 2.87e-05 ***
## SibSp        -2.7339     0.8755  -3.123  0.00179 ** 
## Parch        -0.5260     0.7104  -0.740  0.45910    
## Fare          0.9325     1.2190   0.765  0.44427    
## Embarked     -0.4345     0.2300  -1.889  0.05891 .  
## Title        -0.8603     0.6672  -1.289  0.19724    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1186.66  on 890  degrees of freedom
## Residual deviance:  783.39  on 882  degrees of freedom
## AIC: 801.39
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basically what the above tells us is that Pretty much everything decreases your chances of living. You start at a high level (the intercept has a coefficient of 4.6) and decrease from there. Men have a sex of 1, and women have a sex of 0, so being a man is a strong predictor of dying. The strongest indicator by far is age - being older decreases your chances of living. Let’s take the testing data set and predict what we think the results are likely to be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we predict using the model
threshold &amp;lt;- 0.5
logit_pred &amp;lt;- predict(logit, newdata = test_scl, type = &amp;#39;response&amp;#39;)
hist(logit_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-12-kaggle-titanic_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit_pred &amp;lt;- ifelse(logit_pred &amp;gt; threshold, 1, 0)
# If we&amp;#39;re missing data, predict 0.
logit_pred[is.na(logit_pred)] &amp;lt;- 0
summary(logit_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.3589  1.0000  1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool. Let’s export it and see what results we get!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all &amp;lt;- data.frame(test$PassengerId, logit_pred)
colnames(all) &amp;lt;- c(&amp;quot;PassengerID&amp;quot;, &amp;quot;Survived&amp;quot;)
write_csv(all, &amp;quot;../../data/Titanic/predictions/logit_prediction.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural Networks&lt;/h1&gt;
&lt;p&gt;After submitting to Kaggle, this method gives me an accuracy of 76%, worse than the random forest method, which gave 79%. Let me see if a neural network is any better.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(neuralnet)
set.seed(91)

# Model the neural network
nnet &amp;lt;- neuralnet(Survived ~ Pclass + Sex + Age + SibSp + 
             Parch + Fare + Embarked + Title,
             hidden = c(2,2,2),
             threshold = 0.035,
             stepmax = 400000000,
             data = train_scl,
             lifesign = &amp;#39;full&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hidden: 2, 2, 2    thresh: 0.035    rep: 1/1    steps:      1000 min thresh: 0.1103394579
##                                                             2000 min thresh: 0.05215387676
##                                                             3000 min thresh: 0.04034919191
##                                                             4000 min thresh: 0.03822729544
##                                                             5000 min thresh: 0.03554090753
##                                                             5122 error: 54.00459 time: 3.79 secs&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict the test set
nnet.c &amp;lt;- compute(nnet, test_scl[,1:8])
nnet.c &amp;lt;- nnet.c$net.result
hist(nnet.c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-12-kaggle-titanic_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet.c &amp;lt;- ifelse(nnet.c &amp;gt; threshold, 1, 0)
# If we&amp;#39;re missing data, predict 0.
nnet.c[is.na(nnet.c)] &amp;lt;- 0

summary(nnet.c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1           
##  Min.   :0.0000000  
##  1st Qu.:0.0000000  
##  Median :0.0000000  
##  Mean   :0.3755981  
##  3rd Qu.:1.0000000  
##  Max.   :1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all &amp;lt;- data.frame(test$PassengerId, nnet.c)
colnames(all) &amp;lt;- c(&amp;quot;PassengerID&amp;quot;, &amp;quot;Survived&amp;quot;)
write_csv(all, &amp;quot;../../data/Titanic/predictions/nnet_prediction.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve run a lot of other computation on a variety of neural networks, with up to five layers and a variety of node amounts - I only ever matched random forest accuracy with a relatively uncomplicated neural network with three layers of two nodes, at 79%. I suspect that for this data set, predicting survival is best suited to other algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I used random forests and decision trees as my first submissions. DataCamp’s tutorial does an excellent job explaining the methodology and code, so you can check out the hyperlink above if you’re interested.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Rise of DIY Quants</title>
      <link>/2017/05/16/the-rise-of-diy-quants/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/16/the-rise-of-diy-quants/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This post first appeared in the May 2017 edition of the Reading University Investment Society newspaper - you can find a copy of the whole paper &lt;a href=&#34;https://www.dropbox.com/s/bn6kukgloae9sr9/May.pdf?dl=0&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Modern finance is a constantly evolving field. In the 1970s after Black-Scholes published their seminal paper, derivatives in their current form (for primitive derivatives are found as far back as in ancient Sumerian culture) became common and ubiquitous. Approximately ten years later, we saw the rise of the first quantitative powerhouse Renaissance Technologies, a firm that to this day makes absurd profits. Even today, the rise of passive investing is driven by mathematics as firms construct smart-beta models and strive for mean-variance portfolio optimization.&lt;/p&gt;
&lt;p&gt;But the past five years have seen another interesting development - the rise of DIY quants. Idle software engineers, physicists, students, hedge fund managers, and even yours truly have engaged in the construction of systematic and automated trading tools with free tools like Quantopian, Quandl, CloudQuant, Numerai, and many others. These platforms combine free data with programming and software development tools, and many offer tutorials on quantitative investment strategies and techniques.&lt;/p&gt;
&lt;p&gt;Take Quantopian for example. In about ten minutes from creating an account, you can have a algorithmized strategy for mean-reversion up and running. You can even hook your algorithm up to two brokers for live trading, either Interactive Brokers or Robinhood. Quantopian offers minute-level equities data, as well as futures prices to trade on, all callable by an easy-to-manage API. Participants on the platform can place their algorithms in competitions, with the prize being capital awarded from Quantopian and its full deployment in the market.&lt;/p&gt;
&lt;p&gt;You can couple your Quantopian strategy with Quandl&amp;rsquo;s vast array of core and alternative data, such as satellite imagery, oil tank storage levels, retailer email receipt data, and other custom datasets.  One could imagine a complex algorithm that locates all oil tankers currently shipping, weights the levels in reserve, and prices equities for oil and shipping as well as oil derivatives. In fact, it&amp;rsquo;s likely that some hedge fund somewhere in the world is already doing such a thing.&lt;/p&gt;
&lt;p&gt;The world is becoming more accepting of this type of behavior. In some ways, it reflects global culture&amp;rsquo;s growing aversion to high finance, as we shift away from large banks and financial institutions and move to passive investing. In the same way, DIY quants are democratizing previously unavailable services and reclaiming sovereignty over their investments.&lt;/p&gt;
&lt;p&gt;This type of investing wouldn&amp;rsquo;t have been able to occur even in the 2000s - few had the skills necessary to write the code, the computers were too slow, the digital infrastructure was lacking, and the data was prohibitively expensive. Python and other high-level languages (often condemned by institutional quantitative investors as being too slow for production) have risen in ubiquity. Even in the late 2000&amp;rsquo;s the difference in speed between Python and a faster language, like C or Fortran, was nearly insurmountable due to the slowness of the computers they ran on. Now, a budding math-geek can run a trading bot on a cloud server from a Chromebook, and utilize thousands of times more computational power than was used to send men to the moon. Lastly, the data is too expensive! In the past, someone who wanted to do what someone in their garage today could do for free would have to pay thousands of dollars to have CDs shipped to their house in order to model the data - now, all you need is a quick API call and you can have world-class data.&lt;/p&gt;
&lt;p&gt;I will note that this type of investing cannot compete with true high-frequency trading outfits. Such firms pay millions to co-locate inside broker&amp;rsquo;s facilities to reduce latency, and often have the ability of paying near-zero explicit transaction costs. DIY quantitative investing should work on minute scale or longer. I personally have a portfolio management algorithm that simple performs mean-variance optimization and rebalances once a month, so I don&amp;rsquo;t have to think about my investments.&lt;/p&gt;
&lt;p&gt;If you think you&amp;rsquo;d like to be a cog in the efficient market, go check out any of the quantitative investment sites! Try and find a signal that nobody else has found. If you do, give me a call – maybe I’ll invest.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Github Pages</title>
      <link>/2017/05/13/github-pages/</link>
      <pubDate>Sat, 13 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/13/github-pages/</guid>
      <description>&lt;p&gt;I recently moved my site away from &lt;a href=&#34;zeit.co&#34;&gt;zeit&lt;/a&gt;, and migrated it to &lt;a href=&#34;pages.github.com&#34;&gt;GitHub Pages&lt;/a&gt;. It&amp;rsquo;s much easier to maintain the site with git than with zeit&amp;rsquo;s &lt;code&gt;now&lt;/code&gt; feature, which is a little too high-powered for my tases.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve also changed the theme to a modified version of &lt;a href=&#34;https://github.com/damiencaselli/paperback&#34;&gt;paperback&lt;/a&gt;. The colors were originally all sepia tone, and I liked the current color scheme and typography.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Credit Risk &amp; Logistic Regression</title>
      <link>/2017/05/01/credit-risk-logistic-regression/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/01/credit-risk-logistic-regression/</guid>
      <description>


&lt;p&gt;I thought I’d do a little bit of analysis to showcase some credit risk analysis, using &lt;strong&gt;logistic regression&lt;/strong&gt;. I’ve pulled this sample loan data from a DataCamp course on &lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-credit-risk-modeling-in-r&#34;&gt;credit risk&lt;/a&gt;. It’s a cool class, you should check it out if you have time. In a later post, I will try this same analysis with a neural network to see if it has better predictive capabilities.&lt;/p&gt;
&lt;p&gt;Here’s a look at the data. You can see we have all kinds of valuable information we can use in determining whether someone is likely to default - people with high rates and low credit scores are more likely to default, while people who own their homes and have had long term employment grades are less likely to default. We also have a column called &lt;code&gt;loan_status&lt;/code&gt;, which is a boolean value indicating whether that particular borrower has defaulted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   loan_status loan_amnt int_rate grade emp_length home_ownership
## 1           0      5000    10.65     B         10           RENT
## 2           0      2400       NA     C         25           RENT
## 3           0     10000    13.49     C         13           RENT
## 4           0      5000       NA     A          3           RENT
## 5           0      3000       NA     E          9           RENT
## 6           0     12000    12.69     B         11            OWN
##   annual_inc age
## 1      24000  33
## 2      12252  31
## 3      49200  24
## 4      36000  39
## 5      48000  24
## 6      75000  28&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;If you’re unfamiliar with logistic regression, that’s alright. What it does (in broad strokes) is allow you to predict a value &lt;strong&gt;between 1 and 0&lt;/strong&gt;, and provide you with a degree of certainty. For example, if we ran a logistic regression on a bunch of variables, and then found relevant coefficients, we could use the features of a particular borrower to determine what level of risk they have. A lender could take appropriate measures with someone with a very low chance (0.02) of default by providing them with lower rates, or by simply not lending to someone with a very high chance of default (0.99).&lt;/p&gt;
&lt;p&gt;Now we should tidy up some of the data and get rid of any rows with &lt;code&gt;NA&lt;/code&gt;s. There are more efficient ways of dealing with this problem, but for our purposes we only lose about 3,000 observations, bringing us to about 25,000 observations. This is enough by most measures to build a rudimentary predictive model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Filter out rows with any NAs.
data &amp;lt;- data[complete.cases(data),]
head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   loan_status loan_amnt int_rate grade emp_length home_ownership
## 1           0      5000    10.65     B         10           RENT
## 3           0     10000    13.49     C         13           RENT
## 6           0     12000    12.69     B         11            OWN
## 7           1      9000    13.49     C          0           RENT
## 8           0      3000     9.91     B          3           RENT
## 9           1     10000    10.65     B          3           RENT
##   annual_inc age
## 1      24000  33
## 3      49200  24
## 6      75000  28
## 7      30000  22
## 8      15000  22
## 9     100000  28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s split our dataset into two pieces, 60/40. This allows us to design a model with the 60% dataset and test it on the 40% dataset. If I was performing more exploratory analysis, I’d split the 40 in half, one for messing around in and the other for testing, but I’m pretty much skipping right to the modeling for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9090)
bound &amp;lt;- floor((nrow(data)/4)*3)
data &amp;lt;- data[sample(nrow(data)),]
train &amp;lt;- data[1:bound,]
test &amp;lt;- data[(bound+1):nrow(data),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to the preliminary model. We can use the R’s built-in functions to handle this. See below a summary of the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- glm(loan_status ~., family = binomial(link=&amp;#39;logit&amp;#39;), data = train)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = loan_status ~ ., family = binomial(link = &amp;quot;logit&amp;quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1467  -0.5358  -0.4416  -0.3374   3.3591  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)         -3.006e+00  2.151e-01 -13.976  &amp;lt; 2e-16 ***
## loan_amnt           -2.241e-06  4.141e-06  -0.541  0.58839    
## int_rate             9.058e-02  2.301e-02   3.936 8.29e-05 ***
## gradeB               3.338e-01  1.084e-01   3.080  0.00207 ** 
## gradeC               4.932e-01  1.569e-01   3.143  0.00167 ** 
## gradeD               5.809e-01  1.995e-01   2.911  0.00360 ** 
## gradeE               5.946e-01  2.505e-01   2.374  0.01760 *  
## gradeF               8.550e-01  3.343e-01   2.558  0.01053 *  
## gradeG               1.242e+00  4.367e-01   2.844  0.00446 ** 
## emp_length           5.405e-03  3.655e-03   1.479  0.13920    
## home_ownershipOTHER  7.172e-01  3.331e-01   2.153  0.03130 *  
## home_ownershipOWN   -1.000e-01  9.608e-02  -1.041  0.29795    
## home_ownershipRENT  -1.647e-02  5.329e-02  -0.309  0.75723    
## annual_inc          -5.325e-06  7.722e-07  -6.896 5.36e-12 ***
## age                 -5.048e-03  3.911e-03  -1.291  0.19685    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13297  on 19177  degrees of freedom
## Residual deviance: 12761  on 19163  degrees of freedom
## AIC: 12791
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we test our accuracy. How well does our model predict loan status? The below code spits out the accuracy when we test our model on the &lt;code&gt;test&lt;/code&gt; dataframe, and we get a result of 89%. Not bad!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- predict(model, newdata = test, type = &amp;quot;response&amp;quot;)
fit &amp;lt;- ifelse(fit &amp;gt; 0.5, 1, 0)
error &amp;lt;- mean(fit != test$loan_status)
print(paste( &amp;quot;Accuracy is: &amp;quot;, 1 - error))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy is:  0.893477240732051&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;a href=&#34;https://datascienceplus.com/perform-logistic-regression-in-r/&#34;&gt;lovely blogpost&lt;/a&gt; recommends plotting the true positive vs. false positives. The code for that is below. The plot shows a nearly straight line, which means we really aren’t especially predictive - the output at the bottom of 0.66 similarly shows the same. We’d like this value to be closer to one to indicate predictive ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)
p &amp;lt;- predict(model, newdata = test, type = &amp;quot;response&amp;quot;)
pr &amp;lt;- prediction(p, test$loan_status)
prf &amp;lt;- performance(pr, measure = &amp;quot;tpr&amp;quot;, x.measure = &amp;quot;fpr&amp;quot;)
plot(prf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-01-credit-risk-part-1_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc &amp;lt;- performance(pr, measure=&amp;quot;auc&amp;quot;)
auc &amp;lt;- auc@y.values[[1]]
auc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6606138&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we’ve skipped a couple of important steps in modeling. The model summary shows a litany of variables that really aren’t that predictive; we need to take them out. We’re going to leave anything with a &lt;code&gt;.&lt;/code&gt; or any number of asterisks (&lt;code&gt;*&lt;/code&gt;) in, because they are significant. A 10% significant will suffice for me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model2 &amp;lt;- glm(loan_status ~ int_rate + grade + emp_length +
                (home_ownership==&amp;#39;OTHER&amp;#39;) + annual_inc + age, 
              family = binomial(link=&amp;#39;logit&amp;#39;), 
              data = train)
summary(model2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = loan_status ~ int_rate + grade + emp_length + (home_ownership == 
##     &amp;quot;OTHER&amp;quot;) + annual_inc + age, family = binomial(link = &amp;quot;logit&amp;quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1355  -0.5361  -0.4424  -0.3373   3.3724  
## 
## Coefficients:
##                                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)                   -3.030e+00  2.117e-01 -14.309  &amp;lt; 2e-16 ***
## int_rate                       8.976e-02  2.300e-02   3.903 9.51e-05 ***
## gradeB                         3.324e-01  1.082e-01   3.071  0.00213 ** 
## gradeC                         4.950e-01  1.569e-01   3.155  0.00161 ** 
## gradeD                         5.807e-01  1.994e-01   2.912  0.00359 ** 
## gradeE                         5.941e-01  2.501e-01   2.375  0.01755 *  
## gradeF                         8.522e-01  3.338e-01   2.553  0.01067 *  
## gradeG                         1.230e+00  4.358e-01   2.822  0.00478 ** 
## emp_length                     5.473e-03  3.593e-03   1.523  0.12767    
## home_ownership == &amp;quot;OTHER&amp;quot;TRUE  7.319e-01  3.316e-01   2.208  0.02728 *  
## annual_inc                    -5.386e-06  6.847e-07  -7.867 3.65e-15 ***
## age                           -5.070e-03  3.911e-03  -1.296  0.19490    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13297  on 19177  degrees of freedom
## Residual deviance: 12762  on 19166  degrees of freedom
## AIC: 12786
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to test accuracy. It basically has yielded no meaningful change in predictive ability - but then again, that’s hard to do. All we’ve done is create a more parsimonious model in line with current thinking in statistics and econometrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- predict(model2, newdata = test, type = &amp;quot;response&amp;quot;)
fit &amp;lt;- ifelse(fit &amp;gt; 0.5, 1, 0)
error &amp;lt;- mean(fit != test$loan_status)
print(paste( &amp;quot;Accuracy is: &amp;quot;, 1 - error))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy is:  0.893477240732051&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we plot the cure we showed before. Again, no real difference, but we can feel better that we have a smaller model with less “junk” floating around.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)
p &amp;lt;- predict(model2, newdata = test, type = &amp;quot;response&amp;quot;)
pr &amp;lt;- prediction(p, test$loan_status)
prf &amp;lt;- performance(pr, measure = &amp;quot;tpr&amp;quot;, x.measure = &amp;quot;fpr&amp;quot;)
plot(prf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-05-01-credit-risk-part-1_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc &amp;lt;- performance(pr, measure=&amp;quot;auc&amp;quot;)
auc &amp;lt;- auc@y.values[[1]]
auc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.661003&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks for taking the time to read this post. Check out later posts where I use neural networks to look at this same dataset. It’ll be fun for the whole family!&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I made a newspaper</title>
      <link>/2017/05/01/i-made-a-newspaper/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/01/i-made-a-newspaper/</guid>
      <description>&lt;p&gt;A couple of my friends here at the University of Reading came up with an excellent idea to start a financial newspaper. The concept was that the society&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; would collect articles on investing and other things finance and publish them in a monthly paper. Being one of the few native English speakers with an interest in editing and journalism, I was selected to run the paper as Chief Editor.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m very proud of the results, you can find the finished version of the April edition &lt;a href=&#34;https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0&#34;&gt;here&lt;/a&gt;. I wrote a couple articles, about the &lt;a href=&#34;/2017/04/16/2017-05-01-federal-funds-hike/&#34;&gt;federal funds hike&lt;/a&gt; and &lt;a href=&#34;/2017/04/16/2017-05-01-iex-s-crumbling-quote/&#34;&gt;IEX&lt;/a&gt;. I also wrote a lovely editors note, available in the full edition.&lt;/p&gt;
&lt;p&gt;We intend to release at least a few more editions before we all graduate, and we&amp;rsquo;ll see how they turn out.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;That&amp;rsquo;s what they call &amp;ldquo;clubs&amp;rdquo; in the UK.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/2017/04/30/introduction/</link>
      <pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/30/introduction/</guid>
      <description>&lt;p&gt;Hello! I&amp;rsquo;ve just finished building this site with Hugo and &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt;, an excellent R package built up by the folks at RStudio. The first iteration of this site is being hosted by &lt;a href=&#34;https://zeit.co/now&#34;&gt;zeit&lt;/a&gt;, a super interesting cloud company. Deployment takes about a minute, unlike the absolute nightmare that is Google Cloud. They take Dockerfiles as well, something that I haven&amp;rsquo;t had any experience with until lately.&lt;/p&gt;
&lt;p&gt;As this is the first blog post here, I just thought I&amp;rsquo;d lay out a bit of why I built this. Over the years I have accumulated a strange variety of skills, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Market Microstructure&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCTcsdFvTCGyJ8vSp3iHuTyg&#34;&gt;Improvisational Piano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corporate Finance&lt;/li&gt;
&lt;li&gt;Econometrics&lt;/li&gt;
&lt;li&gt;Programming&lt;/li&gt;
&lt;li&gt;Lighting Design&lt;/li&gt;
&lt;li&gt;Carpentry&lt;/li&gt;
&lt;li&gt;Bookbinding&lt;/li&gt;
&lt;li&gt;Machine Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of those are finance, which is easily my favorite thing to study. At some point I&amp;rsquo;d lke to go back to graduate school and study for a PhD, but at the moment, I&amp;rsquo;m tired of living on student loans. Others are related to my previous career as a professional stagehand, lighting designer, entertainment electrician, and wrench monkey. The piano thing just sort of happened one day.&lt;/p&gt;
&lt;p&gt;My undergraduate degree is in theater arts, and my master&amp;rsquo;s degree is in corporate finance, so it has always been very difficult to demonstrate to employers that I am (i) good at what they&amp;rsquo;re hiring for and (ii) interested in the subject.&lt;/p&gt;
&lt;p&gt;To that end, I hope to produce a series of posts exploring a handful of the things I enjoy. More coming soon.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;$$ PV_n=FV_n\left(1+r\right)^n $$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Federal Funds Hike</title>
      <link>/2017/04/16/federal-funds-hike/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/16/federal-funds-hike/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This article first appeared in the Reading University Investment Society&amp;rsquo;s  &lt;a href=&#34;https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0&#34;&gt;April edition&lt;/a&gt; on April 16th, 2017.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last month, the Federal Reserve raised the short-term interest rate target range by 0.25%, to a range of 0.75% to 1%. This is in line with the central bank’s “slow and steady” approach to rate increases after a decade of historically low interest rates, and is the third such rate increase since June of 2006. The Federal Reserve has further signaled its intentions to complete at least two more quarter-point increases during 2017, with the median projection among Fed officials sitting at 1.4% by year-end. This reflects a positive outlook for the US economy, as wage and job growth continue to display robustness in the face of a volatile global market place. But how likely are rates to continue rising?&lt;/p&gt;
&lt;p&gt;The impending and steady rate increases has caused a dramatic rise in bond issuance, as investors pile in to low rate debt securities. Companies sold a record-high amount of bonds in March to the tune of $414 billion, the Wall Street Journal reported earlier this week. This signals that the market believes the Fed’s intentions to raise rates, lending further evidence to the certainty of rate increases.&lt;/p&gt;
&lt;p&gt;Perhaps a bigger indicator that rates will continue to raise steadily – and may even have an accelerated pace – is the split between US monetary and fiscal policy. As the Federal Reserve undertakes monetary tightening, the Trump administration and the Republican majority have indicated expansionary fiscal policy in preliminary budgets. This includes infrastructure spending, which Trump has stated could be as much as $1 trillion, as well as increased military and defense spending. Each of these are certain to increase money supply in the economy and drive inflation expectations, which may increase Federal Reserve rate pacing.&lt;/p&gt;
&lt;p&gt;Thus, the likelihood of continued rate hikes seems high and potentially increasing. This could signal bad news for both personal income investors who will see the market value of their holdings drop, and institutional investors with actively managed bond funds who will struggle to find profitable investment opportunities. The global impacts will be significant, as the federal funds rate is tightly linked with other interest rates. In fact, one day after the Federal Reserve raised its rates a quarter percent, the People’s Bank of China kept its rates in lock step with its American counterpart.&lt;/p&gt;
&lt;p&gt;The markets are headed towards the normalcy previous generations experienced, with 30-year interest rates above 3% and federal fund rate changes that nobody noticed. Now, when a rate hike is expected, it is talked about endlessly weeks before and after. This is good for savers and pension funds, but bond investors are going to suffer during the transition from almost-free money to rates I haven’t seen since I was 12.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEX&#39;s Crumbling Quote</title>
      <link>/2017/04/16/iexs-crumbling-quote/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/16/iexs-crumbling-quote/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This article first appeared in the Reading University Investment Society&amp;rsquo;s  &lt;a href=&#34;https://www.dropbox.com/s/vicfm15cza2gt8u/RUIS-April.pdf?dl=0&#34;&gt;April edition&lt;/a&gt; on April 16th, 2017.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Investor’s Exchange, also referred to as IEX, came to prominence in 2014 after Michael Lewis’ book Flash Boys was released. IEX was designed as an exchange to protect lumbering buy side firms from high-frequency traders and other nimble market actors.&lt;/p&gt;
&lt;p&gt;Their first innovation was to treat what is referred to as stale quote arbitrage. In this type of arbitrage, a fast trader could detect when the best bid or offer had changed on one exchange, and then immediately go to another exchange and pick off a midpoint quote before the midpoint updated. To solve this problem, IEX famously built a box in a warehouse with a very long coil of fiber optic cable designed to introduce a 350-microsecond delay into quote updates. Any trader who could detect a midpoint quote before it updated on another exchange could not immediately post an order on IEX, because it would suffer a delay while the exchange updated the midpoint quotes.&lt;/p&gt;
&lt;p&gt;IEX’s dedication to ensuring low transaction costs to its buy side clients is ceaseless, and it has published a new working paper about dealing with another type of arbitrage, which it calls “crumbling quote arbitrage”. The paper, written by Allison Bishop is entitled “&lt;a href=&#34;https://iextrading.com/docs/The%20Evolution%20of%20the%20Crumbling%20Quote%20Signal.pdf?utm_medium=email&amp;amp;utm_source=newsletter&amp;amp;utm_term=170411&amp;amp;utm_campaign=moneystuff&#34;&gt;The Evolution of the Crumbling Quote Signal&lt;/a&gt;” and details the exchange’s cunning way of dealing with an interesting problem.&lt;/p&gt;
&lt;p&gt;Crumbling quotes refer to when the number of exchanges on the national best bid or offer is eroding over time as the market eats up posted volumes. This can and does happen in legitimate market trading, but some predatory firms may intentionally claim all posted volumes and have in place an order to take advantage of IEX’s delay.&lt;/p&gt;
&lt;p&gt;The working paper is fascinating, as Bishop demonstrates IEX’s approach to predicting when a crumbling quote is likely to occur, and allowing orders pegged to the midpoint to exercise discretion when IEX’s “signal” is on. Their new rule was approved by the SEC for use in production, though it has yet to be fully installed exchange-wide.&lt;/p&gt;
&lt;p&gt;IEX has been at the forefront of protecting its client’s interests, and this is simply the next step at the frontier of combating high frequency traders and other technologically advanced market participants. IEX has a powerful advantage over market predators, and that is that it – along with other exchanges – has access to both a greater quantity and quality of data that traders would usually have to pay thousands of dollars a month for. It will certainly be interesting to watch the development and competition between IEX and those it tries to protect its clients from.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog</title>
      <link>/blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>